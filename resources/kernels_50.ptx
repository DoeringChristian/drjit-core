//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_50
.address_size 64

	// .globl	reduce_add_u32
.extern .shared .align 4 .b8 shared[];
.extern .shared .align 8 .b8 shared_d[];

.visible .entry reduce_add_u32(
	.param .u64 reduce_add_u32_param_0,
	.param .u32 reduce_add_u32_param_1,
	.param .u64 reduce_add_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_add_u32_param_0];
	ld.param.u32 	%r21, [reduce_add_u32_param_1];
	ld.param.u64 	%rd2, [reduce_add_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB0_4;

BB0_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	add.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB0_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	add.s32 	%r62, %r28, %r62;

BB0_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB0_1;

BB0_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB0_6;

	ld.shared.u32 	%r33, [%r11+2048];
	add.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB0_8;

	ld.shared.u32 	%r35, [%r11+1024];
	add.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB0_10;

	ld.shared.u32 	%r37, [%r11+512];
	add.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB0_12;

	ld.shared.u32 	%r39, [%r11+256];
	add.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB0_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB0_15;

	ld.shared.u32 	%r41, [%r11+128];
	add.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	add.s32 	%r47, %r46, %r42;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	add.s32 	%r50, %r49, %r47;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	add.s32 	%r53, %r52, %r50;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	add.s32 	%r56, %r55, %r53;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	add.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB0_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB0_15:
	ret;
}

	// .globl	reduce_add_u64
.visible .entry reduce_add_u64(
	.param .u64 reduce_add_u64_param_0,
	.param .u32 reduce_add_u64_param_1,
	.param .u64 reduce_add_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_add_u64_param_0];
	ld.param.u32 	%r9, [reduce_add_u64_param_1];
	ld.param.u64 	%rd16, [reduce_add_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 0;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB1_4;

BB1_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	add.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB1_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	add.s64 	%rd44, %rd25, %rd44;

BB1_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB1_1;

BB1_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB1_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	add.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB1_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB1_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	add.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB1_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB1_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	add.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB1_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB1_12;

	ld.shared.u64 	%rd29, [%r8+512];
	add.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB1_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB1_15;

	ld.shared.u64 	%rd40, [%r8+256];
	add.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	add.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	add.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	add.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	add.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	add.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB1_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB1_15:
	ret;
}

	// .globl	reduce_add_f16
.visible .entry reduce_add_f16(
	.param .u64 reduce_add_f16_param_0,
	.param .u32 reduce_add_f16_param_1,
	.param .u64 reduce_add_f16_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<78>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_add_f16_param_0];
	ld.param.u32 	%r7, [reduce_add_f16_param_1];
	ld.param.u64 	%rd2, [reduce_add_f16_param_2];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 11;
	add.s32 	%r39, %r11, %r9;
	mov.u32 	%r8, 0;
	// inline asm
	cvt.rn.f16.s32 %rs1, %r8;
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs1;}

	// inline asm
	setp.ge.u32	%p1, %r39, %r7;
	@%p1 bra 	BB2_4;

BB2_1:
	// inline asm
	{  cvt.rn.f16.f32 %rs3, %f92;}

	// inline asm
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r39, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f18, %rs4;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs5, %f18;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs3;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs5;}

	// inline asm
	add.f32 	%f22, %f20, %f21;
	// inline asm
	{  cvt.rn.f16.f32 %rs8, %f22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs8;}

	// inline asm
	add.s32 	%r3, %r39, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB2_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f92;}

	// inline asm
	mul.wide.u32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u16 	%rs11, [%rd8];
	// inline asm
	{  cvt.f32.f16 %f25, %rs11;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs12, %f25;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f27, %rs10;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f28, %rs12;}

	// inline asm
	add.f32 	%f29, %f27, %f28;
	// inline asm
	{  cvt.rn.f16.f32 %rs15, %f29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs15;}

	// inline asm

BB2_3:
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r13, %r12, 11;
	add.s32 	%r39, %r39, %r13;
	setp.lt.u32	%p3, %r39, %r7;
	@%p3 bra 	BB2_1;

BB2_4:
	shl.b32 	%r14, %r9, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r6, %r15, %r14;
	st.shared.f32 	[%r6], %f92;
	bar.sync 	0;
	setp.gt.u32	%p4, %r9, 511;
	@%p4 bra 	BB2_6;

	// inline asm
	{  cvt.rn.f16.f32 %rs17, %f92;}

	// inline asm
	ld.shared.f32 	%f32, [%r6+2048];
	// inline asm
	{  cvt.rn.f16.f32 %rs18, %f32;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f33, %rs17;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f34, %rs18;}

	// inline asm
	add.f32 	%f35, %f33, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs21;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB2_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r9, 255;
	@%p5 bra 	BB2_8;

	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f92;}

	// inline asm
	ld.shared.f32 	%f38, [%r6+1024];
	// inline asm
	{  cvt.rn.f16.f32 %rs24, %f38;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f39, %rs23;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f40, %rs24;}

	// inline asm
	add.f32 	%f41, %f39, %f40;
	// inline asm
	{  cvt.rn.f16.f32 %rs27, %f41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs27;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB2_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r9, 127;
	@%p6 bra 	BB2_10;

	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f92;}

	// inline asm
	ld.shared.f32 	%f44, [%r6+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f44;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f45, %rs29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f46, %rs30;}

	// inline asm
	add.f32 	%f47, %f45, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs33, %f47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs33;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB2_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r9, 63;
	@%p7 bra 	BB2_12;

	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f92;}

	// inline asm
	ld.shared.f32 	%f50, [%r6+256];
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f50;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f51, %rs35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f52, %rs36;}

	// inline asm
	add.f32 	%f53, %f51, %f52;
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs39;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB2_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r9, 31;
	@%p8 bra 	BB2_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f92;}

	// inline asm
	ld.shared.f32 	%f56, [%r6+128];
	// inline asm
	{  cvt.rn.f16.f32 %rs42, %f56;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f57, %rs41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f58, %rs42;}

	// inline asm
	add.f32 	%f59, %f57, %f58;
	// inline asm
	{  cvt.rn.f16.f32 %rs45, %f59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f60, %rs45;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs47, %f60;}

	// inline asm
	mov.b32 	 %r20, %f60;
	mov.u32 	%r21, 31;
	mov.u32 	%r22, 1;
	mov.u32 	%r23, -1;
	shfl.sync.bfly.b32 	%r24|%p9, %r20, %r22, %r21, %r23;
	mov.b32 	 %f62, %r24;
	// inline asm
	{  cvt.rn.f16.f32 %rs48, %f62;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f63, %rs47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f64, %rs48;}

	// inline asm
	add.f32 	%f65, %f63, %f64;
	// inline asm
	{  cvt.rn.f16.f32 %rs51, %f65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f66, %rs51;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs53, %f66;}

	// inline asm
	mov.b32 	 %r25, %f66;
	mov.u32 	%r26, 2;
	shfl.sync.bfly.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f68, %r27;
	// inline asm
	{  cvt.rn.f16.f32 %rs54, %f68;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f69, %rs53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f70, %rs54;}

	// inline asm
	add.f32 	%f71, %f69, %f70;
	// inline asm
	{  cvt.rn.f16.f32 %rs57, %f71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f72, %rs57;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs59, %f72;}

	// inline asm
	mov.b32 	 %r28, %f72;
	mov.u32 	%r29, 4;
	shfl.sync.bfly.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f74, %r30;
	// inline asm
	{  cvt.rn.f16.f32 %rs60, %f74;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f75, %rs59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f76, %rs60;}

	// inline asm
	add.f32 	%f77, %f75, %f76;
	// inline asm
	{  cvt.rn.f16.f32 %rs63, %f77;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f78, %rs63;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f78;}

	// inline asm
	mov.b32 	 %r31, %f78;
	mov.u32 	%r32, 8;
	shfl.sync.bfly.b32 	%r33|%p12, %r31, %r32, %r21, %r23;
	mov.b32 	 %f80, %r33;
	// inline asm
	{  cvt.rn.f16.f32 %rs66, %f80;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f81, %rs65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f82, %rs66;}

	// inline asm
	add.f32 	%f83, %f81, %f82;
	// inline asm
	{  cvt.rn.f16.f32 %rs69, %f83;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f84, %rs69;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f84;}

	// inline asm
	mov.b32 	 %r34, %f84;
	mov.u32 	%r35, 16;
	shfl.sync.bfly.b32 	%r36|%p13, %r34, %r35, %r21, %r23;
	mov.b32 	 %f86, %r36;
	// inline asm
	{  cvt.rn.f16.f32 %rs72, %f86;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f87, %rs71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f88, %rs72;}

	// inline asm
	add.f32 	%f89, %f87, %f88;
	// inline asm
	{  cvt.rn.f16.f32 %rs75, %f89;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f90, %rs75;}

	// inline asm
	setp.ne.s32	%p14, %r9, 0;
	@%p14 bra 	BB2_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs77, %f90;}

	// inline asm
	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r10, 2;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u16 	[%rd11], %rs77;

BB2_15:
	ret;
}

	// .globl	reduce_add_f32
.visible .entry reduce_add_f32(
	.param .u64 reduce_add_f32_param_0,
	.param .u32 reduce_add_f32_param_1,
	.param .u64 reduce_add_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_add_f32_param_0];
	ld.param.u32 	%r7, [reduce_add_f32_param_1];
	ld.param.u64 	%rd2, [reduce_add_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f00000000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB3_4;

BB3_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	add.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB3_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	add.f32 	%f34, %f34, %f18;

BB3_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB3_1;

BB3_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB3_6;

	ld.shared.f32 	%f19, [%r6+2048];
	add.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB3_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB3_8;

	ld.shared.f32 	%f20, [%r6+1024];
	add.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB3_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB3_10;

	ld.shared.f32 	%f21, [%r6+512];
	add.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB3_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB3_12;

	ld.shared.f32 	%f22, [%r6+256];
	add.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB3_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB3_15;

	ld.shared.f32 	%f23, [%r6+128];
	add.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f25, %r23;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	 %r24, %f26;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f27, %r26;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	 %r27, %f28;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f29, %r29;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	 %r30, %f30;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f31, %r32;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f33, %r35;
	add.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB3_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB3_15:
	ret;
}

	// .globl	reduce_add_f64
.visible .entry reduce_add_f64(
	.param .u64 reduce_add_f64_param_0,
	.param .u32 reduce_add_f64_param_1,
	.param .u64 reduce_add_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_add_f64_param_0];
	ld.param.u32 	%r9, [reduce_add_f64_param_1];
	ld.param.u64 	%rd2, [reduce_add_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d0000000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB4_4;

BB4_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	add.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB4_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	add.f64 	%fd34, %fd34, %fd18;

BB4_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB4_1;

BB4_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB4_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	add.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB4_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB4_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	add.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB4_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB4_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	add.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB4_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB4_12;

	ld.shared.f64 	%fd22, [%r8+512];
	add.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB4_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB4_15;

	ld.shared.f64 	%fd33, [%r8+256];
	add.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	add.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	add.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	add.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	add.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	add.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB4_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB4_15:
	ret;
}

	// .globl	reduce_mul_i32
.visible .entry reduce_mul_i32(
	.param .u64 reduce_mul_i32_param_0,
	.param .u32 reduce_mul_i32_param_1,
	.param .u64 reduce_mul_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_i32_param_0];
	ld.param.u32 	%r21, [reduce_mul_i32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB5_4;

BB5_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	mul.lo.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB5_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	mul.lo.s32 	%r62, %r28, %r62;

BB5_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB5_1;

BB5_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB5_6;

	ld.shared.u32 	%r33, [%r11+2048];
	mul.lo.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB5_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB5_8;

	ld.shared.u32 	%r35, [%r11+1024];
	mul.lo.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB5_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB5_10;

	ld.shared.u32 	%r37, [%r11+512];
	mul.lo.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB5_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB5_12;

	ld.shared.u32 	%r39, [%r11+256];
	mul.lo.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB5_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB5_15;

	ld.shared.u32 	%r41, [%r11+128];
	mul.lo.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	mul.lo.s32 	%r47, %r46, %r42;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	mul.lo.s32 	%r50, %r49, %r47;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	mul.lo.s32 	%r53, %r52, %r50;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	mul.lo.s32 	%r56, %r55, %r53;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	mul.lo.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB5_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB5_15:
	ret;
}

	// .globl	reduce_mul_u32
.visible .entry reduce_mul_u32(
	.param .u64 reduce_mul_u32_param_0,
	.param .u32 reduce_mul_u32_param_1,
	.param .u64 reduce_mul_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_u32_param_0];
	ld.param.u32 	%r21, [reduce_mul_u32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB6_4;

BB6_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	mul.lo.s32 	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB6_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	mul.lo.s32 	%r62, %r28, %r62;

BB6_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB6_1;

BB6_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB6_6;

	ld.shared.u32 	%r33, [%r11+2048];
	mul.lo.s32 	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB6_8;

	ld.shared.u32 	%r35, [%r11+1024];
	mul.lo.s32 	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB6_10;

	ld.shared.u32 	%r37, [%r11+512];
	mul.lo.s32 	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB6_12;

	ld.shared.u32 	%r39, [%r11+256];
	mul.lo.s32 	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB6_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB6_15;

	ld.shared.u32 	%r41, [%r11+128];
	mul.lo.s32 	%r42, %r41, %r62;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	mul.lo.s32 	%r47, %r46, %r42;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	mul.lo.s32 	%r50, %r49, %r47;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	mul.lo.s32 	%r53, %r52, %r50;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	mul.lo.s32 	%r56, %r55, %r53;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	mul.lo.s32 	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB6_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB6_15:
	ret;
}

	// .globl	reduce_mul_i64
.visible .entry reduce_mul_i64(
	.param .u64 reduce_mul_i64_param_0,
	.param .u32 reduce_mul_i64_param_1,
	.param .u64 reduce_mul_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_mul_i64_param_0];
	ld.param.u32 	%r9, [reduce_mul_i64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB7_4;

BB7_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB7_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

BB7_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB7_1;

BB7_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB7_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB7_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB7_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB7_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB7_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB7_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB7_12;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB7_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB7_15;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB7_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB7_15:
	ret;
}

	// .globl	reduce_mul_u64
.visible .entry reduce_mul_u64(
	.param .u64 reduce_mul_u64_param_0,
	.param .u32 reduce_mul_u64_param_1,
	.param .u64 reduce_mul_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_mul_u64_param_0];
	ld.param.u32 	%r9, [reduce_mul_u64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB8_4;

BB8_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB8_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

BB8_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB8_1;

BB8_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB8_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB8_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB8_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB8_12;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

BB8_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB8_15;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB8_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB8_15:
	ret;
}

	// .globl	reduce_mul_f16
.visible .entry reduce_mul_f16(
	.param .u64 reduce_mul_f16_param_0,
	.param .u32 reduce_mul_f16_param_1,
	.param .u64 reduce_mul_f16_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<78>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_f16_param_0];
	ld.param.u32 	%r7, [reduce_mul_f16_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f16_param_2];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 11;
	add.s32 	%r39, %r11, %r9;
	mov.u32 	%r8, 1;
	// inline asm
	cvt.rn.f16.s32 %rs1, %r8;
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs1;}

	// inline asm
	setp.ge.u32	%p1, %r39, %r7;
	@%p1 bra 	BB9_4;

BB9_1:
	// inline asm
	{  cvt.rn.f16.f32 %rs3, %f92;}

	// inline asm
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r39, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs4, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f18, %rs4;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs5, %f18;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs3;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs5;}

	// inline asm
	mul.f32 	%f22, %f20, %f21;
	// inline asm
	{  cvt.rn.f16.f32 %rs8, %f22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs8;}

	// inline asm
	add.s32 	%r3, %r39, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB9_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs10, %f92;}

	// inline asm
	mul.wide.u32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u16 	%rs11, [%rd8];
	// inline asm
	{  cvt.f32.f16 %f25, %rs11;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs12, %f25;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f27, %rs10;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f28, %rs12;}

	// inline asm
	mul.f32 	%f29, %f27, %f28;
	// inline asm
	{  cvt.rn.f16.f32 %rs15, %f29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs15;}

	// inline asm

BB9_3:
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r13, %r12, 11;
	add.s32 	%r39, %r39, %r13;
	setp.lt.u32	%p3, %r39, %r7;
	@%p3 bra 	BB9_1;

BB9_4:
	shl.b32 	%r14, %r9, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r6, %r15, %r14;
	st.shared.f32 	[%r6], %f92;
	bar.sync 	0;
	setp.gt.u32	%p4, %r9, 511;
	@%p4 bra 	BB9_6;

	// inline asm
	{  cvt.rn.f16.f32 %rs17, %f92;}

	// inline asm
	ld.shared.f32 	%f32, [%r6+2048];
	// inline asm
	{  cvt.rn.f16.f32 %rs18, %f32;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f33, %rs17;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f34, %rs18;}

	// inline asm
	mul.f32 	%f35, %f33, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs21, %f35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs21;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB9_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r9, 255;
	@%p5 bra 	BB9_8;

	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f92;}

	// inline asm
	ld.shared.f32 	%f38, [%r6+1024];
	// inline asm
	{  cvt.rn.f16.f32 %rs24, %f38;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f39, %rs23;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f40, %rs24;}

	// inline asm
	mul.f32 	%f41, %f39, %f40;
	// inline asm
	{  cvt.rn.f16.f32 %rs27, %f41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs27;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB9_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r9, 127;
	@%p6 bra 	BB9_10;

	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f92;}

	// inline asm
	ld.shared.f32 	%f44, [%r6+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs30, %f44;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f45, %rs29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f46, %rs30;}

	// inline asm
	mul.f32 	%f47, %f45, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs33, %f47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs33;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB9_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r9, 63;
	@%p7 bra 	BB9_12;

	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f92;}

	// inline asm
	ld.shared.f32 	%f50, [%r6+256];
	// inline asm
	{  cvt.rn.f16.f32 %rs36, %f50;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f51, %rs35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f52, %rs36;}

	// inline asm
	mul.f32 	%f53, %f51, %f52;
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs39;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB9_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r9, 31;
	@%p8 bra 	BB9_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f92;}

	// inline asm
	ld.shared.f32 	%f56, [%r6+128];
	// inline asm
	{  cvt.rn.f16.f32 %rs42, %f56;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f57, %rs41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f58, %rs42;}

	// inline asm
	mul.f32 	%f59, %f57, %f58;
	// inline asm
	{  cvt.rn.f16.f32 %rs45, %f59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f60, %rs45;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs47, %f60;}

	// inline asm
	mov.b32 	 %r20, %f60;
	mov.u32 	%r21, 31;
	mov.u32 	%r23, -1;
	shfl.sync.bfly.b32 	%r24|%p9, %r20, %r8, %r21, %r23;
	mov.b32 	 %f62, %r24;
	// inline asm
	{  cvt.rn.f16.f32 %rs48, %f62;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f63, %rs47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f64, %rs48;}

	// inline asm
	mul.f32 	%f65, %f63, %f64;
	// inline asm
	{  cvt.rn.f16.f32 %rs51, %f65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f66, %rs51;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs53, %f66;}

	// inline asm
	mov.b32 	 %r25, %f66;
	mov.u32 	%r26, 2;
	shfl.sync.bfly.b32 	%r27|%p10, %r25, %r26, %r21, %r23;
	mov.b32 	 %f68, %r27;
	// inline asm
	{  cvt.rn.f16.f32 %rs54, %f68;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f69, %rs53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f70, %rs54;}

	// inline asm
	mul.f32 	%f71, %f69, %f70;
	// inline asm
	{  cvt.rn.f16.f32 %rs57, %f71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f72, %rs57;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs59, %f72;}

	// inline asm
	mov.b32 	 %r28, %f72;
	mov.u32 	%r29, 4;
	shfl.sync.bfly.b32 	%r30|%p11, %r28, %r29, %r21, %r23;
	mov.b32 	 %f74, %r30;
	// inline asm
	{  cvt.rn.f16.f32 %rs60, %f74;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f75, %rs59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f76, %rs60;}

	// inline asm
	mul.f32 	%f77, %f75, %f76;
	// inline asm
	{  cvt.rn.f16.f32 %rs63, %f77;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f78, %rs63;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f78;}

	// inline asm
	mov.b32 	 %r31, %f78;
	mov.u32 	%r32, 8;
	shfl.sync.bfly.b32 	%r33|%p12, %r31, %r32, %r21, %r23;
	mov.b32 	 %f80, %r33;
	// inline asm
	{  cvt.rn.f16.f32 %rs66, %f80;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f81, %rs65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f82, %rs66;}

	// inline asm
	mul.f32 	%f83, %f81, %f82;
	// inline asm
	{  cvt.rn.f16.f32 %rs69, %f83;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f84, %rs69;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f84;}

	// inline asm
	mov.b32 	 %r34, %f84;
	mov.u32 	%r35, 16;
	shfl.sync.bfly.b32 	%r36|%p13, %r34, %r35, %r21, %r23;
	mov.b32 	 %f86, %r36;
	// inline asm
	{  cvt.rn.f16.f32 %rs72, %f86;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f87, %rs71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f88, %rs72;}

	// inline asm
	mul.f32 	%f89, %f87, %f88;
	// inline asm
	{  cvt.rn.f16.f32 %rs75, %f89;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f90, %rs75;}

	// inline asm
	setp.ne.s32	%p14, %r9, 0;
	@%p14 bra 	BB9_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs77, %f90;}

	// inline asm
	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r10, 2;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u16 	[%rd11], %rs77;

BB9_15:
	ret;
}

	// .globl	reduce_mul_f32
.visible .entry reduce_mul_f32(
	.param .u64 reduce_mul_f32_param_0,
	.param .u32 reduce_mul_f32_param_1,
	.param .u64 reduce_mul_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_mul_f32_param_0];
	ld.param.u32 	%r7, [reduce_mul_f32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f3F800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB10_4;

BB10_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	mul.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB10_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	mul.f32 	%f34, %f34, %f18;

BB10_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB10_1;

BB10_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB10_6;

	ld.shared.f32 	%f19, [%r6+2048];
	mul.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB10_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB10_8;

	ld.shared.f32 	%f20, [%r6+1024];
	mul.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB10_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB10_10;

	ld.shared.f32 	%f21, [%r6+512];
	mul.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB10_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB10_12;

	ld.shared.f32 	%f22, [%r6+256];
	mul.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB10_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB10_15;

	ld.shared.f32 	%f23, [%r6+128];
	mul.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f25, %r23;
	mul.f32 	%f26, %f24, %f25;
	mov.b32 	 %r24, %f26;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f27, %r26;
	mul.f32 	%f28, %f26, %f27;
	mov.b32 	 %r27, %f28;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f29, %r29;
	mul.f32 	%f30, %f28, %f29;
	mov.b32 	 %r30, %f30;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f31, %r32;
	mul.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f33, %r35;
	mul.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB10_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB10_15:
	ret;
}

	// .globl	reduce_mul_f64
.visible .entry reduce_mul_f64(
	.param .u64 reduce_mul_f64_param_0,
	.param .u32 reduce_mul_f64_param_1,
	.param .u64 reduce_mul_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_f64_param_0];
	ld.param.u32 	%r9, [reduce_mul_f64_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d3FF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB11_4;

BB11_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	mul.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB11_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	mul.f64 	%fd34, %fd34, %fd18;

BB11_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB11_1;

BB11_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB11_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	mul.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB11_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB11_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	mul.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB11_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB11_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	mul.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB11_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB11_12;

	ld.shared.f64 	%fd22, [%r8+512];
	mul.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB11_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB11_15;

	ld.shared.f64 	%fd33, [%r8+256];
	mul.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	mul.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	mul.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	mul.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	mul.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	mul.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB11_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB11_15:
	ret;
}

	// .globl	reduce_min_i32
.visible .entry reduce_min_i32(
	.param .u64 reduce_min_i32_param_0,
	.param .u32 reduce_min_i32_param_1,
	.param .u64 reduce_min_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_i32_param_0];
	ld.param.u32 	%r21, [reduce_min_i32_param_1];
	ld.param.u64 	%rd2, [reduce_min_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 2147483647;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB12_4;

BB12_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	min.s32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB12_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	min.s32 	%r62, %r62, %r28;

BB12_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB12_1;

BB12_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB12_6;

	ld.shared.u32 	%r33, [%r11+2048];
	min.s32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB12_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB12_8;

	ld.shared.u32 	%r35, [%r11+1024];
	min.s32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB12_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB12_10;

	ld.shared.u32 	%r37, [%r11+512];
	min.s32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB12_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB12_12;

	ld.shared.u32 	%r39, [%r11+256];
	min.s32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB12_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB12_15;

	ld.shared.u32 	%r41, [%r11+128];
	min.s32 	%r42, %r62, %r41;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	min.s32 	%r47, %r42, %r46;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	min.s32 	%r50, %r47, %r49;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	min.s32 	%r53, %r50, %r52;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	min.s32 	%r56, %r53, %r55;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	min.s32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB12_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB12_15:
	ret;
}

	// .globl	reduce_min_u32
.visible .entry reduce_min_u32(
	.param .u64 reduce_min_u32_param_0,
	.param .u32 reduce_min_u32_param_1,
	.param .u64 reduce_min_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_u32_param_0];
	ld.param.u32 	%r21, [reduce_min_u32_param_1];
	ld.param.u64 	%rd2, [reduce_min_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB13_4;

BB13_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	min.u32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB13_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	min.u32 	%r62, %r62, %r28;

BB13_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB13_1;

BB13_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB13_6;

	ld.shared.u32 	%r33, [%r11+2048];
	min.u32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB13_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB13_8;

	ld.shared.u32 	%r35, [%r11+1024];
	min.u32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB13_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB13_10;

	ld.shared.u32 	%r37, [%r11+512];
	min.u32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB13_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB13_12;

	ld.shared.u32 	%r39, [%r11+256];
	min.u32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB13_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB13_15;

	ld.shared.u32 	%r41, [%r11+128];
	min.u32 	%r42, %r62, %r41;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	min.u32 	%r47, %r42, %r46;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	min.u32 	%r50, %r47, %r49;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	min.u32 	%r53, %r50, %r52;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	min.u32 	%r56, %r53, %r55;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	min.u32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB13_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB13_15:
	ret;
}

	// .globl	reduce_min_i64
.visible .entry reduce_min_i64(
	.param .u64 reduce_min_i64_param_0,
	.param .u32 reduce_min_i64_param_1,
	.param .u64 reduce_min_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_min_i64_param_0];
	ld.param.u32 	%r9, [reduce_min_i64_param_1];
	ld.param.u64 	%rd16, [reduce_min_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 9223372036854775807;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB14_4;

BB14_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB14_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.s64 	%rd44, %rd44, %rd25;

BB14_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB14_1;

BB14_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB14_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB14_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB14_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB14_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB14_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB14_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB14_12;

	ld.shared.u64 	%rd29, [%r8+512];
	min.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB14_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB14_15;

	ld.shared.u64 	%rd40, [%r8+256];
	min.s64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	min.s64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	min.s64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	min.s64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	min.s64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	min.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB14_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB14_15:
	ret;
}

	// .globl	reduce_min_u64
.visible .entry reduce_min_u64(
	.param .u64 reduce_min_u64_param_0,
	.param .u32 reduce_min_u64_param_1,
	.param .u64 reduce_min_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_min_u64_param_0];
	ld.param.u32 	%r9, [reduce_min_u64_param_1];
	ld.param.u64 	%rd16, [reduce_min_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, -1;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB15_4;

BB15_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB15_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.u64 	%rd44, %rd44, %rd25;

BB15_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB15_1;

BB15_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB15_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB15_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB15_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB15_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB15_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB15_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB15_12;

	ld.shared.u64 	%rd29, [%r8+512];
	min.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB15_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB15_15;

	ld.shared.u64 	%rd40, [%r8+256];
	min.u64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	min.u64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	min.u64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	min.u64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	min.u64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	min.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB15_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB15_15:
	ret;
}

	// .globl	reduce_min_f16
.visible .entry reduce_min_f16(
	.param .u64 reduce_min_f16_param_0,
	.param .u32 reduce_min_f16_param_1,
	.param .u64 reduce_min_f16_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<77>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_f16_param_0];
	ld.param.u32 	%r7, [reduce_min_f16_param_1];
	ld.param.u64 	%rd2, [reduce_min_f16_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.u16 	%rs1, 31743;
	// inline asm
	{  cvt.f32.f16 %f92, %rs1;}

	// inline asm
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB16_4;

BB16_1:
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f92;}

	// inline asm
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs3, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f18, %rs3;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs4, %f18;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs2;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs4;}

	// inline asm
	min.f32 	%f22, %f20, %f21;
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs7;}

	// inline asm
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB16_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs9, %f92;}

	// inline asm
	mul.wide.u32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u16 	%rs10, [%rd8];
	// inline asm
	{  cvt.f32.f16 %f25, %rs10;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs11, %f25;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f27, %rs9;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f28, %rs11;}

	// inline asm
	min.f32 	%f29, %f27, %f28;
	// inline asm
	{  cvt.rn.f16.f32 %rs14, %f29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs14;}

	// inline asm

BB16_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB16_1;

BB16_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f92;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB16_6;

	// inline asm
	{  cvt.rn.f16.f32 %rs16, %f92;}

	// inline asm
	ld.shared.f32 	%f32, [%r6+2048];
	// inline asm
	{  cvt.rn.f16.f32 %rs17, %f32;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f33, %rs16;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f34, %rs17;}

	// inline asm
	min.f32 	%f35, %f33, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs20, %f35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs20;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB16_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB16_8;

	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f92;}

	// inline asm
	ld.shared.f32 	%f38, [%r6+1024];
	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f38;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f39, %rs22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f40, %rs23;}

	// inline asm
	min.f32 	%f41, %f39, %f40;
	// inline asm
	{  cvt.rn.f16.f32 %rs26, %f41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs26;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB16_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB16_10;

	// inline asm
	{  cvt.rn.f16.f32 %rs28, %f92;}

	// inline asm
	ld.shared.f32 	%f44, [%r6+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f44;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f45, %rs28;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f46, %rs29;}

	// inline asm
	min.f32 	%f47, %f45, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs32, %f47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs32;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB16_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB16_12;

	// inline asm
	{  cvt.rn.f16.f32 %rs34, %f92;}

	// inline asm
	ld.shared.f32 	%f50, [%r6+256];
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f50;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f51, %rs34;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f52, %rs35;}

	// inline asm
	min.f32 	%f53, %f51, %f52;
	// inline asm
	{  cvt.rn.f16.f32 %rs38, %f53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs38;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB16_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB16_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs40, %f92;}

	// inline asm
	ld.shared.f32 	%f56, [%r6+128];
	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f56;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f57, %rs40;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f58, %rs41;}

	// inline asm
	min.f32 	%f59, %f57, %f58;
	// inline asm
	{  cvt.rn.f16.f32 %rs44, %f59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f60, %rs44;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs46, %f60;}

	// inline asm
	mov.b32 	 %r19, %f60;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f62, %r23;
	// inline asm
	{  cvt.rn.f16.f32 %rs47, %f62;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f63, %rs46;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f64, %rs47;}

	// inline asm
	min.f32 	%f65, %f63, %f64;
	// inline asm
	{  cvt.rn.f16.f32 %rs50, %f65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f66, %rs50;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs52, %f66;}

	// inline asm
	mov.b32 	 %r24, %f66;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f68, %r26;
	// inline asm
	{  cvt.rn.f16.f32 %rs53, %f68;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f69, %rs52;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f70, %rs53;}

	// inline asm
	min.f32 	%f71, %f69, %f70;
	// inline asm
	{  cvt.rn.f16.f32 %rs56, %f71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f72, %rs56;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs58, %f72;}

	// inline asm
	mov.b32 	 %r27, %f72;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f74, %r29;
	// inline asm
	{  cvt.rn.f16.f32 %rs59, %f74;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f75, %rs58;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f76, %rs59;}

	// inline asm
	min.f32 	%f77, %f75, %f76;
	// inline asm
	{  cvt.rn.f16.f32 %rs62, %f77;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f78, %rs62;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs64, %f78;}

	// inline asm
	mov.b32 	 %r30, %f78;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f80, %r32;
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f80;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f81, %rs64;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f82, %rs65;}

	// inline asm
	min.f32 	%f83, %f81, %f82;
	// inline asm
	{  cvt.rn.f16.f32 %rs68, %f83;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f84, %rs68;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f84;}

	// inline asm
	mov.b32 	 %r33, %f84;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f86, %r35;
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f86;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f87, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f88, %rs71;}

	// inline asm
	min.f32 	%f89, %f87, %f88;
	// inline asm
	{  cvt.rn.f16.f32 %rs74, %f89;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f90, %rs74;}

	// inline asm
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB16_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs76, %f90;}

	// inline asm
	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 2;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u16 	[%rd11], %rs76;

BB16_15:
	ret;
}

	// .globl	reduce_min_f32
.visible .entry reduce_min_f32(
	.param .u64 reduce_min_f32_param_0,
	.param .u32 reduce_min_f32_param_1,
	.param .u64 reduce_min_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_min_f32_param_0];
	ld.param.u32 	%r7, [reduce_min_f32_param_1];
	ld.param.u64 	%rd2, [reduce_min_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0f7F800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB17_4;

BB17_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	min.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB17_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	min.f32 	%f34, %f34, %f18;

BB17_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB17_1;

BB17_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB17_6;

	ld.shared.f32 	%f19, [%r6+2048];
	min.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB17_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB17_8;

	ld.shared.f32 	%f20, [%r6+1024];
	min.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB17_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB17_10;

	ld.shared.f32 	%f21, [%r6+512];
	min.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB17_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB17_12;

	ld.shared.f32 	%f22, [%r6+256];
	min.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB17_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB17_15;

	ld.shared.f32 	%f23, [%r6+128];
	min.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f25, %r23;
	min.f32 	%f26, %f24, %f25;
	mov.b32 	 %r24, %f26;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f27, %r26;
	min.f32 	%f28, %f26, %f27;
	mov.b32 	 %r27, %f28;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f29, %r29;
	min.f32 	%f30, %f28, %f29;
	mov.b32 	 %r30, %f30;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f31, %r32;
	min.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f33, %r35;
	min.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB17_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB17_15:
	ret;
}

	// .globl	reduce_min_f64
.visible .entry reduce_min_f64(
	.param .u64 reduce_min_f64_param_0,
	.param .u32 reduce_min_f64_param_1,
	.param .u64 reduce_min_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_f64_param_0];
	ld.param.u32 	%r9, [reduce_min_f64_param_1];
	ld.param.u64 	%rd2, [reduce_min_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0d7FF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB18_4;

BB18_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	min.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB18_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	min.f64 	%fd34, %fd34, %fd18;

BB18_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB18_1;

BB18_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB18_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	min.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB18_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB18_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	min.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB18_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB18_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	min.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB18_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB18_12;

	ld.shared.f64 	%fd22, [%r8+512];
	min.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB18_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB18_15;

	ld.shared.f64 	%fd33, [%r8+256];
	min.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	min.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	min.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	min.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	min.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	min.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB18_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB18_15:
	ret;
}

	// .globl	reduce_max_i32
.visible .entry reduce_max_i32(
	.param .u64 reduce_max_i32_param_0,
	.param .u32 reduce_max_i32_param_1,
	.param .u64 reduce_max_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_i32_param_0];
	ld.param.u32 	%r21, [reduce_max_i32_param_1];
	ld.param.u64 	%rd2, [reduce_max_i32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -2147483648;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB19_4;

BB19_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	max.s32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB19_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	max.s32 	%r62, %r62, %r28;

BB19_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB19_1;

BB19_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB19_6;

	ld.shared.u32 	%r33, [%r11+2048];
	max.s32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB19_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB19_8;

	ld.shared.u32 	%r35, [%r11+1024];
	max.s32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB19_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB19_10;

	ld.shared.u32 	%r37, [%r11+512];
	max.s32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB19_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB19_12;

	ld.shared.u32 	%r39, [%r11+256];
	max.s32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB19_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB19_15;

	ld.shared.u32 	%r41, [%r11+128];
	max.s32 	%r42, %r62, %r41;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	max.s32 	%r47, %r42, %r46;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	max.s32 	%r50, %r47, %r49;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	max.s32 	%r53, %r50, %r52;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	max.s32 	%r56, %r53, %r55;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	max.s32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB19_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB19_15:
	ret;
}

	// .globl	reduce_max_u32
.visible .entry reduce_max_u32(
	.param .u64 reduce_max_u32_param_0,
	.param .u32 reduce_max_u32_param_1,
	.param .u64 reduce_max_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_u32_param_0];
	ld.param.u32 	%r21, [reduce_max_u32_param_1];
	ld.param.u64 	%rd2, [reduce_max_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB20_4;

BB20_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	max.u32 	%r62, %r62, %r27;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB20_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	max.u32 	%r62, %r62, %r28;

BB20_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB20_1;

BB20_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB20_6;

	ld.shared.u32 	%r33, [%r11+2048];
	max.u32 	%r62, %r62, %r33;
	st.shared.u32 	[%r11], %r62;

BB20_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB20_8;

	ld.shared.u32 	%r35, [%r11+1024];
	max.u32 	%r62, %r62, %r35;
	st.shared.u32 	[%r11], %r62;

BB20_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB20_10;

	ld.shared.u32 	%r37, [%r11+512];
	max.u32 	%r62, %r62, %r37;
	st.shared.u32 	[%r11], %r62;

BB20_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB20_12;

	ld.shared.u32 	%r39, [%r11+256];
	max.u32 	%r62, %r62, %r39;
	st.shared.u32 	[%r11], %r62;

BB20_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB20_15;

	ld.shared.u32 	%r41, [%r11+128];
	max.u32 	%r42, %r62, %r41;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	max.u32 	%r47, %r42, %r46;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	max.u32 	%r50, %r47, %r49;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	max.u32 	%r53, %r50, %r52;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	max.u32 	%r56, %r53, %r55;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	max.u32 	%r20, %r56, %r58;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB20_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB20_15:
	ret;
}

	// .globl	reduce_max_i64
.visible .entry reduce_max_i64(
	.param .u64 reduce_max_i64_param_0,
	.param .u32 reduce_max_i64_param_1,
	.param .u64 reduce_max_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_max_i64_param_0];
	ld.param.u32 	%r9, [reduce_max_i64_param_1];
	ld.param.u64 	%rd16, [reduce_max_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, -9223372036854775808;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB21_4;

BB21_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB21_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.s64 	%rd44, %rd44, %rd25;

BB21_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB21_1;

BB21_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB21_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB21_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB21_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB21_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB21_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB21_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB21_12;

	ld.shared.u64 	%rd29, [%r8+512];
	max.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB21_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB21_15;

	ld.shared.u64 	%rd40, [%r8+256];
	max.s64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	max.s64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	max.s64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	max.s64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	max.s64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	max.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB21_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB21_15:
	ret;
}

	// .globl	reduce_max_u64
.visible .entry reduce_max_u64(
	.param .u64 reduce_max_u64_param_0,
	.param .u32 reduce_max_u64_param_1,
	.param .u64 reduce_max_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd19, [reduce_max_u64_param_0];
	ld.param.u32 	%r9, [reduce_max_u64_param_1];
	ld.param.u64 	%rd16, [reduce_max_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.u64 	%rd44, 0;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB22_4;

BB22_1:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB22_3;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.u64 	%rd44, %rd44, %rd25;

BB22_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB22_1;

BB22_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB22_6;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

BB22_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB22_8;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

BB22_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB22_10;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

BB22_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB22_12;

	ld.shared.u64 	%rd29, [%r8+512];
	max.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

BB22_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB22_15;

	ld.shared.u64 	%rd40, [%r8+256];
	max.u64 	%rd30, %rd44, %rd40;
	// inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %rd31, {%r16,%r17};
	// inline asm
	max.u64 	%rd32, %rd30, %rd31;
	// inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %rd33, {%r20,%r21};
	// inline asm
	max.u64 	%rd34, %rd32, %rd33;
	// inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %rd35, {%r24,%r25};
	// inline asm
	max.u64 	%rd36, %rd34, %rd35;
	// inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %rd37, {%r28,%r29};
	// inline asm
	max.u64 	%rd38, %rd36, %rd37;
	// inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %rd39, {%r32,%r33};
	// inline asm
	max.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB22_15;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

BB22_15:
	ret;
}

	// .globl	reduce_max_f16
.visible .entry reduce_max_f16(
	.param .u64 reduce_max_f16_param_0,
	.param .u32 reduce_max_f16_param_1,
	.param .u64 reduce_max_f16_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<77>;
	.reg .f32 	%f<99>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_f16_param_0];
	ld.param.u32 	%r7, [reduce_max_f16_param_1];
	ld.param.u64 	%rd2, [reduce_max_f16_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.u16 	%rs1, -1024;
	// inline asm
	{  cvt.f32.f16 %f92, %rs1;}

	// inline asm
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB23_4;

BB23_1:
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f92;}

	// inline asm
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs3, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f18, %rs3;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs4, %f18;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f20, %rs2;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f21, %rs4;}

	// inline asm
	max.f32 	%f22, %f20, %f21;
	// inline asm
	{  cvt.rn.f16.f32 %rs7, %f22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs7;}

	// inline asm
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB23_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs9, %f92;}

	// inline asm
	mul.wide.u32 	%rd7, %r3, 2;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u16 	%rs10, [%rd8];
	// inline asm
	{  cvt.f32.f16 %f25, %rs10;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs11, %f25;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f27, %rs9;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f28, %rs11;}

	// inline asm
	max.f32 	%f29, %f27, %f28;
	// inline asm
	{  cvt.rn.f16.f32 %rs14, %f29;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs14;}

	// inline asm

BB23_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB23_1;

BB23_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f92;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB23_6;

	// inline asm
	{  cvt.rn.f16.f32 %rs16, %f92;}

	// inline asm
	ld.shared.f32 	%f32, [%r6+2048];
	// inline asm
	{  cvt.rn.f16.f32 %rs17, %f32;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f33, %rs16;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f34, %rs17;}

	// inline asm
	max.f32 	%f35, %f33, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs20, %f35;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs20;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB23_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB23_8;

	// inline asm
	{  cvt.rn.f16.f32 %rs22, %f92;}

	// inline asm
	ld.shared.f32 	%f38, [%r6+1024];
	// inline asm
	{  cvt.rn.f16.f32 %rs23, %f38;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f39, %rs22;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f40, %rs23;}

	// inline asm
	max.f32 	%f41, %f39, %f40;
	// inline asm
	{  cvt.rn.f16.f32 %rs26, %f41;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs26;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB23_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB23_10;

	// inline asm
	{  cvt.rn.f16.f32 %rs28, %f92;}

	// inline asm
	ld.shared.f32 	%f44, [%r6+512];
	// inline asm
	{  cvt.rn.f16.f32 %rs29, %f44;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f45, %rs28;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f46, %rs29;}

	// inline asm
	max.f32 	%f47, %f45, %f46;
	// inline asm
	{  cvt.rn.f16.f32 %rs32, %f47;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs32;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB23_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB23_12;

	// inline asm
	{  cvt.rn.f16.f32 %rs34, %f92;}

	// inline asm
	ld.shared.f32 	%f50, [%r6+256];
	// inline asm
	{  cvt.rn.f16.f32 %rs35, %f50;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f51, %rs34;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f52, %rs35;}

	// inline asm
	max.f32 	%f53, %f51, %f52;
	// inline asm
	{  cvt.rn.f16.f32 %rs38, %f53;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f92, %rs38;}

	// inline asm
	st.shared.f32 	[%r6], %f92;

BB23_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB23_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs40, %f92;}

	// inline asm
	ld.shared.f32 	%f56, [%r6+128];
	// inline asm
	{  cvt.rn.f16.f32 %rs41, %f56;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f57, %rs40;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f58, %rs41;}

	// inline asm
	max.f32 	%f59, %f57, %f58;
	// inline asm
	{  cvt.rn.f16.f32 %rs44, %f59;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f60, %rs44;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs46, %f60;}

	// inline asm
	mov.b32 	 %r19, %f60;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f62, %r23;
	// inline asm
	{  cvt.rn.f16.f32 %rs47, %f62;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f63, %rs46;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f64, %rs47;}

	// inline asm
	max.f32 	%f65, %f63, %f64;
	// inline asm
	{  cvt.rn.f16.f32 %rs50, %f65;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f66, %rs50;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs52, %f66;}

	// inline asm
	mov.b32 	 %r24, %f66;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f68, %r26;
	// inline asm
	{  cvt.rn.f16.f32 %rs53, %f68;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f69, %rs52;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f70, %rs53;}

	// inline asm
	max.f32 	%f71, %f69, %f70;
	// inline asm
	{  cvt.rn.f16.f32 %rs56, %f71;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f72, %rs56;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs58, %f72;}

	// inline asm
	mov.b32 	 %r27, %f72;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f74, %r29;
	// inline asm
	{  cvt.rn.f16.f32 %rs59, %f74;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f75, %rs58;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f76, %rs59;}

	// inline asm
	max.f32 	%f77, %f75, %f76;
	// inline asm
	{  cvt.rn.f16.f32 %rs62, %f77;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f78, %rs62;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs64, %f78;}

	// inline asm
	mov.b32 	 %r30, %f78;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f80, %r32;
	// inline asm
	{  cvt.rn.f16.f32 %rs65, %f80;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f81, %rs64;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f82, %rs65;}

	// inline asm
	max.f32 	%f83, %f81, %f82;
	// inline asm
	{  cvt.rn.f16.f32 %rs68, %f83;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f84, %rs68;}

	// inline asm
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f84;}

	// inline asm
	mov.b32 	 %r33, %f84;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f86, %r35;
	// inline asm
	{  cvt.rn.f16.f32 %rs71, %f86;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f87, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f88, %rs71;}

	// inline asm
	max.f32 	%f89, %f87, %f88;
	// inline asm
	{  cvt.rn.f16.f32 %rs74, %f89;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f90, %rs74;}

	// inline asm
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB23_15;

	// inline asm
	{  cvt.rn.f16.f32 %rs76, %f90;}

	// inline asm
	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 2;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u16 	[%rd11], %rs76;

BB23_15:
	ret;
}

	// .globl	reduce_max_f32
.visible .entry reduce_max_f32(
	.param .u64 reduce_max_f32_param_0,
	.param .u32 reduce_max_f32_param_1,
	.param .u64 reduce_max_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_max_f32_param_0];
	ld.param.u32 	%r7, [reduce_max_f32_param_1];
	ld.param.u64 	%rd2, [reduce_max_f32_param_2];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f34, 0fFF800000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB24_4;

BB24_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r38, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	max.f32 	%f34, %f34, %f17;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB24_3;

	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f18, [%rd8];
	max.f32 	%f34, %f34, %f18;

BB24_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB24_1;

BB24_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB24_6;

	ld.shared.f32 	%f19, [%r6+2048];
	max.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r6], %f34;

BB24_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB24_8;

	ld.shared.f32 	%f20, [%r6+1024];
	max.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r6], %f34;

BB24_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB24_10;

	ld.shared.f32 	%f21, [%r6+512];
	max.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r6], %f34;

BB24_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB24_12;

	ld.shared.f32 	%f22, [%r6+256];
	max.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r6], %f34;

BB24_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB24_15;

	ld.shared.f32 	%f23, [%r6+128];
	max.f32 	%f24, %f34, %f23;
	mov.b32 	 %r19, %f24;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f25, %r23;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	 %r24, %f26;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f27, %r26;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	 %r27, %f28;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f29, %r29;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	 %r30, %f30;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f31, %r32;
	max.f32 	%f32, %f30, %f31;
	mov.b32 	 %r33, %f32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f33, %r35;
	max.f32 	%f14, %f32, %f33;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB24_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.f32 	[%rd11], %f14;

BB24_15:
	ret;
}

	// .globl	reduce_max_f64
.visible .entry reduce_max_f64(
	.param .u64 reduce_max_f64_param_0,
	.param .u32 reduce_max_f64_param_1,
	.param .u64 reduce_max_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_f64_param_0];
	ld.param.u32 	%r9, [reduce_max_f64_param_1];
	ld.param.u64 	%rd2, [reduce_max_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd34, 0dFFF0000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB25_4;

BB25_1:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	max.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB25_3;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	max.f64 	%fd34, %fd34, %fd18;

BB25_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB25_1;

BB25_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB25_6;

	ld.shared.f64 	%fd19, [%r8+4096];
	max.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

BB25_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB25_8;

	ld.shared.f64 	%fd20, [%r8+2048];
	max.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

BB25_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB25_10;

	ld.shared.f64 	%fd21, [%r8+1024];
	max.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

BB25_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB25_12;

	ld.shared.f64 	%fd22, [%r8+512];
	max.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

BB25_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB25_15;

	ld.shared.f64 	%fd33, [%r8+256];
	max.f64 	%fd23, %fd34, %fd33;
	// inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %fd24, {%r16,%r17};
	// inline asm
	max.f64 	%fd25, %fd23, %fd24;
	// inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %fd26, {%r20,%r21};
	// inline asm
	max.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %fd28, {%r24,%r25};
	// inline asm
	max.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %fd30, {%r28,%r29};
	// inline asm
	max.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %fd32, {%r32,%r33};
	// inline asm
	max.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB25_15;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

BB25_15:
	ret;
}

	// .globl	reduce_or_u32
.visible .entry reduce_or_u32(
	.param .u64 reduce_or_u32_param_0,
	.param .u32 reduce_or_u32_param_1,
	.param .u64 reduce_or_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_or_u32_param_0];
	ld.param.u32 	%r21, [reduce_or_u32_param_1];
	ld.param.u64 	%rd2, [reduce_or_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, 0;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB26_4;

BB26_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	or.b32  	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB26_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	or.b32  	%r62, %r28, %r62;

BB26_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB26_1;

BB26_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB26_6;

	ld.shared.u32 	%r33, [%r11+2048];
	or.b32  	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB26_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB26_8;

	ld.shared.u32 	%r35, [%r11+1024];
	or.b32  	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB26_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB26_10;

	ld.shared.u32 	%r37, [%r11+512];
	or.b32  	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB26_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB26_12;

	ld.shared.u32 	%r39, [%r11+256];
	or.b32  	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB26_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB26_15;

	ld.shared.u32 	%r41, [%r11+128];
	or.b32  	%r42, %r41, %r62;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	or.b32  	%r47, %r46, %r42;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	or.b32  	%r50, %r49, %r47;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	or.b32  	%r53, %r52, %r50;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	or.b32  	%r56, %r55, %r53;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	or.b32  	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB26_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB26_15:
	ret;
}

	// .globl	reduce_and_u32
.visible .entry reduce_and_u32(
	.param .u64 reduce_and_u32_param_0,
	.param .u32 reduce_and_u32_param_1,
	.param .u64 reduce_and_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [reduce_and_u32_param_0];
	ld.param.u32 	%r21, [reduce_and_u32_param_1];
	ld.param.u64 	%rd2, [reduce_and_u32_param_2];
	mov.u32 	%r24, %tid.x;
	mov.u32 	%r25, %ctaid.x;
	shl.b32 	%r26, %r25, 11;
	add.s32 	%r61, %r26, %r24;
	mov.u32 	%r62, -1;
	setp.ge.u32	%p1, %r61, %r21;
	@%p1 bra 	BB27_4;

BB27_1:
	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r61, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r27, [%rd5];
	and.b32  	%r62, %r27, %r62;
	add.s32 	%r5, %r61, 1024;
	setp.ge.u32	%p2, %r5, %r21;
	@%p2 bra 	BB27_3;

	mul.wide.u32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r28, [%rd8];
	and.b32  	%r62, %r28, %r62;

BB27_3:
	mov.u32 	%r29, %nctaid.x;
	shl.b32 	%r30, %r29, 11;
	add.s32 	%r61, %r61, %r30;
	setp.lt.u32	%p3, %r61, %r21;
	@%p3 bra 	BB27_1;

BB27_4:
	shl.b32 	%r31, %r24, 2;
	mov.u32 	%r32, shared;
	add.s32 	%r11, %r32, %r31;
	st.shared.u32 	[%r11], %r62;
	bar.sync 	0;
	setp.gt.u32	%p4, %r24, 511;
	@%p4 bra 	BB27_6;

	ld.shared.u32 	%r33, [%r11+2048];
	and.b32  	%r62, %r33, %r62;
	st.shared.u32 	[%r11], %r62;

BB27_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r24, 255;
	@%p5 bra 	BB27_8;

	ld.shared.u32 	%r35, [%r11+1024];
	and.b32  	%r62, %r35, %r62;
	st.shared.u32 	[%r11], %r62;

BB27_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r24, 127;
	@%p6 bra 	BB27_10;

	ld.shared.u32 	%r37, [%r11+512];
	and.b32  	%r62, %r37, %r62;
	st.shared.u32 	[%r11], %r62;

BB27_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r24, 63;
	@%p7 bra 	BB27_12;

	ld.shared.u32 	%r39, [%r11+256];
	and.b32  	%r62, %r39, %r62;
	st.shared.u32 	[%r11], %r62;

BB27_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r24, 31;
	@%p8 bra 	BB27_15;

	ld.shared.u32 	%r41, [%r11+128];
	and.b32  	%r42, %r41, %r62;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p9, %r42, %r44, %r43, %r45;
	and.b32  	%r47, %r46, %r42;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p10, %r47, %r48, %r43, %r45;
	and.b32  	%r50, %r49, %r47;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p11, %r50, %r51, %r43, %r45;
	and.b32  	%r53, %r52, %r50;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p12, %r53, %r54, %r43, %r45;
	and.b32  	%r56, %r55, %r53;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p13, %r56, %r57, %r43, %r45;
	and.b32  	%r20, %r58, %r56;
	setp.ne.s32	%p14, %r24, 0;
	@%p14 bra 	BB27_15;

	cvta.to.global.u64 	%rd9, %rd2;
	mul.wide.u32 	%rd10, %r25, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %r20;

BB27_15:
	ret;
}

	// .globl	reduce_dot_f16
.visible .entry reduce_dot_f16(
	.param .u64 reduce_dot_f16_param_0,
	.param .u64 reduce_dot_f16_param_1,
	.param .u32 reduce_dot_f16_param_2,
	.param .u64 reduce_dot_f16_param_3
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<77>;
	.reg .f32 	%f<39>;
	.reg .b32 	%r<73>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [reduce_dot_f16_param_0];
	ld.param.u64 	%rd2, [reduce_dot_f16_param_1];
	ld.param.u32 	%r7, [reduce_dot_f16_param_2];
	ld.param.u64 	%rd3, [reduce_dot_f16_param_3];
	mov.u32 	%r9, %tid.x;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 11;
	add.s32 	%r72, %r11, %r9;
	mov.u32 	%r8, 0;
	// inline asm
	cvt.rn.f16.s32 %rs70, %r8;
	// inline asm
	setp.ge.u32	%p1, %r72, %r7;
	@%p1 bra 	BB28_4;

BB28_1:
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.u32 	%rd5, %r72, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs17, [%rd6];
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.u16 	%rs18, [%rd8];
	// inline asm
	{  cvt.f32.f16 %f1, %rs17;}

	// inline asm
	cvt.f64.f32	%fd1, %f1;
	// inline asm
	{  cvt.f32.f16 %f2, %rs18;}

	// inline asm
	cvt.f64.f32	%fd2, %f2;
	// inline asm
	{  cvt.f32.f16 %f3, %rs70;}

	// inline asm
	cvt.f64.f32	%fd3, %f3;
	fma.rn.f64 	%fd4, %fd1, %fd2, %fd3;
	cvt.rn.f32.f64	%f4, %fd4;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f4;}

	// inline asm
	add.s32 	%r3, %r72, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB28_3;

	mul.wide.u32 	%rd10, %r3, 2;
	add.s64 	%rd11, %rd4, %rd10;
	ld.global.u16 	%rs21, [%rd11];
	add.s64 	%rd13, %rd7, %rd10;
	ld.global.u16 	%rs22, [%rd13];
	// inline asm
	{  cvt.f32.f16 %f5, %rs21;}

	// inline asm
	cvt.f64.f32	%fd5, %f5;
	// inline asm
	{  cvt.f32.f16 %f6, %rs22;}

	// inline asm
	cvt.f64.f32	%fd6, %f6;
	// inline asm
	{  cvt.f32.f16 %f7, %rs70;}

	// inline asm
	cvt.f64.f32	%fd7, %f7;
	fma.rn.f64 	%fd8, %fd5, %fd6, %fd7;
	cvt.rn.f32.f64	%f8, %fd8;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f8;}

	// inline asm

BB28_3:
	mov.u32 	%r12, %nctaid.x;
	shl.b32 	%r13, %r12, 11;
	add.s32 	%r72, %r72, %r13;
	setp.lt.u32	%p3, %r72, %r7;
	@%p3 bra 	BB28_1;

BB28_4:
	shl.b32 	%r14, %r9, 1;
	mov.u32 	%r15, shared;
	add.s32 	%r6, %r15, %r14;
	st.shared.u16 	[%r6], %rs70;
	bar.sync 	0;
	setp.gt.u32	%p4, %r9, 511;
	@%p4 bra 	BB28_6;

	ld.shared.u16 	%rs26, [%r6+1024];
	// inline asm
	{  cvt.f32.f16 %f9, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f10, %rs26;}

	// inline asm
	add.f32 	%f11, %f9, %f10;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f11;}

	// inline asm
	st.shared.u16 	[%r6], %rs70;

BB28_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r9, 255;
	@%p5 bra 	BB28_8;

	ld.shared.u16 	%rs29, [%r6+512];
	// inline asm
	{  cvt.f32.f16 %f12, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f13, %rs29;}

	// inline asm
	add.f32 	%f14, %f12, %f13;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f14;}

	// inline asm
	st.shared.u16 	[%r6], %rs70;

BB28_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r9, 127;
	@%p6 bra 	BB28_10;

	ld.shared.u16 	%rs32, [%r6+256];
	// inline asm
	{  cvt.f32.f16 %f15, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f16, %rs32;}

	// inline asm
	add.f32 	%f17, %f15, %f16;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f17;}

	// inline asm
	st.shared.u16 	[%r6], %rs70;

BB28_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r9, 63;
	@%p7 bra 	BB28_12;

	ld.shared.u16 	%rs35, [%r6+128];
	// inline asm
	{  cvt.f32.f16 %f18, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f19, %rs35;}

	// inline asm
	add.f32 	%f20, %f18, %f19;
	// inline asm
	{  cvt.rn.f16.f32 %rs70, %f20;}

	// inline asm
	st.shared.u16 	[%r6], %rs70;

BB28_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r9, 31;
	@%p8 bra 	BB28_15;

	ld.shared.u16 	%rs38, [%r6+64];
	// inline asm
	{  cvt.f32.f16 %f21, %rs70;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f22, %rs38;}

	// inline asm
	add.f32 	%f23, %f21, %f22;
	// inline asm
	{  cvt.rn.f16.f32 %rs39, %f23;}

	// inline asm
	// inline asm
	{  mov.b32 %r20, {%rs39,%rs39};}

	// inline asm
	// inline asm
	{mov.u32 %r21, WARP_SZ;
}
	// inline asm
	shl.b32 	%r60, %r21, 8;
	mov.u32 	%r48, 8;
	add.s32 	%r61, %r60, -8192;
	or.b32  	%r25, %r61, 31;
	mov.u32 	%r24, 1;
	mov.u32 	%r58, -1;
	// inline asm
	{shfl.sync.bfly.b32 %r22,%r20,%r24,%r25,%r58;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r22;
 mov.b16 %rs42, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f24, %rs39;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f25, %rs42;}

	// inline asm
	add.f32 	%f26, %f24, %f25;
	// inline asm
	{  cvt.rn.f16.f32 %rs45, %f26;}

	// inline asm
	// inline asm
	{  mov.b32 %r28, {%rs45,%rs45};}

	// inline asm
	// inline asm
	{mov.u32 %r29, WARP_SZ;
}
	// inline asm
	shl.b32 	%r62, %r29, 8;
	add.s32 	%r63, %r62, -8192;
	or.b32  	%r33, %r63, 31;
	mov.u32 	%r32, 2;
	// inline asm
	{shfl.sync.bfly.b32 %r30,%r28,%r32,%r33,%r58;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r30;
 mov.b16 %rs48, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f27, %rs45;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f28, %rs48;}

	// inline asm
	add.f32 	%f29, %f27, %f28;
	// inline asm
	{  cvt.rn.f16.f32 %rs51, %f29;}

	// inline asm
	// inline asm
	{  mov.b32 %r36, {%rs51,%rs51};}

	// inline asm
	// inline asm
	{mov.u32 %r37, WARP_SZ;
}
	// inline asm
	shl.b32 	%r64, %r37, 8;
	add.s32 	%r65, %r64, -8192;
	or.b32  	%r41, %r65, 31;
	mov.u32 	%r40, 4;
	// inline asm
	{shfl.sync.bfly.b32 %r38,%r36,%r40,%r41,%r58;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r38;
 mov.b16 %rs54, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f30, %rs51;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f31, %rs54;}

	// inline asm
	add.f32 	%f32, %f30, %f31;
	// inline asm
	{  cvt.rn.f16.f32 %rs57, %f32;}

	// inline asm
	// inline asm
	{  mov.b32 %r44, {%rs57,%rs57};}

	// inline asm
	// inline asm
	{mov.u32 %r45, WARP_SZ;
}
	// inline asm
	shl.b32 	%r66, %r45, 8;
	add.s32 	%r67, %r66, -8192;
	or.b32  	%r49, %r67, 31;
	// inline asm
	{shfl.sync.bfly.b32 %r46,%r44,%r48,%r49,%r58;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r46;
 mov.b16 %rs60, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f33, %rs57;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f34, %rs60;}

	// inline asm
	add.f32 	%f35, %f33, %f34;
	// inline asm
	{  cvt.rn.f16.f32 %rs63, %f35;}

	// inline asm
	// inline asm
	{  mov.b32 %r52, {%rs63,%rs63};}

	// inline asm
	// inline asm
	{mov.u32 %r53, WARP_SZ;
}
	// inline asm
	shl.b32 	%r68, %r53, 8;
	add.s32 	%r69, %r68, -8192;
	or.b32  	%r57, %r69, 31;
	mov.u32 	%r56, 16;
	// inline asm
	{shfl.sync.bfly.b32 %r54,%r52,%r56,%r57,%r58;
}
	// inline asm
	// inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r54;
 mov.b16 %rs66, low;}
	// inline asm
	// inline asm
	{  cvt.f32.f16 %f36, %rs63;}

	// inline asm
	// inline asm
	{  cvt.f32.f16 %f37, %rs66;}

	// inline asm
	add.f32 	%f38, %f36, %f37;
	// inline asm
	{  cvt.rn.f16.f32 %rs69, %f38;}

	// inline asm
	setp.ne.s32	%p9, %r9, 0;
	@%p9 bra 	BB28_15;

	cvta.to.global.u64 	%rd14, %rd3;
	mul.wide.u32 	%rd15, %r10, 2;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u16 	[%rd16], %rs69;

BB28_15:
	ret;
}

	// .globl	reduce_dot_f32
.visible .entry reduce_dot_f32(
	.param .u64 reduce_dot_f32_param_0,
	.param .u64 reduce_dot_f32_param_1,
	.param .u32 reduce_dot_f32_param_2,
	.param .u64 reduce_dot_f32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [reduce_dot_f32_param_0];
	ld.param.u64 	%rd2, [reduce_dot_f32_param_1];
	ld.param.u32 	%r7, [reduce_dot_f32_param_2];
	ld.param.u64 	%rd3, [reduce_dot_f32_param_3];
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	add.s32 	%r38, %r10, %r8;
	mov.f32 	%f36, 0f00000000;
	setp.ge.u32	%p1, %r38, %r7;
	@%p1 bra 	BB29_4;

BB29_1:
	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.u32 	%rd5, %r38, 4;
	add.s64 	%rd6, %rd4, %rd5;
	cvta.to.global.u64 	%rd7, %rd2;
	add.s64 	%rd8, %rd7, %rd5;
	ld.global.f32 	%f17, [%rd8];
	ld.global.f32 	%f18, [%rd6];
	fma.rn.f32 	%f36, %f18, %f17, %f36;
	add.s32 	%r3, %r38, 1024;
	setp.ge.u32	%p2, %r3, %r7;
	@%p2 bra 	BB29_3;

	mul.wide.u32 	%rd10, %r3, 4;
	add.s64 	%rd11, %rd4, %rd10;
	add.s64 	%rd13, %rd7, %rd10;
	ld.global.f32 	%f19, [%rd13];
	ld.global.f32 	%f20, [%rd11];
	fma.rn.f32 	%f36, %f20, %f19, %f36;

BB29_3:
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r12, %r11, 11;
	add.s32 	%r38, %r38, %r12;
	setp.lt.u32	%p3, %r38, %r7;
	@%p3 bra 	BB29_1;

BB29_4:
	shl.b32 	%r13, %r8, 2;
	mov.u32 	%r14, shared;
	add.s32 	%r6, %r14, %r13;
	st.shared.f32 	[%r6], %f36;
	bar.sync 	0;
	setp.gt.u32	%p4, %r8, 511;
	@%p4 bra 	BB29_6;

	ld.shared.f32 	%f21, [%r6+2048];
	add.f32 	%f36, %f36, %f21;
	st.shared.f32 	[%r6], %f36;

BB29_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r8, 255;
	@%p5 bra 	BB29_8;

	ld.shared.f32 	%f22, [%r6+1024];
	add.f32 	%f36, %f36, %f22;
	st.shared.f32 	[%r6], %f36;

BB29_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r8, 127;
	@%p6 bra 	BB29_10;

	ld.shared.f32 	%f23, [%r6+512];
	add.f32 	%f36, %f36, %f23;
	st.shared.f32 	[%r6], %f36;

BB29_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r8, 63;
	@%p7 bra 	BB29_12;

	ld.shared.f32 	%f24, [%r6+256];
	add.f32 	%f36, %f36, %f24;
	st.shared.f32 	[%r6], %f36;

BB29_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r8, 31;
	@%p8 bra 	BB29_15;

	ld.shared.f32 	%f25, [%r6+128];
	add.f32 	%f26, %f36, %f25;
	mov.b32 	 %r19, %f26;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p9, %r19, %r21, %r20, %r22;
	mov.b32 	 %f27, %r23;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	 %r24, %f28;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p10, %r24, %r25, %r20, %r22;
	mov.b32 	 %f29, %r26;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	 %r27, %f30;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r28, %r20, %r22;
	mov.b32 	 %f31, %r29;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	 %r30, %f32;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p12, %r30, %r31, %r20, %r22;
	mov.b32 	 %f33, %r32;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	 %r33, %f34;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p13, %r33, %r34, %r20, %r22;
	mov.b32 	 %f35, %r35;
	add.f32 	%f14, %f34, %f35;
	setp.ne.s32	%p14, %r8, 0;
	@%p14 bra 	BB29_15;

	cvta.to.global.u64 	%rd14, %rd3;
	mul.wide.u32 	%rd15, %r9, 4;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.f32 	[%rd16], %f14;

BB29_15:
	ret;
}

	// .globl	reduce_dot_f64
.visible .entry reduce_dot_f64(
	.param .u64 reduce_dot_f64_param_0,
	.param .u64 reduce_dot_f64_param_1,
	.param .u32 reduce_dot_f64_param_2,
	.param .u64 reduce_dot_f64_param_3
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<43>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd4, [reduce_dot_f64_param_0];
	ld.param.u64 	%rd5, [reduce_dot_f64_param_1];
	ld.param.u32 	%r9, [reduce_dot_f64_param_2];
	ld.param.u64 	%rd3, [reduce_dot_f64_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;
	mov.f64 	%fd36, 0d0000000000000000;
	setp.ge.u32	%p1, %r41, %r9;
	@%p1 bra 	BB30_4;

BB30_1:
	mul.wide.u32 	%rd6, %r41, 8;
	add.s64 	%rd7, %rd2, %rd6;
	add.s64 	%rd8, %rd1, %rd6;
	ld.global.f64 	%fd17, [%rd8];
	ld.global.f64 	%fd18, [%rd7];
	fma.rn.f64 	%fd36, %fd18, %fd17, %fd36;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32	%p2, %r6, %r9;
	@%p2 bra 	BB30_3;

	mul.wide.u32 	%rd9, %r6, 8;
	add.s64 	%rd10, %rd2, %rd9;
	add.s64 	%rd11, %rd1, %rd9;
	ld.global.f64 	%fd19, [%rd11];
	ld.global.f64 	%fd20, [%rd10];
	fma.rn.f64 	%fd36, %fd20, %fd19, %fd36;

BB30_3:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32	%p3, %r41, %r9;
	@%p3 bra 	BB30_1;

BB30_4:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd36;
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 511;
	@%p4 bra 	BB30_6;

	ld.shared.f64 	%fd21, [%r8+4096];
	add.f64 	%fd36, %fd36, %fd21;
	st.shared.f64 	[%r8], %fd36;

BB30_6:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 255;
	@%p5 bra 	BB30_8;

	ld.shared.f64 	%fd22, [%r8+2048];
	add.f64 	%fd36, %fd36, %fd22;
	st.shared.f64 	[%r8], %fd36;

BB30_8:
	bar.sync 	0;
	setp.gt.u32	%p6, %r2, 127;
	@%p6 bra 	BB30_10;

	ld.shared.f64 	%fd23, [%r8+1024];
	add.f64 	%fd36, %fd36, %fd23;
	st.shared.f64 	[%r8], %fd36;

BB30_10:
	bar.sync 	0;
	setp.gt.u32	%p7, %r2, 63;
	@%p7 bra 	BB30_12;

	ld.shared.f64 	%fd24, [%r8+512];
	add.f64 	%fd36, %fd36, %fd24;
	st.shared.f64 	[%r8], %fd36;

BB30_12:
	bar.sync 	0;
	setp.gt.u32	%p8, %r2, 31;
	@%p8 bra 	BB30_15;

	ld.shared.f64 	%fd35, [%r8+256];
	add.f64 	%fd25, %fd36, %fd35;
	// inline asm
	mov.b64 {%r14,%r15}, %fd25;
	// inline asm
	mov.u32 	%r34, 31;
	mov.u32 	%r35, 1;
	mov.u32 	%r36, -1;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r35, %r34, %r36;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r35, %r34, %r36;
	// inline asm
	mov.b64 %fd26, {%r16,%r17};
	// inline asm
	add.f64 	%fd27, %fd25, %fd26;
	// inline asm
	mov.b64 {%r18,%r19}, %fd27;
	// inline asm
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r37, %r34, %r36;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r37, %r34, %r36;
	// inline asm
	mov.b64 %fd28, {%r20,%r21};
	// inline asm
	add.f64 	%fd29, %fd27, %fd28;
	// inline asm
	mov.b64 {%r22,%r23}, %fd29;
	// inline asm
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r38, %r34, %r36;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r38, %r34, %r36;
	// inline asm
	mov.b64 %fd30, {%r24,%r25};
	// inline asm
	add.f64 	%fd31, %fd29, %fd30;
	// inline asm
	mov.b64 {%r26,%r27}, %fd31;
	// inline asm
	mov.u32 	%r39, 8;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r39, %r34, %r36;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r39, %r34, %r36;
	// inline asm
	mov.b64 %fd32, {%r28,%r29};
	// inline asm
	add.f64 	%fd33, %fd31, %fd32;
	// inline asm
	mov.b64 {%r30,%r31}, %fd33;
	// inline asm
	mov.u32 	%r40, 16;
	shfl.sync.bfly.b32 	%r33|%p17, %r31, %r40, %r34, %r36;
	shfl.sync.bfly.b32 	%r32|%p18, %r30, %r40, %r34, %r36;
	// inline asm
	mov.b64 %fd34, {%r32,%r33};
	// inline asm
	add.f64 	%fd14, %fd33, %fd34;
	setp.ne.s32	%p19, %r2, 0;
	@%p19 bra 	BB30_15;

	cvta.to.global.u64 	%rd12, %rd3;
	mul.wide.u32 	%rd13, %r1, 8;
	add.s64 	%rd14, %rd12, %rd13;
	st.global.f64 	[%rd14], %fd14;

BB30_15:
	ret;
}

	// .globl	prefix_sum_large_init
.visible .entry prefix_sum_large_init(
	.param .u64 prefix_sum_large_init_param_0,
	.param .u32 prefix_sum_large_init_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [prefix_sum_large_init_param_0];
	ld.param.u32 	%r6, [prefix_sum_large_init_param_1];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.u32	%p1, %r10, %r6;
	@%p1 bra 	BB31_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

BB31_2:
	setp.lt.u32	%p2, %r10, 32;
	selp.b64	%rd3, 2, 0, %p2;
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r3, %r10;
	setp.lt.u32	%p3, %r10, %r6;
	@%p3 bra 	BB31_2;

BB31_3:
	ret;
}

	// .globl	prefix_sum_exc_small_u32
.visible .entry prefix_sum_exc_small_u32(
	.param .u64 prefix_sum_exc_small_u32_param_0,
	.param .u64 prefix_sum_exc_small_u32_param_1,
	.param .u32 prefix_sum_exc_small_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_exc_small_u32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_exc_small_u32_param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r13, %tid.x;
	cvt.u64.u32	%rd1, %r13;
	mul.wide.u32 	%rd5, %r13, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u32 	{%r14, %r15, %r16, %r17}, [%rd6];
	mov.u32 	%r18, 0;
	add.s32 	%r2, %r15, %r14;
	add.s32 	%r3, %r16, %r2;
	add.s32 	%r4, %r17, %r3;
	shl.b32 	%r22, %r13, 2;
	mov.u32 	%r23, shared;
	add.s32 	%r24, %r23, %r22;
	st.shared.u32 	[%r24], %r18;
	mov.u32 	%r5, %ntid.x;
	add.s32 	%r6, %r5, %r13;
	setp.lt.u32	%p1, %r5, 2;
	mov.u32 	%r40, %r4;
	@%p1 bra 	BB32_3;

	shl.b32 	%r26, %r6, 2;
	add.s32 	%r7, %r23, %r26;
	mov.u32 	%r38, 1;
	mov.u32 	%r40, %r4;

BB32_2:
	st.shared.u32 	[%r7], %r40;
	bar.sync 	0;
	sub.s32 	%r28, %r6, %r38;
	shl.b32 	%r29, %r28, 2;
	add.s32 	%r31, %r23, %r29;
	ld.shared.u32 	%r32, [%r31];
	ld.shared.u32 	%r33, [%r7];
	add.s32 	%r40, %r32, %r33;
	bar.sync 	0;
	shl.b32 	%r38, %r38, 1;
	setp.lt.u32	%p2, %r38, %r5;
	@%p2 bra 	BB32_2;

BB32_3:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	sub.s32 	%r34, %r40, %r4;
	add.s32 	%r35, %r34, %r3;
	add.s32 	%r36, %r34, %r2;
	add.s32 	%r37, %r34, %r14;
	st.global.v4.u32 	[%rd9], {%r34, %r37, %r36, %r35};
	ret;
}

	// .globl	prefix_sum_exc_small_u64
.visible .entry prefix_sum_exc_small_u64(
	.param .u64 prefix_sum_exc_small_u64_param_0,
	.param .u64 prefix_sum_exc_small_u64_param_1,
	.param .u32 prefix_sum_exc_small_u64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd8, [prefix_sum_exc_small_u64_param_0];
	ld.param.u64 	%rd7, [prefix_sum_exc_small_u64_param_1];
	cvta.to.global.u64 	%rd9, %rd8;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32	%rd1, %r6;
	mul.wide.u32 	%rd10, %r6, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v2.u64 	{%rd12, %rd13}, [%rd11];
	add.s64 	%rd3, %rd13, %rd12;
	shl.b32 	%r7, %r6, 3;
	mov.u32 	%r8, shared;
	add.s32 	%r9, %r8, %r7;
	mov.u64 	%rd15, 0;
	st.shared.u64 	[%r9], %rd15;
	mov.u32 	%r1, %ntid.x;
	add.s32 	%r2, %r1, %r6;
	setp.lt.u32	%p1, %r1, 2;
	mov.u64 	%rd24, %rd3;
	@%p1 bra 	BB33_3;

	shl.b32 	%r11, %r2, 3;
	add.s32 	%r3, %r8, %r11;
	mov.u32 	%r17, 1;
	mov.u64 	%rd24, %rd3;

BB33_2:
	st.shared.u64 	[%r3], %rd24;
	bar.sync 	0;
	sub.s32 	%r13, %r2, %r17;
	shl.b32 	%r14, %r13, 3;
	add.s32 	%r16, %r8, %r14;
	ld.shared.u64 	%rd16, [%r16];
	ld.shared.u64 	%rd17, [%r3];
	add.s64 	%rd24, %rd16, %rd17;
	bar.sync 	0;
	shl.b32 	%r17, %r17, 1;
	setp.lt.u32	%p2, %r17, %r1;
	@%p2 bra 	BB33_2;

BB33_3:
	cvta.to.global.u64 	%rd18, %rd7;
	shl.b64 	%rd19, %rd1, 4;
	add.s64 	%rd20, %rd18, %rd19;
	sub.s64 	%rd21, %rd24, %rd3;
	add.s64 	%rd22, %rd21, %rd12;
	st.global.v2.u64 	[%rd20], {%rd21, %rd22};
	ret;
}

	// .globl	prefix_sum_exc_small_f32
.visible .entry prefix_sum_exc_small_f32(
	.param .u64 prefix_sum_exc_small_f32_param_0,
	.param .u64 prefix_sum_exc_small_f32_param_1,
	.param .u32 prefix_sum_exc_small_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [prefix_sum_exc_small_f32_param_0];
	ld.param.u64 	%rd1, [prefix_sum_exc_small_f32_param_1];
	ld.param.u32 	%r6, [prefix_sum_exc_small_f32_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r7, %tid.x;
	mul.wide.u32 	%rd4, %r7, 16;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd5];
	shl.b32 	%r8, %r7, 2;
	setp.lt.u32	%p1, %r8, %r6;
	add.f32 	%f16, %f8, 0f00000000;
	selp.f32	%f1, %f16, 0f00000000, %p1;
	add.s32 	%r9, %r8, 1;
	setp.lt.u32	%p2, %r9, %r6;
	selp.f32	%f17, %f9, 0f00000000, %p2;
	add.f32 	%f2, %f1, %f17;
	add.s32 	%r10, %r8, 2;
	setp.lt.u32	%p3, %r10, %r6;
	selp.f32	%f18, %f10, 0f00000000, %p3;
	add.f32 	%f3, %f2, %f18;
	add.s32 	%r11, %r8, 3;
	setp.lt.u32	%p4, %r11, %r6;
	selp.f32	%f19, %f11, 0f00000000, %p4;
	add.f32 	%f4, %f3, %f19;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r8;
	mov.u32 	%r14, 0;
	st.shared.u32 	[%r13], %r14;
	mov.u32 	%r15, %ntid.x;
	setp.lt.u32	%p5, %r15, 2;
	mov.f32 	%f28, %f4;
	@%p5 bra 	BB34_3;

	add.s32 	%r19, %r15, %r7;
	shl.b32 	%r20, %r19, 2;
	add.s32 	%r1, %r12, %r20;
	mov.u32 	%r29, 1;
	mov.f32 	%f28, %f4;

BB34_2:
	st.shared.f32 	[%r1], %f28;
	bar.sync 	0;
	sub.s32 	%r24, %r19, %r29;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r27, %r12, %r25;
	ld.shared.f32 	%f20, [%r27];
	ld.shared.f32 	%f21, [%r1];
	add.f32 	%f28, %f21, %f20;
	bar.sync 	0;
	shl.b32 	%r29, %r29, 1;
	setp.lt.u32	%p6, %r29, %r15;
	@%p6 bra 	BB34_2;

BB34_3:
	sub.f32 	%f22, %f28, %f4;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	add.f32 	%f23, %f22, %f3;
	add.f32 	%f24, %f22, %f2;
	add.f32 	%f25, %f22, %f1;
	add.f32 	%f26, %f22, 0f00000000;
	st.global.v4.f32 	[%rd8], {%f26, %f25, %f24, %f23};
	ret;
}

	// .globl	prefix_sum_exc_small_f64
.visible .entry prefix_sum_exc_small_f64(
	.param .u64 prefix_sum_exc_small_f64_param_0,
	.param .u64 prefix_sum_exc_small_f64_param_1,
	.param .u32 prefix_sum_exc_small_f64_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [prefix_sum_exc_small_f64_param_0];
	ld.param.u64 	%rd2, [prefix_sum_exc_small_f64_param_1];
	ld.param.u32 	%r6, [prefix_sum_exc_small_f64_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32	%rd1, %r7;
	mul.wide.u32 	%rd5, %r7, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd6];
	shl.b32 	%r8, %r7, 1;
	setp.lt.u32	%p1, %r8, %r6;
	add.f64 	%fd10, %fd6, 0d0000000000000000;
	selp.f64	%fd1, %fd10, 0d0000000000000000, %p1;
	add.s32 	%r9, %r8, 1;
	setp.lt.u32	%p2, %r9, %r6;
	selp.f64	%fd11, %fd7, 0d0000000000000000, %p2;
	add.f64 	%fd2, %fd1, %fd11;
	shl.b32 	%r10, %r7, 3;
	mov.u32 	%r11, shared_d;
	add.s32 	%r12, %r11, %r10;
	mov.u64 	%rd7, 0;
	st.shared.u64 	[%r12], %rd7;
	mov.u32 	%r1, %ntid.x;
	add.s32 	%r2, %r1, %r7;
	setp.lt.u32	%p3, %r1, 2;
	mov.f64 	%fd18, %fd2;
	@%p3 bra 	BB35_3;

	shl.b32 	%r14, %r2, 3;
	add.s32 	%r3, %r11, %r14;
	mov.u32 	%r20, 1;
	mov.f64 	%fd18, %fd2;

BB35_2:
	st.shared.f64 	[%r3], %fd18;
	bar.sync 	0;
	sub.s32 	%r16, %r2, %r20;
	shl.b32 	%r17, %r16, 3;
	add.s32 	%r19, %r11, %r17;
	ld.shared.f64 	%fd12, [%r19];
	ld.shared.f64 	%fd13, [%r3];
	add.f64 	%fd18, %fd13, %fd12;
	bar.sync 	0;
	shl.b32 	%r20, %r20, 1;
	setp.lt.u32	%p4, %r20, %r1;
	@%p4 bra 	BB35_2;

BB35_3:
	sub.f64 	%fd14, %fd18, %fd2;
	cvta.to.global.u64 	%rd8, %rd2;
	shl.b64 	%rd9, %rd1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	add.f64 	%fd15, %fd14, %fd1;
	add.f64 	%fd16, %fd14, 0d0000000000000000;
	st.global.v2.f64 	[%rd10], {%fd16, %fd15};
	ret;
}

	// .globl	prefix_sum_inc_small_u32
.visible .entry prefix_sum_inc_small_u32(
	.param .u64 prefix_sum_inc_small_u32_param_0,
	.param .u64 prefix_sum_inc_small_u32_param_1,
	.param .u32 prefix_sum_inc_small_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_inc_small_u32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_inc_small_u32_param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r13, %tid.x;
	cvt.u64.u32	%rd1, %r13;
	mul.wide.u32 	%rd5, %r13, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u32 	{%r14, %r15, %r16, %r17}, [%rd6];
	mov.u32 	%r18, 0;
	add.s32 	%r2, %r15, %r14;
	add.s32 	%r3, %r16, %r2;
	add.s32 	%r4, %r17, %r3;
	shl.b32 	%r22, %r13, 2;
	mov.u32 	%r23, shared;
	add.s32 	%r24, %r23, %r22;
	st.shared.u32 	[%r24], %r18;
	mov.u32 	%r5, %ntid.x;
	add.s32 	%r6, %r5, %r13;
	setp.lt.u32	%p1, %r5, 2;
	mov.u32 	%r40, %r4;
	@%p1 bra 	BB36_3;

	shl.b32 	%r26, %r6, 2;
	add.s32 	%r7, %r23, %r26;
	mov.u32 	%r38, 1;
	mov.u32 	%r40, %r4;

BB36_2:
	st.shared.u32 	[%r7], %r40;
	bar.sync 	0;
	sub.s32 	%r28, %r6, %r38;
	shl.b32 	%r29, %r28, 2;
	add.s32 	%r31, %r23, %r29;
	ld.shared.u32 	%r32, [%r31];
	ld.shared.u32 	%r33, [%r7];
	add.s32 	%r40, %r32, %r33;
	bar.sync 	0;
	shl.b32 	%r38, %r38, 1;
	setp.lt.u32	%p2, %r38, %r5;
	@%p2 bra 	BB36_2;

BB36_3:
	sub.s32 	%r34, %r40, %r4;
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	add.s32 	%r35, %r34, %r3;
	add.s32 	%r36, %r34, %r2;
	add.s32 	%r37, %r34, %r14;
	st.global.v4.u32 	[%rd9], {%r37, %r36, %r35, %r40};
	ret;
}

	// .globl	prefix_sum_inc_small_u64
.visible .entry prefix_sum_inc_small_u64(
	.param .u64 prefix_sum_inc_small_u64_param_0,
	.param .u64 prefix_sum_inc_small_u64_param_1,
	.param .u32 prefix_sum_inc_small_u64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd8, [prefix_sum_inc_small_u64_param_0];
	ld.param.u64 	%rd7, [prefix_sum_inc_small_u64_param_1];
	cvta.to.global.u64 	%rd9, %rd8;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32	%rd1, %r6;
	mul.wide.u32 	%rd10, %r6, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v2.u64 	{%rd12, %rd13}, [%rd11];
	add.s64 	%rd3, %rd13, %rd12;
	shl.b32 	%r7, %r6, 3;
	mov.u32 	%r8, shared;
	add.s32 	%r9, %r8, %r7;
	mov.u64 	%rd15, 0;
	st.shared.u64 	[%r9], %rd15;
	mov.u32 	%r1, %ntid.x;
	add.s32 	%r2, %r1, %r6;
	setp.lt.u32	%p1, %r1, 2;
	mov.u64 	%rd24, %rd3;
	@%p1 bra 	BB37_3;

	shl.b32 	%r11, %r2, 3;
	add.s32 	%r3, %r8, %r11;
	mov.u32 	%r17, 1;
	mov.u64 	%rd24, %rd3;

BB37_2:
	st.shared.u64 	[%r3], %rd24;
	bar.sync 	0;
	sub.s32 	%r13, %r2, %r17;
	shl.b32 	%r14, %r13, 3;
	add.s32 	%r16, %r8, %r14;
	ld.shared.u64 	%rd16, [%r16];
	ld.shared.u64 	%rd17, [%r3];
	add.s64 	%rd24, %rd16, %rd17;
	bar.sync 	0;
	shl.b32 	%r17, %r17, 1;
	setp.lt.u32	%p2, %r17, %r1;
	@%p2 bra 	BB37_2;

BB37_3:
	sub.s64 	%rd18, %rd24, %rd3;
	cvta.to.global.u64 	%rd19, %rd7;
	shl.b64 	%rd20, %rd1, 4;
	add.s64 	%rd21, %rd19, %rd20;
	add.s64 	%rd22, %rd18, %rd12;
	st.global.v2.u64 	[%rd21], {%rd22, %rd24};
	ret;
}

	// .globl	prefix_sum_inc_small_f32
.visible .entry prefix_sum_inc_small_f32(
	.param .u64 prefix_sum_inc_small_f32_param_0,
	.param .u64 prefix_sum_inc_small_f32_param_1,
	.param .u32 prefix_sum_inc_small_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [prefix_sum_inc_small_f32_param_0];
	ld.param.u64 	%rd1, [prefix_sum_inc_small_f32_param_1];
	ld.param.u32 	%r6, [prefix_sum_inc_small_f32_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r7, %tid.x;
	mul.wide.u32 	%rd4, %r7, 16;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd5];
	shl.b32 	%r8, %r7, 2;
	setp.lt.u32	%p1, %r8, %r6;
	add.f32 	%f16, %f8, 0f00000000;
	selp.f32	%f1, %f16, 0f00000000, %p1;
	add.s32 	%r9, %r8, 1;
	setp.lt.u32	%p2, %r9, %r6;
	selp.f32	%f17, %f9, 0f00000000, %p2;
	add.f32 	%f2, %f1, %f17;
	add.s32 	%r10, %r8, 2;
	setp.lt.u32	%p3, %r10, %r6;
	selp.f32	%f18, %f10, 0f00000000, %p3;
	add.f32 	%f3, %f2, %f18;
	add.s32 	%r11, %r8, 3;
	setp.lt.u32	%p4, %r11, %r6;
	selp.f32	%f19, %f11, 0f00000000, %p4;
	add.f32 	%f4, %f3, %f19;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r8;
	mov.u32 	%r14, 0;
	st.shared.u32 	[%r13], %r14;
	mov.u32 	%r15, %ntid.x;
	setp.lt.u32	%p5, %r15, 2;
	mov.f32 	%f28, %f4;
	@%p5 bra 	BB38_3;

	add.s32 	%r19, %r15, %r7;
	shl.b32 	%r20, %r19, 2;
	add.s32 	%r1, %r12, %r20;
	mov.u32 	%r29, 1;
	mov.f32 	%f28, %f4;

BB38_2:
	st.shared.f32 	[%r1], %f28;
	bar.sync 	0;
	sub.s32 	%r24, %r19, %r29;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r27, %r12, %r25;
	ld.shared.f32 	%f20, [%r27];
	ld.shared.f32 	%f21, [%r1];
	add.f32 	%f28, %f21, %f20;
	bar.sync 	0;
	shl.b32 	%r29, %r29, 1;
	setp.lt.u32	%p6, %r29, %r15;
	@%p6 bra 	BB38_2;

BB38_3:
	sub.f32 	%f22, %f28, %f4;
	cvta.to.global.u64 	%rd6, %rd1;
	add.s64 	%rd8, %rd6, %rd4;
	add.f32 	%f23, %f22, %f4;
	add.f32 	%f24, %f22, %f3;
	add.f32 	%f25, %f22, %f2;
	add.f32 	%f26, %f22, %f1;
	st.global.v4.f32 	[%rd8], {%f26, %f25, %f24, %f23};
	ret;
}

	// .globl	prefix_sum_inc_small_f64
.visible .entry prefix_sum_inc_small_f64(
	.param .u64 prefix_sum_inc_small_f64_param_0,
	.param .u64 prefix_sum_inc_small_f64_param_1,
	.param .u32 prefix_sum_inc_small_f64_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [prefix_sum_inc_small_f64_param_0];
	ld.param.u64 	%rd2, [prefix_sum_inc_small_f64_param_1];
	ld.param.u32 	%r6, [prefix_sum_inc_small_f64_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32	%rd1, %r7;
	mul.wide.u32 	%rd5, %r7, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd6];
	shl.b32 	%r8, %r7, 1;
	setp.lt.u32	%p1, %r8, %r6;
	add.f64 	%fd10, %fd6, 0d0000000000000000;
	selp.f64	%fd1, %fd10, 0d0000000000000000, %p1;
	add.s32 	%r9, %r8, 1;
	setp.lt.u32	%p2, %r9, %r6;
	selp.f64	%fd11, %fd7, 0d0000000000000000, %p2;
	add.f64 	%fd2, %fd1, %fd11;
	shl.b32 	%r10, %r7, 3;
	mov.u32 	%r11, shared_d;
	add.s32 	%r12, %r11, %r10;
	mov.u64 	%rd7, 0;
	st.shared.u64 	[%r12], %rd7;
	mov.u32 	%r1, %ntid.x;
	add.s32 	%r2, %r1, %r7;
	setp.lt.u32	%p3, %r1, 2;
	mov.f64 	%fd18, %fd2;
	@%p3 bra 	BB39_3;

	shl.b32 	%r14, %r2, 3;
	add.s32 	%r3, %r11, %r14;
	mov.u32 	%r20, 1;
	mov.f64 	%fd18, %fd2;

BB39_2:
	st.shared.f64 	[%r3], %fd18;
	bar.sync 	0;
	sub.s32 	%r16, %r2, %r20;
	shl.b32 	%r17, %r16, 3;
	add.s32 	%r19, %r11, %r17;
	ld.shared.f64 	%fd12, [%r19];
	ld.shared.f64 	%fd13, [%r3];
	add.f64 	%fd18, %fd13, %fd12;
	bar.sync 	0;
	shl.b32 	%r20, %r20, 1;
	setp.lt.u32	%p4, %r20, %r1;
	@%p4 bra 	BB39_2;

BB39_3:
	sub.f64 	%fd14, %fd18, %fd2;
	cvta.to.global.u64 	%rd8, %rd2;
	shl.b64 	%rd9, %rd1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	add.f64 	%fd15, %fd14, %fd2;
	add.f64 	%fd16, %fd14, %fd1;
	st.global.v2.f64 	[%rd10], {%fd16, %fd15};
	ret;
}

	// .globl	prefix_sum_exc_large_u32
.visible .entry prefix_sum_exc_large_u32(
	.param .u64 prefix_sum_exc_large_u32_param_0,
	.param .u64 prefix_sum_exc_large_u32_param_1,
	.param .u32 prefix_sum_exc_large_u32_param_2,
	.param .u64 prefix_sum_exc_large_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<202>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd6, [prefix_sum_exc_large_u32_param_0];
	ld.param.u64 	%rd4, [prefix_sum_exc_large_u32_param_1];
	ld.param.u64 	%rd5, [prefix_sum_exc_large_u32_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r39, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r40, %r39, %r2;
	cvt.u64.u32	%rd1, %r40;
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r40, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u32 	{%r41, %r42, %r43, %r44}, [%rd9];
	ld.global.v4.u32 	{%r49, %r50, %r51, %r52}, [%rd9+2048];
	ld.global.v4.u32 	{%r57, %r58, %r59, %r60}, [%rd9+4096];
	ld.global.v4.u32 	{%r65, %r66, %r67, %r68}, [%rd9+6144];
	shl.b32 	%r73, %r2, 4;
	mov.u32 	%r74, shared;
	add.s32 	%r3, %r74, %r73;
	st.shared.v4.u32 	[%r3], {%r41, %r42, %r43, %r44};
	st.shared.v4.u32 	[%r3+2048], {%r49, %r50, %r51, %r52};
	st.shared.v4.u32 	[%r3+4096], {%r57, %r58, %r59, %r60};
	st.shared.v4.u32 	[%r3+6144], {%r65, %r66, %r67, %r68};
	bar.sync 	0;
	shl.b32 	%r75, %r2, 6;
	add.s32 	%r4, %r74, %r75;
	ld.shared.v4.u32 	{%r77, %r78, %r79, %r80}, [%r4];
	ld.shared.v4.u32 	{%r84, %r85, %r86, %r87}, [%r4+16];
	ld.shared.v4.u32 	{%r92, %r93, %r94, %r95}, [%r4+32];
	ld.shared.v4.u32 	{%r100, %r101, %r102, %r103}, [%r4+48];
	add.s32 	%r6, %r78, %r77;
	add.s32 	%r7, %r79, %r6;
	add.s32 	%r8, %r80, %r7;
	add.s32 	%r9, %r84, %r8;
	add.s32 	%r10, %r85, %r9;
	add.s32 	%r11, %r86, %r10;
	add.s32 	%r12, %r87, %r11;
	add.s32 	%r13, %r92, %r12;
	add.s32 	%r14, %r93, %r13;
	add.s32 	%r15, %r94, %r14;
	add.s32 	%r16, %r95, %r15;
	add.s32 	%r17, %r100, %r16;
	add.s32 	%r18, %r101, %r17;
	add.s32 	%r19, %r102, %r18;
	add.s32 	%r20, %r103, %r19;
	bar.sync 	0;
	shl.b32 	%r108, %r2, 2;
	add.s32 	%r21, %r74, %r108;
	mov.u32 	%r201, 0;
	st.shared.u32 	[%r21], %r201;
	st.shared.u32 	[%r21+512], %r20;
	bar.sync 	0;
	ld.shared.u32 	%r111, [%r21+508];
	ld.shared.u32 	%r112, [%r21+512];
	add.s32 	%r22, %r111, %r112;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r22;
	bar.sync 	0;
	ld.shared.u32 	%r113, [%r21+504];
	ld.shared.u32 	%r114, [%r21+512];
	add.s32 	%r23, %r113, %r114;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r23;
	bar.sync 	0;
	ld.shared.u32 	%r115, [%r21+496];
	ld.shared.u32 	%r116, [%r21+512];
	add.s32 	%r24, %r115, %r116;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r24;
	bar.sync 	0;
	ld.shared.u32 	%r117, [%r21+480];
	ld.shared.u32 	%r118, [%r21+512];
	add.s32 	%r25, %r117, %r118;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r25;
	bar.sync 	0;
	ld.shared.u32 	%r119, [%r21+448];
	ld.shared.u32 	%r120, [%r21+512];
	add.s32 	%r26, %r119, %r120;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r121, [%r21+384];
	ld.shared.u32 	%r122, [%r21+512];
	add.s32 	%r27, %r121, %r122;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r123, [%r21+256];
	ld.shared.u32 	%r124, [%r21+512];
	add.s32 	%r28, %r123, %r124;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd10, %r1, 8;
	add.s64 	%rd3, %rd5, %rd10;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB40_2;

	cvt.u64.u32	%rd13, %r28;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd12, %rd14, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd12;
	// inline asm

BB40_2:
	and.b32  	%r29, %r2, 31;
	add.s32 	%r200, %r29, -32;
	bra.uni 	BB40_3;

BB40_8:
	add.s32 	%r201, %r34, %r201;
	add.s32 	%r200, %r200, -32;

BB40_3:
	cvt.s64.s32	%rd17, %r200;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 3;
	add.s64 	%rd16, %rd5, %rd19;
	// inline asm
	ld.cg.u64 %rd15, [%rd16];
	// inline asm
	cvt.u32.u64	%r33, %rd15;
	shr.u64 	%rd20, %rd15, 32;
	cvt.u32.u64	%r34, %rd20;
	setp.eq.s32	%p2, %r33, 0;
	mov.u32 	%r126, -1;
	vote.sync.any.pred 	%p3, %p2, %r126;
	@%p3 bra 	BB40_3;

	setp.eq.s32	%p4, %r33, 2;
	vote.sync.ballot.b32 	%r35, %p4, %r126;
	setp.eq.s32	%p6, %r35, 0;
	@%p6 bra 	BB40_8;

	clz.b32 	%r129, %r35;
	mov.u32 	%r130, 31;
	sub.s32 	%r131, %r130, %r129;
	setp.lt.u32	%p7, %r29, %r131;
	selp.b32	%r132, 0, %r34, %p7;
	mov.u32 	%r133, 0;
	add.s32 	%r134, %r132, %r201;
	mov.u32 	%r135, 2;
	mov.u32 	%r136, 16;
	shfl.sync.down.b32 	%r138|%p8, %r134, %r136, %r130, %r126;
	add.s32 	%r139, %r138, %r134;
	mov.u32 	%r140, 8;
	shfl.sync.down.b32 	%r141|%p9, %r139, %r140, %r130, %r126;
	add.s32 	%r142, %r141, %r139;
	mov.u32 	%r143, 4;
	shfl.sync.down.b32 	%r144|%p10, %r142, %r143, %r130, %r126;
	add.s32 	%r145, %r144, %r142;
	shfl.sync.down.b32 	%r146|%p11, %r145, %r135, %r130, %r126;
	add.s32 	%r147, %r146, %r145;
	mov.u32 	%r148, 1;
	shfl.sync.down.b32 	%r149|%p12, %r147, %r148, %r130, %r126;
	add.s32 	%r150, %r149, %r147;
	shfl.sync.idx.b32 	%r151|%p13, %r150, %r133, %r130, %r126;
	add.s32 	%r36, %r151, %r28;
	@%p1 bra 	BB40_7;

	cvt.u64.u32	%rd23, %r36;
	shl.b64 	%rd24, %rd23, 32;
	or.b64  	%rd22, %rd24, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd22;
	// inline asm

BB40_7:
	sub.s32 	%r152, %r36, %r20;
	add.s32 	%r153, %r152, %r7;
	add.s32 	%r154, %r152, %r6;
	add.s32 	%r155, %r152, %r77;
	st.shared.v4.u32 	[%r4], {%r152, %r155, %r154, %r153};
	add.s32 	%r156, %r152, %r11;
	add.s32 	%r157, %r152, %r10;
	add.s32 	%r158, %r152, %r9;
	add.s32 	%r159, %r152, %r8;
	st.shared.v4.u32 	[%r4+16], {%r159, %r158, %r157, %r156};
	add.s32 	%r160, %r152, %r15;
	add.s32 	%r161, %r152, %r14;
	add.s32 	%r162, %r152, %r13;
	add.s32 	%r163, %r152, %r12;
	st.shared.v4.u32 	[%r4+32], {%r163, %r162, %r161, %r160};
	add.s32 	%r164, %r152, %r19;
	add.s32 	%r165, %r152, %r18;
	add.s32 	%r166, %r152, %r17;
	add.s32 	%r167, %r152, %r16;
	st.shared.v4.u32 	[%r4+48], {%r167, %r166, %r165, %r164};
	bar.sync 	0;
	cvta.to.global.u64 	%rd25, %rd4;
	shl.b64 	%rd26, %rd1, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.shared.v4.u32 	{%r168, %r169, %r170, %r171}, [%r3];
	st.global.v4.u32 	[%rd27], {%r168, %r169, %r170, %r171};
	ld.shared.v4.u32 	{%r176, %r177, %r178, %r179}, [%r3+2048];
	st.global.v4.u32 	[%rd27+2048], {%r176, %r177, %r178, %r179};
	ld.shared.v4.u32 	{%r184, %r185, %r186, %r187}, [%r3+4096];
	st.global.v4.u32 	[%rd27+4096], {%r184, %r185, %r186, %r187};
	ld.shared.v4.u32 	{%r192, %r193, %r194, %r195}, [%r3+6144];
	st.global.v4.u32 	[%rd27+6144], {%r192, %r193, %r194, %r195};
	ret;
}

	// .globl	prefix_sum_exc_large_u64
.visible .entry prefix_sum_exc_large_u64(
	.param .u64 prefix_sum_exc_large_u64_param_0,
	.param .u64 prefix_sum_exc_large_u64_param_1,
	.param .u32 prefix_sum_exc_large_u64_param_2,
	.param .u64 prefix_sum_exc_large_u64_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<104>;
	.reg .b64 	%rd<118>;


	ld.param.u64 	%rd25, [prefix_sum_exc_large_u64_param_0];
	ld.param.u64 	%rd24, [prefix_sum_exc_large_u64_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r12, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r13, %r12, %r2;
	cvta.to.global.u64 	%rd26, %rd25;
	mul.wide.u32 	%rd27, %r13, 16;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.v2.u64 	{%rd29, %rd30}, [%rd28];
	ld.global.v2.u64 	{%rd33, %rd34}, [%rd28+2048];
	ld.global.v2.u64 	{%rd37, %rd38}, [%rd28+4096];
	ld.global.v2.u64 	{%rd41, %rd42}, [%rd28+6144];
	shl.b32 	%r14, %r2, 4;
	mov.u32 	%r15, shared;
	add.s32 	%r3, %r15, %r14;
	st.shared.v2.u64 	[%r3], {%rd29, %rd30};
	st.shared.v2.u64 	[%r3+2048], {%rd33, %rd34};
	st.shared.v2.u64 	[%r3+4096], {%rd37, %rd38};
	st.shared.v2.u64 	[%r3+6144], {%rd41, %rd42};
	bar.sync 	0;
	shl.b32 	%r16, %r2, 6;
	add.s32 	%r4, %r15, %r16;
	ld.shared.v2.u64 	{%rd45, %rd46}, [%r4];
	ld.shared.v2.u64 	{%rd48, %rd49}, [%r4+16];
	ld.shared.v2.u64 	{%rd52, %rd53}, [%r4+32];
	ld.shared.v2.u64 	{%rd56, %rd57}, [%r4+48];
	add.s64 	%rd3, %rd46, %rd45;
	add.s64 	%rd4, %rd48, %rd3;
	add.s64 	%rd5, %rd49, %rd4;
	add.s64 	%rd6, %rd52, %rd5;
	add.s64 	%rd7, %rd53, %rd6;
	add.s64 	%rd8, %rd56, %rd7;
	add.s64 	%rd9, %rd57, %rd8;
	bar.sync 	0;
	shl.b32 	%r18, %r2, 3;
	add.s32 	%r5, %r15, %r18;
	mov.u64 	%rd117, 0;
	st.shared.u64 	[%r5], %rd117;
	st.shared.u64 	[%r5+1024], %rd9;
	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r5+1016];
	ld.shared.u64 	%rd62, [%r5+1024];
	add.s64 	%rd10, %rd61, %rd62;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd10;
	bar.sync 	0;
	ld.shared.u64 	%rd63, [%r5+1008];
	ld.shared.u64 	%rd64, [%r5+1024];
	add.s64 	%rd11, %rd63, %rd64;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd11;
	bar.sync 	0;
	ld.shared.u64 	%rd65, [%r5+992];
	ld.shared.u64 	%rd66, [%r5+1024];
	add.s64 	%rd12, %rd65, %rd66;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd12;
	bar.sync 	0;
	ld.shared.u64 	%rd67, [%r5+960];
	ld.shared.u64 	%rd68, [%r5+1024];
	add.s64 	%rd13, %rd67, %rd68;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd13;
	bar.sync 	0;
	ld.shared.u64 	%rd69, [%r5+896];
	ld.shared.u64 	%rd70, [%r5+1024];
	add.s64 	%rd14, %rd69, %rd70;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd14;
	bar.sync 	0;
	ld.shared.u64 	%rd71, [%r5+768];
	ld.shared.u64 	%rd72, [%r5+1024];
	add.s64 	%rd15, %rd71, %rd72;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd15;
	bar.sync 	0;
	ld.shared.u64 	%rd73, [%r5+512];
	ld.shared.u64 	%rd74, [%r5+1024];
	add.s64 	%rd16, %rd73, %rd74;
	bar.sync 	0;
	cvt.u64.u32	%rd17, %r1;
	mul.wide.u32 	%rd75, %r1, 8;
	add.s64 	%rd18, %rd24, %rd75;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB41_2;

	shl.b64 	%rd78, %rd16, 2;
	or.b64  	%rd77, %rd78, 1;
	// inline asm
	st.cg.u64 [%rd18], %rd77;
	// inline asm

BB41_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r103, %r6, -32;
	bra.uni 	BB41_3;

BB41_8:
	add.s64 	%rd117, %rd20, %rd117;
	add.s32 	%r103, %r103, -32;

BB41_3:
	cvt.s64.s32	%rd82, %r103;
	add.s64 	%rd83, %rd82, %rd17;
	shl.b64 	%rd84, %rd83, 3;
	add.s64 	%rd81, %rd24, %rd84;
	// inline asm
	ld.cg.u64 %rd80, [%rd81];
	// inline asm
	cvt.u32.u64	%r20, %rd80;
	and.b32  	%r9, %r20, 3;
	shr.u64 	%rd20, %rd80, 2;
	setp.eq.s32	%p2, %r9, 0;
	mov.u32 	%r21, -1;
	vote.sync.any.pred 	%p3, %p2, %r21;
	@%p3 bra 	BB41_3;

	setp.eq.s32	%p4, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p4, %r21;
	setp.eq.s32	%p6, %r10, 0;
	@%p6 bra 	BB41_8;

	clz.b32 	%r48, %r10;
	mov.u32 	%r49, 31;
	sub.s32 	%r50, %r49, %r48;
	setp.lt.u32	%p7, %r6, %r50;
	selp.b64	%rd97, 0, %rd20, %p7;
	add.s64 	%rd85, %rd97, %rd117;
	// inline asm
	mov.b64 {%r24,%r25}, %rd85;
	// inline asm
	mov.u32 	%r51, 2;
	mov.u32 	%r52, 16;
	shfl.sync.down.b32 	%r27|%p8, %r25, %r52, %r49, %r21;
	shfl.sync.down.b32 	%r26|%p9, %r24, %r52, %r49, %r21;
	// inline asm
	mov.b64 %rd86, {%r26,%r27};
	// inline asm
	add.s64 	%rd87, %rd86, %rd85;
	// inline asm
	mov.b64 {%r28,%r29}, %rd87;
	// inline asm
	mov.u32 	%r54, 8;
	shfl.sync.down.b32 	%r31|%p10, %r29, %r54, %r49, %r21;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r54, %r49, %r21;
	// inline asm
	mov.b64 %rd88, {%r30,%r31};
	// inline asm
	add.s64 	%rd89, %rd88, %rd87;
	// inline asm
	mov.b64 {%r32,%r33}, %rd89;
	// inline asm
	mov.u32 	%r55, 4;
	shfl.sync.down.b32 	%r35|%p12, %r33, %r55, %r49, %r21;
	shfl.sync.down.b32 	%r34|%p13, %r32, %r55, %r49, %r21;
	// inline asm
	mov.b64 %rd90, {%r34,%r35};
	// inline asm
	add.s64 	%rd91, %rd90, %rd89;
	// inline asm
	mov.b64 {%r36,%r37}, %rd91;
	// inline asm
	shfl.sync.down.b32 	%r39|%p14, %r37, %r51, %r49, %r21;
	shfl.sync.down.b32 	%r38|%p15, %r36, %r51, %r49, %r21;
	// inline asm
	mov.b64 %rd92, {%r38,%r39};
	// inline asm
	add.s64 	%rd93, %rd92, %rd91;
	// inline asm
	mov.b64 {%r40,%r41}, %rd93;
	// inline asm
	mov.u32 	%r56, 1;
	shfl.sync.down.b32 	%r43|%p16, %r41, %r56, %r49, %r21;
	shfl.sync.down.b32 	%r42|%p17, %r40, %r56, %r49, %r21;
	// inline asm
	mov.b64 %rd94, {%r42,%r43};
	// inline asm
	add.s64 	%rd95, %rd94, %rd93;
	// inline asm
	mov.b64 {%r44,%r45}, %rd95;
	// inline asm
	mov.u32 	%r57, 0;
	shfl.sync.idx.b32 	%r47|%p18, %r45, %r57, %r49, %r21;
	shfl.sync.idx.b32 	%r46|%p19, %r44, %r57, %r49, %r21;
	// inline asm
	mov.b64 %rd96, {%r46,%r47};
	// inline asm
	add.s64 	%rd21, %rd96, %rd16;
	@%p1 bra 	BB41_7;

	mov.u32 	%r102, %ctaid.x;
	mul.wide.u32 	%rd116, %r102, 8;
	ld.param.u64 	%rd115, [prefix_sum_exc_large_u64_param_3];
	add.s64 	%rd114, %rd115, %rd116;
	shl.b64 	%rd100, %rd21, 2;
	or.b64  	%rd99, %rd100, 2;
	// inline asm
	st.cg.u64 [%rd114], %rd99;
	// inline asm

BB41_7:
	mov.u32 	%r93, %tid.x;
	shl.b32 	%r92, %r93, 6;
	mov.u32 	%r91, shared;
	add.s32 	%r90, %r91, %r92;
	sub.s64 	%rd101, %rd21, %rd9;
	add.s64 	%rd102, %rd101, %rd45;
	st.shared.v2.u64 	[%r90], {%rd101, %rd102};
	add.s64 	%rd103, %rd101, %rd4;
	add.s64 	%rd104, %rd101, %rd3;
	st.shared.v2.u64 	[%r90+16], {%rd104, %rd103};
	add.s64 	%rd105, %rd101, %rd6;
	add.s64 	%rd106, %rd101, %rd5;
	st.shared.v2.u64 	[%r90+32], {%rd106, %rd105};
	add.s64 	%rd107, %rd101, %rd8;
	add.s64 	%rd108, %rd101, %rd7;
	st.shared.v2.u64 	[%r90+48], {%rd108, %rd107};
	bar.sync 	0;
	mov.u32 	%r101, %tid.x;
	shl.b32 	%r100, %r101, 4;
	mov.u32 	%r99, shared;
	add.s32 	%r98, %r99, %r100;
	mov.u32 	%r97, %ctaid.x;
	mov.u32 	%r96, %tid.x;
	shl.b32 	%r95, %r97, 9;
	add.s32 	%r94, %r95, %r96;
	cvt.u64.u32	%rd113, %r94;
	ld.param.u64 	%rd112, [prefix_sum_exc_large_u64_param_1];
	cvta.to.global.u64 	%rd109, %rd112;
	shl.b64 	%rd110, %rd113, 4;
	add.s64 	%rd111, %rd109, %rd110;
	ld.shared.v4.u32 	{%r58, %r59, %r60, %r61}, [%r98];
	st.global.v4.u32 	[%rd111], {%r58, %r59, %r60, %r61};
	ld.shared.v4.u32 	{%r66, %r67, %r68, %r69}, [%r98+2048];
	st.global.v4.u32 	[%rd111+2048], {%r66, %r67, %r68, %r69};
	ld.shared.v4.u32 	{%r74, %r75, %r76, %r77}, [%r98+4096];
	st.global.v4.u32 	[%rd111+4096], {%r74, %r75, %r76, %r77};
	ld.shared.v4.u32 	{%r82, %r83, %r84, %r85}, [%r98+6144];
	st.global.v4.u32 	[%rd111+6144], {%r82, %r83, %r84, %r85};
	ret;
}

	// .globl	prefix_sum_exc_large_f32
.visible .entry prefix_sum_exc_large_f32(
	.param .u64 prefix_sum_exc_large_f32_param_0,
	.param .u64 prefix_sum_exc_large_f32_param_1,
	.param .u32 prefix_sum_exc_large_f32_param_2,
	.param .u64 prefix_sum_exc_large_f32_param_3
)
{
	.reg .pred 	%p<31>;
	.reg .f32 	%f<153>;
	.reg .b32 	%r<102>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd7, [prefix_sum_exc_large_f32_param_0];
	ld.param.u64 	%rd5, [prefix_sum_exc_large_f32_param_1];
	ld.param.u32 	%r12, [prefix_sum_exc_large_f32_param_2];
	ld.param.u64 	%rd6, [prefix_sum_exc_large_f32_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r13, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r14, %r13, %r2;
	cvt.u64.u32	%rd1, %r14;
	cvta.to.global.u64 	%rd8, %rd7;
	mul.wide.u32 	%rd9, %r14, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.f32 	{%f27, %f28, %f29, %f30}, [%rd10];
	shl.b32 	%r15, %r14, 2;
	setp.lt.u32	%p1, %r15, %r12;
	add.s32 	%r16, %r15, 1;
	setp.lt.u32	%p2, %r16, %r12;
	add.s32 	%r17, %r15, 2;
	setp.lt.u32	%p3, %r17, %r12;
	add.s32 	%r18, %r15, 3;
	setp.lt.u32	%p4, %r18, %r12;
	add.s32 	%r19, %r14, 128;
	ld.global.v4.f32 	{%f35, %f36, %f37, %f38}, [%rd10+2048];
	shl.b32 	%r20, %r19, 2;
	setp.lt.u32	%p5, %r20, %r12;
	add.s32 	%r21, %r20, 1;
	setp.lt.u32	%p6, %r21, %r12;
	add.s32 	%r22, %r20, 2;
	setp.lt.u32	%p7, %r22, %r12;
	add.s32 	%r23, %r20, 3;
	setp.lt.u32	%p8, %r23, %r12;
	add.s32 	%r24, %r14, 256;
	ld.global.v4.f32 	{%f43, %f44, %f45, %f46}, [%rd10+4096];
	shl.b32 	%r25, %r24, 2;
	setp.lt.u32	%p9, %r25, %r12;
	add.s32 	%r26, %r25, 1;
	setp.lt.u32	%p10, %r26, %r12;
	add.s32 	%r27, %r25, 2;
	setp.lt.u32	%p11, %r27, %r12;
	add.s32 	%r28, %r25, 3;
	setp.lt.u32	%p12, %r28, %r12;
	add.s32 	%r29, %r14, 384;
	ld.global.v4.f32 	{%f51, %f52, %f53, %f54}, [%rd10+6144];
	shl.b32 	%r30, %r29, 2;
	setp.lt.u32	%p13, %r30, %r12;
	add.s32 	%r31, %r30, 1;
	setp.lt.u32	%p14, %r31, %r12;
	add.s32 	%r32, %r30, 2;
	setp.lt.u32	%p15, %r32, %r12;
	add.s32 	%r33, %r30, 3;
	setp.lt.u32	%p16, %r33, %r12;
	shl.b32 	%r34, %r2, 4;
	mov.u32 	%r35, shared;
	add.s32 	%r3, %r35, %r34;
	selp.f32	%f59, %f30, 0f00000000, %p4;
	selp.f32	%f60, %f29, 0f00000000, %p3;
	selp.f32	%f61, %f28, 0f00000000, %p2;
	selp.f32	%f62, %f27, 0f00000000, %p1;
	st.shared.v4.f32 	[%r3], {%f62, %f61, %f60, %f59};
	selp.f32	%f63, %f38, 0f00000000, %p8;
	selp.f32	%f64, %f37, 0f00000000, %p7;
	selp.f32	%f65, %f36, 0f00000000, %p6;
	selp.f32	%f66, %f35, 0f00000000, %p5;
	st.shared.v4.f32 	[%r3+2048], {%f66, %f65, %f64, %f63};
	selp.f32	%f67, %f46, 0f00000000, %p12;
	selp.f32	%f68, %f45, 0f00000000, %p11;
	selp.f32	%f69, %f44, 0f00000000, %p10;
	selp.f32	%f70, %f43, 0f00000000, %p9;
	st.shared.v4.f32 	[%r3+4096], {%f70, %f69, %f68, %f67};
	selp.f32	%f71, %f54, 0f00000000, %p16;
	selp.f32	%f72, %f53, 0f00000000, %p15;
	selp.f32	%f73, %f52, 0f00000000, %p14;
	selp.f32	%f74, %f51, 0f00000000, %p13;
	st.shared.v4.f32 	[%r3+6144], {%f74, %f73, %f72, %f71};
	bar.sync 	0;
	shl.b32 	%r36, %r2, 6;
	add.s32 	%r4, %r35, %r36;
	ld.shared.v4.f32 	{%f75, %f76, %f77, %f78}, [%r4];
	ld.shared.v4.f32 	{%f83, %f84, %f85, %f86}, [%r4+16];
	ld.shared.v4.f32 	{%f91, %f92, %f93, %f94}, [%r4+32];
	ld.shared.v4.f32 	{%f99, %f100, %f101, %f102}, [%r4+48];
	add.f32 	%f1, %f75, 0f00000000;
	add.f32 	%f2, %f1, %f76;
	add.f32 	%f3, %f2, %f77;
	add.f32 	%f4, %f3, %f78;
	add.f32 	%f5, %f4, %f83;
	add.f32 	%f6, %f5, %f84;
	add.f32 	%f7, %f6, %f85;
	add.f32 	%f8, %f7, %f86;
	add.f32 	%f9, %f8, %f91;
	add.f32 	%f10, %f9, %f92;
	add.f32 	%f11, %f10, %f93;
	add.f32 	%f12, %f11, %f94;
	add.f32 	%f13, %f12, %f99;
	add.f32 	%f14, %f13, %f100;
	add.f32 	%f15, %f14, %f101;
	add.f32 	%f16, %f15, %f102;
	bar.sync 	0;
	shl.b32 	%r38, %r2, 2;
	add.s32 	%r5, %r35, %r38;
	mov.u32 	%r40, 0;
	st.shared.u32 	[%r5], %r40;
	st.shared.f32 	[%r5+512], %f16;
	bar.sync 	0;
	ld.shared.f32 	%f107, [%r5+508];
	ld.shared.f32 	%f108, [%r5+512];
	add.f32 	%f17, %f108, %f107;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f17;
	bar.sync 	0;
	ld.shared.f32 	%f109, [%r5+504];
	ld.shared.f32 	%f110, [%r5+512];
	add.f32 	%f18, %f110, %f109;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f18;
	bar.sync 	0;
	ld.shared.f32 	%f111, [%r5+496];
	ld.shared.f32 	%f112, [%r5+512];
	add.f32 	%f19, %f112, %f111;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f19;
	bar.sync 	0;
	ld.shared.f32 	%f113, [%r5+480];
	ld.shared.f32 	%f114, [%r5+512];
	add.f32 	%f20, %f114, %f113;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f20;
	bar.sync 	0;
	ld.shared.f32 	%f115, [%r5+448];
	ld.shared.f32 	%f116, [%r5+512];
	add.f32 	%f21, %f116, %f115;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f21;
	bar.sync 	0;
	ld.shared.f32 	%f117, [%r5+384];
	ld.shared.f32 	%f118, [%r5+512];
	add.f32 	%f22, %f118, %f117;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f22;
	bar.sync 	0;
	ld.shared.f32 	%f119, [%r5+256];
	ld.shared.f32 	%f120, [%r5+512];
	add.f32 	%f23, %f120, %f119;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd11, %r1, 8;
	add.s64 	%rd3, %rd6, %rd11;
	setp.ne.s32	%p17, %r2, 127;
	@%p17 bra 	BB42_2;

	mov.b32 	 %r41, %f23;
	cvt.u64.u32	%rd14, %r41;
	shl.b64 	%rd15, %rd14, 32;
	or.b64  	%rd13, %rd15, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd13;
	// inline asm

BB42_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r101, %r6, -32;
	mov.f32 	%f152, 0f00000000;
	bra.uni 	BB42_3;

BB42_8:
	add.s32 	%r101, %r101, -32;
	mov.f32 	%f152, %f25;

BB42_3:
	cvt.s64.s32	%rd18, %r101;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 3;
	add.s64 	%rd17, %rd6, %rd20;
	// inline asm
	ld.cg.u64 %rd16, [%rd17];
	// inline asm
	cvt.u32.u64	%r9, %rd16;
	setp.eq.s32	%p18, %r9, 0;
	mov.u32 	%r42, -1;
	vote.sync.any.pred 	%p19, %p18, %r42;
	@%p19 bra 	BB42_3;

	shr.u64 	%rd21, %rd16, 32;
	cvt.u32.u64	%r44, %rd21;
	mov.b32 	 %f122, %r44;
	setp.eq.s32	%p20, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p20, %r42;
	setp.eq.s32	%p22, %r10, 0;
	add.f32 	%f25, %f152, %f122;
	@%p22 bra 	BB42_8;

	clz.b32 	%r46, %r10;
	mov.u32 	%r47, 31;
	sub.s32 	%r48, %r47, %r46;
	setp.lt.u32	%p23, %r6, %r48;
	selp.f32	%f123, %f152, %f25, %p23;
	mov.b32 	 %r49, %f123;
	mov.u32 	%r50, 2;
	mov.u32 	%r51, 16;
	shfl.sync.down.b32 	%r53|%p24, %r49, %r51, %r47, %r42;
	mov.b32 	 %f124, %r53;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	 %r54, %f125;
	mov.u32 	%r55, 8;
	shfl.sync.down.b32 	%r56|%p25, %r54, %r55, %r47, %r42;
	mov.b32 	 %f126, %r56;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	 %r57, %f127;
	mov.u32 	%r58, 4;
	shfl.sync.down.b32 	%r59|%p26, %r57, %r58, %r47, %r42;
	mov.b32 	 %f128, %r59;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	 %r60, %f129;
	shfl.sync.down.b32 	%r61|%p27, %r60, %r50, %r47, %r42;
	mov.b32 	 %f130, %r61;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	 %r62, %f131;
	mov.u32 	%r63, 1;
	shfl.sync.down.b32 	%r64|%p28, %r62, %r63, %r47, %r42;
	mov.b32 	 %f132, %r64;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	 %r65, %f133;
	shfl.sync.idx.b32 	%r67|%p29, %r65, %r40, %r47, %r42;
	mov.b32 	 %f134, %r67;
	add.f32 	%f26, %f23, %f134;
	@%p17 bra 	BB42_7;

	mov.b32 	 %r68, %f26;
	cvt.u64.u32	%rd24, %r68;
	shl.b64 	%rd25, %rd24, 32;
	or.b64  	%rd23, %rd25, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd23;
	// inline asm

BB42_7:
	sub.f32 	%f135, %f26, %f16;
	add.f32 	%f136, %f135, %f3;
	add.f32 	%f137, %f135, %f2;
	add.f32 	%f138, %f135, %f1;
	add.f32 	%f139, %f135, 0f00000000;
	st.shared.v4.f32 	[%r4], {%f139, %f138, %f137, %f136};
	add.f32 	%f140, %f135, %f7;
	add.f32 	%f141, %f135, %f6;
	add.f32 	%f142, %f135, %f5;
	add.f32 	%f143, %f135, %f4;
	st.shared.v4.f32 	[%r4+16], {%f143, %f142, %f141, %f140};
	add.f32 	%f144, %f135, %f11;
	add.f32 	%f145, %f135, %f10;
	add.f32 	%f146, %f135, %f9;
	add.f32 	%f147, %f135, %f8;
	st.shared.v4.f32 	[%r4+32], {%f147, %f146, %f145, %f144};
	add.f32 	%f148, %f135, %f15;
	add.f32 	%f149, %f135, %f14;
	add.f32 	%f150, %f135, %f13;
	add.f32 	%f151, %f135, %f12;
	st.shared.v4.f32 	[%r4+48], {%f151, %f150, %f149, %f148};
	bar.sync 	0;
	cvta.to.global.u64 	%rd26, %rd5;
	shl.b64 	%rd27, %rd1, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.shared.v4.u32 	{%r69, %r70, %r71, %r72}, [%r3];
	st.global.v4.u32 	[%rd28], {%r69, %r70, %r71, %r72};
	ld.shared.v4.u32 	{%r77, %r78, %r79, %r80}, [%r3+2048];
	st.global.v4.u32 	[%rd28+2048], {%r77, %r78, %r79, %r80};
	ld.shared.v4.u32 	{%r85, %r86, %r87, %r88}, [%r3+4096];
	st.global.v4.u32 	[%rd28+4096], {%r85, %r86, %r87, %r88};
	ld.shared.v4.u32 	{%r93, %r94, %r95, %r96}, [%r3+6144];
	st.global.v4.u32 	[%rd28+6144], {%r93, %r94, %r95, %r96};
	ret;
}

	// .globl	prefix_sum_exc_large_f64
.visible .entry prefix_sum_exc_large_f64(
	.param .u64 prefix_sum_exc_large_f64_param_0,
	.param .u64 prefix_sum_exc_large_f64_param_1,
	.param .u32 prefix_sum_exc_large_f64_param_2,
	.param .u64 prefix_sum_exc_large_f64_param_3
)
{
	.reg .pred 	%p<29>;
	.reg .b32 	%r<116>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd7, [prefix_sum_exc_large_f64_param_0];
	ld.param.u32 	%r12, [prefix_sum_exc_large_f64_param_2];
	ld.param.u64 	%rd6, [prefix_sum_exc_large_f64_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r13, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r14, %r13, %r2;
	cvta.to.global.u64 	%rd8, %rd7;
	mul.wide.u32 	%rd9, %r14, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd10];
	shl.b32 	%r15, %r14, 1;
	setp.lt.u32	%p1, %r15, %r12;
	add.s32 	%r16, %r15, 1;
	setp.lt.u32	%p2, %r16, %r12;
	add.s32 	%r17, %r14, 128;
	ld.global.v2.f64 	{%fd23, %fd24}, [%rd10+2048];
	shl.b32 	%r18, %r17, 1;
	setp.lt.u32	%p3, %r18, %r12;
	add.s32 	%r19, %r18, 1;
	setp.lt.u32	%p4, %r19, %r12;
	add.s32 	%r20, %r14, 256;
	ld.global.v2.f64 	{%fd27, %fd28}, [%rd10+4096];
	shl.b32 	%r21, %r20, 1;
	setp.lt.u32	%p5, %r21, %r12;
	add.s32 	%r22, %r21, 1;
	setp.lt.u32	%p6, %r22, %r12;
	add.s32 	%r23, %r14, 384;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd10+6144];
	shl.b32 	%r24, %r23, 1;
	setp.lt.u32	%p7, %r24, %r12;
	add.s32 	%r25, %r24, 1;
	setp.lt.u32	%p8, %r25, %r12;
	shl.b32 	%r26, %r2, 4;
	mov.u32 	%r27, shared_d;
	add.s32 	%r3, %r27, %r26;
	selp.f64	%fd35, %fd20, 0d0000000000000000, %p2;
	selp.f64	%fd36, %fd19, 0d0000000000000000, %p1;
	st.shared.v2.f64 	[%r3], {%fd36, %fd35};
	selp.f64	%fd37, %fd24, 0d0000000000000000, %p4;
	selp.f64	%fd38, %fd23, 0d0000000000000000, %p3;
	st.shared.v2.f64 	[%r3+2048], {%fd38, %fd37};
	selp.f64	%fd39, %fd28, 0d0000000000000000, %p6;
	selp.f64	%fd40, %fd27, 0d0000000000000000, %p5;
	st.shared.v2.f64 	[%r3+4096], {%fd40, %fd39};
	selp.f64	%fd41, %fd32, 0d0000000000000000, %p8;
	selp.f64	%fd42, %fd31, 0d0000000000000000, %p7;
	st.shared.v2.f64 	[%r3+6144], {%fd42, %fd41};
	bar.sync 	0;
	shl.b32 	%r28, %r2, 6;
	add.s32 	%r4, %r27, %r28;
	ld.shared.v2.f64 	{%fd43, %fd44}, [%r4];
	ld.shared.v2.f64 	{%fd47, %fd48}, [%r4+16];
	ld.shared.v2.f64 	{%fd51, %fd52}, [%r4+32];
	ld.shared.v2.f64 	{%fd55, %fd56}, [%r4+48];
	add.f64 	%fd1, %fd43, 0d0000000000000000;
	add.f64 	%fd2, %fd1, %fd44;
	add.f64 	%fd3, %fd2, %fd47;
	add.f64 	%fd4, %fd3, %fd48;
	add.f64 	%fd5, %fd4, %fd51;
	add.f64 	%fd6, %fd5, %fd52;
	add.f64 	%fd7, %fd6, %fd55;
	add.f64 	%fd8, %fd7, %fd56;
	bar.sync 	0;
	shl.b32 	%r30, %r2, 3;
	add.s32 	%r5, %r27, %r30;
	mov.u64 	%rd11, 0;
	st.shared.u64 	[%r5], %rd11;
	st.shared.f64 	[%r5+1024], %fd8;
	bar.sync 	0;
	ld.shared.f64 	%fd59, [%r5+1016];
	ld.shared.f64 	%fd60, [%r5+1024];
	add.f64 	%fd9, %fd60, %fd59;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd9;
	bar.sync 	0;
	ld.shared.f64 	%fd61, [%r5+1008];
	ld.shared.f64 	%fd62, [%r5+1024];
	add.f64 	%fd10, %fd62, %fd61;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd10;
	bar.sync 	0;
	ld.shared.f64 	%fd63, [%r5+992];
	ld.shared.f64 	%fd64, [%r5+1024];
	add.f64 	%fd11, %fd64, %fd63;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd11;
	bar.sync 	0;
	ld.shared.f64 	%fd65, [%r5+960];
	ld.shared.f64 	%fd66, [%r5+1024];
	add.f64 	%fd12, %fd66, %fd65;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd12;
	bar.sync 	0;
	ld.shared.f64 	%fd67, [%r5+896];
	ld.shared.f64 	%fd68, [%r5+1024];
	add.f64 	%fd13, %fd68, %fd67;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd13;
	bar.sync 	0;
	ld.shared.f64 	%fd69, [%r5+768];
	ld.shared.f64 	%fd70, [%r5+1024];
	add.f64 	%fd14, %fd70, %fd69;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd14;
	bar.sync 	0;
	ld.shared.f64 	%fd71, [%r5+512];
	ld.shared.f64 	%fd72, [%r5+1024];
	add.f64 	%fd15, %fd72, %fd71;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd12, %r1, 8;
	add.s64 	%rd3, %rd6, %rd12;
	setp.ne.s32	%p9, %r2, 127;
	@%p9 bra 	BB43_2;

	mov.b64 	 %rd15, %fd15;
	and.b64  	%rd16, %rd15, -4;
	or.b64  	%rd14, %rd16, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd14;
	// inline asm

BB43_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r115, %r6, -32;
	mov.f64 	%fd96, 0d0000000000000000;
	bra.uni 	BB43_3;

BB43_8:
	add.s32 	%r115, %r115, -32;
	mov.f64 	%fd96, %fd17;

BB43_3:
	cvt.s64.s32	%rd19, %r115;
	add.s64 	%rd20, %rd19, %rd2;
	shl.b64 	%rd21, %rd20, 3;
	add.s64 	%rd18, %rd6, %rd21;
	// inline asm
	ld.cg.u64 %rd17, [%rd18];
	// inline asm
	cvt.u32.u64	%r32, %rd17;
	and.b32  	%r9, %r32, 3;
	setp.eq.s32	%p10, %r9, 0;
	mov.u32 	%r33, -1;
	vote.sync.any.pred 	%p11, %p10, %r33;
	@%p11 bra 	BB43_3;

	and.b64  	%rd22, %rd17, -4;
	mov.b64 	 %fd74, %rd22;
	setp.eq.s32	%p12, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p12, %r33;
	setp.eq.s32	%p14, %r10, 0;
	add.f64 	%fd17, %fd96, %fd74;
	@%p14 bra 	BB43_8;

	clz.b32 	%r60, %r10;
	mov.u32 	%r61, 31;
	sub.s32 	%r62, %r61, %r60;
	setp.lt.u32	%p15, %r6, %r62;
	selp.f64	%fd75, %fd96, %fd17, %p15;
	// inline asm
	mov.b64 {%r36,%r37}, %fd75;
	// inline asm
	mov.u32 	%r63, 2;
	mov.u32 	%r64, 16;
	shfl.sync.down.b32 	%r39|%p16, %r37, %r64, %r61, %r33;
	shfl.sync.down.b32 	%r38|%p17, %r36, %r64, %r61, %r33;
	// inline asm
	mov.b64 %fd76, {%r38,%r39};
	// inline asm
	add.f64 	%fd77, %fd75, %fd76;
	// inline asm
	mov.b64 {%r40,%r41}, %fd77;
	// inline asm
	mov.u32 	%r66, 8;
	shfl.sync.down.b32 	%r43|%p18, %r41, %r66, %r61, %r33;
	shfl.sync.down.b32 	%r42|%p19, %r40, %r66, %r61, %r33;
	// inline asm
	mov.b64 %fd78, {%r42,%r43};
	// inline asm
	add.f64 	%fd79, %fd77, %fd78;
	// inline asm
	mov.b64 {%r44,%r45}, %fd79;
	// inline asm
	mov.u32 	%r67, 4;
	shfl.sync.down.b32 	%r47|%p20, %r45, %r67, %r61, %r33;
	shfl.sync.down.b32 	%r46|%p21, %r44, %r67, %r61, %r33;
	// inline asm
	mov.b64 %fd80, {%r46,%r47};
	// inline asm
	add.f64 	%fd81, %fd79, %fd80;
	// inline asm
	mov.b64 {%r48,%r49}, %fd81;
	// inline asm
	shfl.sync.down.b32 	%r51|%p22, %r49, %r63, %r61, %r33;
	shfl.sync.down.b32 	%r50|%p23, %r48, %r63, %r61, %r33;
	// inline asm
	mov.b64 %fd82, {%r50,%r51};
	// inline asm
	add.f64 	%fd83, %fd81, %fd82;
	// inline asm
	mov.b64 {%r52,%r53}, %fd83;
	// inline asm
	mov.u32 	%r68, 1;
	shfl.sync.down.b32 	%r55|%p24, %r53, %r68, %r61, %r33;
	shfl.sync.down.b32 	%r54|%p25, %r52, %r68, %r61, %r33;
	// inline asm
	mov.b64 %fd84, {%r54,%r55};
	// inline asm
	add.f64 	%fd85, %fd83, %fd84;
	// inline asm
	mov.b64 {%r56,%r57}, %fd85;
	// inline asm
	mov.u32 	%r69, 0;
	shfl.sync.idx.b32 	%r59|%p26, %r57, %r69, %r61, %r33;
	shfl.sync.idx.b32 	%r58|%p27, %r56, %r69, %r61, %r33;
	// inline asm
	mov.b64 %fd86, {%r58,%r59};
	// inline asm
	add.f64 	%fd18, %fd15, %fd86;
	@%p9 bra 	BB43_7;

	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd34, %r114, 8;
	ld.param.u64 	%rd33, [prefix_sum_exc_large_f64_param_3];
	add.s64 	%rd32, %rd33, %rd34;
	mov.b64 	 %rd25, %fd18;
	and.b64  	%rd26, %rd25, -4;
	or.b64  	%rd24, %rd26, 2;
	// inline asm
	st.cg.u64 [%rd32], %rd24;
	// inline asm

BB43_7:
	mov.u32 	%r105, %tid.x;
	shl.b32 	%r104, %r105, 6;
	mov.u32 	%r103, shared_d;
	add.s32 	%r102, %r103, %r104;
	sub.f64 	%fd87, %fd18, %fd8;
	add.f64 	%fd88, %fd87, %fd1;
	add.f64 	%fd89, %fd87, 0d0000000000000000;
	st.shared.v2.f64 	[%r102], {%fd89, %fd88};
	add.f64 	%fd90, %fd87, %fd3;
	add.f64 	%fd91, %fd87, %fd2;
	st.shared.v2.f64 	[%r102+16], {%fd91, %fd90};
	add.f64 	%fd92, %fd87, %fd5;
	add.f64 	%fd93, %fd87, %fd4;
	st.shared.v2.f64 	[%r102+32], {%fd93, %fd92};
	add.f64 	%fd94, %fd87, %fd7;
	add.f64 	%fd95, %fd87, %fd6;
	st.shared.v2.f64 	[%r102+48], {%fd95, %fd94};
	bar.sync 	0;
	mov.u32 	%r113, %tid.x;
	shl.b32 	%r112, %r113, 4;
	mov.u32 	%r111, shared_d;
	add.s32 	%r110, %r111, %r112;
	mov.u32 	%r109, %ctaid.x;
	mov.u32 	%r108, %tid.x;
	shl.b32 	%r107, %r109, 9;
	add.s32 	%r106, %r107, %r108;
	cvt.u64.u32	%rd31, %r106;
	ld.param.u64 	%rd30, [prefix_sum_exc_large_f64_param_1];
	cvta.to.global.u64 	%rd27, %rd30;
	shl.b64 	%rd28, %rd31, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.shared.v4.u32 	{%r70, %r71, %r72, %r73}, [%r110];
	st.global.v4.u32 	[%rd29], {%r70, %r71, %r72, %r73};
	ld.shared.v4.u32 	{%r78, %r79, %r80, %r81}, [%r110+2048];
	st.global.v4.u32 	[%rd29+2048], {%r78, %r79, %r80, %r81};
	ld.shared.v4.u32 	{%r86, %r87, %r88, %r89}, [%r110+4096];
	st.global.v4.u32 	[%rd29+4096], {%r86, %r87, %r88, %r89};
	ld.shared.v4.u32 	{%r94, %r95, %r96, %r97}, [%r110+6144];
	st.global.v4.u32 	[%rd29+6144], {%r94, %r95, %r96, %r97};
	ret;
}

	// .globl	prefix_sum_inc_large_u32
.visible .entry prefix_sum_inc_large_u32(
	.param .u64 prefix_sum_inc_large_u32_param_0,
	.param .u64 prefix_sum_inc_large_u32_param_1,
	.param .u32 prefix_sum_inc_large_u32_param_2,
	.param .u64 prefix_sum_inc_large_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<202>;
	.reg .b64 	%rd<28>;


	ld.param.u64 	%rd6, [prefix_sum_inc_large_u32_param_0];
	ld.param.u64 	%rd4, [prefix_sum_inc_large_u32_param_1];
	ld.param.u64 	%rd5, [prefix_sum_inc_large_u32_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r39, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r40, %r39, %r2;
	cvt.u64.u32	%rd1, %r40;
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r40, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u32 	{%r41, %r42, %r43, %r44}, [%rd9];
	ld.global.v4.u32 	{%r49, %r50, %r51, %r52}, [%rd9+2048];
	ld.global.v4.u32 	{%r57, %r58, %r59, %r60}, [%rd9+4096];
	ld.global.v4.u32 	{%r65, %r66, %r67, %r68}, [%rd9+6144];
	shl.b32 	%r73, %r2, 4;
	mov.u32 	%r74, shared;
	add.s32 	%r3, %r74, %r73;
	st.shared.v4.u32 	[%r3], {%r41, %r42, %r43, %r44};
	st.shared.v4.u32 	[%r3+2048], {%r49, %r50, %r51, %r52};
	st.shared.v4.u32 	[%r3+4096], {%r57, %r58, %r59, %r60};
	st.shared.v4.u32 	[%r3+6144], {%r65, %r66, %r67, %r68};
	bar.sync 	0;
	shl.b32 	%r75, %r2, 6;
	add.s32 	%r4, %r74, %r75;
	ld.shared.v4.u32 	{%r77, %r78, %r79, %r80}, [%r4];
	ld.shared.v4.u32 	{%r84, %r85, %r86, %r87}, [%r4+16];
	ld.shared.v4.u32 	{%r92, %r93, %r94, %r95}, [%r4+32];
	ld.shared.v4.u32 	{%r100, %r101, %r102, %r103}, [%r4+48];
	add.s32 	%r6, %r78, %r77;
	add.s32 	%r7, %r79, %r6;
	add.s32 	%r8, %r80, %r7;
	add.s32 	%r9, %r84, %r8;
	add.s32 	%r10, %r85, %r9;
	add.s32 	%r11, %r86, %r10;
	add.s32 	%r12, %r87, %r11;
	add.s32 	%r13, %r92, %r12;
	add.s32 	%r14, %r93, %r13;
	add.s32 	%r15, %r94, %r14;
	add.s32 	%r16, %r95, %r15;
	add.s32 	%r17, %r100, %r16;
	add.s32 	%r18, %r101, %r17;
	add.s32 	%r19, %r102, %r18;
	add.s32 	%r20, %r103, %r19;
	bar.sync 	0;
	shl.b32 	%r108, %r2, 2;
	add.s32 	%r21, %r74, %r108;
	mov.u32 	%r201, 0;
	st.shared.u32 	[%r21], %r201;
	st.shared.u32 	[%r21+512], %r20;
	bar.sync 	0;
	ld.shared.u32 	%r111, [%r21+508];
	ld.shared.u32 	%r112, [%r21+512];
	add.s32 	%r22, %r111, %r112;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r22;
	bar.sync 	0;
	ld.shared.u32 	%r113, [%r21+504];
	ld.shared.u32 	%r114, [%r21+512];
	add.s32 	%r23, %r113, %r114;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r23;
	bar.sync 	0;
	ld.shared.u32 	%r115, [%r21+496];
	ld.shared.u32 	%r116, [%r21+512];
	add.s32 	%r24, %r115, %r116;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r24;
	bar.sync 	0;
	ld.shared.u32 	%r117, [%r21+480];
	ld.shared.u32 	%r118, [%r21+512];
	add.s32 	%r25, %r117, %r118;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r25;
	bar.sync 	0;
	ld.shared.u32 	%r119, [%r21+448];
	ld.shared.u32 	%r120, [%r21+512];
	add.s32 	%r26, %r119, %r120;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r121, [%r21+384];
	ld.shared.u32 	%r122, [%r21+512];
	add.s32 	%r27, %r121, %r122;
	bar.sync 	0;
	st.shared.u32 	[%r21+512], %r27;
	bar.sync 	0;
	ld.shared.u32 	%r123, [%r21+256];
	ld.shared.u32 	%r124, [%r21+512];
	add.s32 	%r28, %r123, %r124;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd10, %r1, 8;
	add.s64 	%rd3, %rd5, %rd10;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB44_2;

	cvt.u64.u32	%rd13, %r28;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd12, %rd14, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd12;
	// inline asm

BB44_2:
	and.b32  	%r29, %r2, 31;
	add.s32 	%r200, %r29, -32;
	bra.uni 	BB44_3;

BB44_8:
	add.s32 	%r201, %r34, %r201;
	add.s32 	%r200, %r200, -32;

BB44_3:
	cvt.s64.s32	%rd17, %r200;
	add.s64 	%rd18, %rd17, %rd2;
	shl.b64 	%rd19, %rd18, 3;
	add.s64 	%rd16, %rd5, %rd19;
	// inline asm
	ld.cg.u64 %rd15, [%rd16];
	// inline asm
	cvt.u32.u64	%r33, %rd15;
	shr.u64 	%rd20, %rd15, 32;
	cvt.u32.u64	%r34, %rd20;
	setp.eq.s32	%p2, %r33, 0;
	mov.u32 	%r126, -1;
	vote.sync.any.pred 	%p3, %p2, %r126;
	@%p3 bra 	BB44_3;

	setp.eq.s32	%p4, %r33, 2;
	vote.sync.ballot.b32 	%r35, %p4, %r126;
	setp.eq.s32	%p6, %r35, 0;
	@%p6 bra 	BB44_8;

	clz.b32 	%r129, %r35;
	mov.u32 	%r130, 31;
	sub.s32 	%r131, %r130, %r129;
	setp.lt.u32	%p7, %r29, %r131;
	selp.b32	%r132, 0, %r34, %p7;
	mov.u32 	%r133, 0;
	add.s32 	%r134, %r132, %r201;
	mov.u32 	%r135, 2;
	mov.u32 	%r136, 16;
	shfl.sync.down.b32 	%r138|%p8, %r134, %r136, %r130, %r126;
	add.s32 	%r139, %r138, %r134;
	mov.u32 	%r140, 8;
	shfl.sync.down.b32 	%r141|%p9, %r139, %r140, %r130, %r126;
	add.s32 	%r142, %r141, %r139;
	mov.u32 	%r143, 4;
	shfl.sync.down.b32 	%r144|%p10, %r142, %r143, %r130, %r126;
	add.s32 	%r145, %r144, %r142;
	shfl.sync.down.b32 	%r146|%p11, %r145, %r135, %r130, %r126;
	add.s32 	%r147, %r146, %r145;
	mov.u32 	%r148, 1;
	shfl.sync.down.b32 	%r149|%p12, %r147, %r148, %r130, %r126;
	add.s32 	%r150, %r149, %r147;
	shfl.sync.idx.b32 	%r151|%p13, %r150, %r133, %r130, %r126;
	add.s32 	%r36, %r151, %r28;
	@%p1 bra 	BB44_7;

	cvt.u64.u32	%rd23, %r36;
	shl.b64 	%rd24, %rd23, 32;
	or.b64  	%rd22, %rd24, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd22;
	// inline asm

BB44_7:
	sub.s32 	%r152, %r36, %r20;
	add.s32 	%r153, %r152, %r8;
	add.s32 	%r154, %r152, %r7;
	add.s32 	%r155, %r152, %r6;
	add.s32 	%r156, %r152, %r77;
	st.shared.v4.u32 	[%r4], {%r156, %r155, %r154, %r153};
	add.s32 	%r157, %r152, %r12;
	add.s32 	%r158, %r152, %r11;
	add.s32 	%r159, %r152, %r10;
	add.s32 	%r160, %r152, %r9;
	st.shared.v4.u32 	[%r4+16], {%r160, %r159, %r158, %r157};
	add.s32 	%r161, %r152, %r16;
	add.s32 	%r162, %r152, %r15;
	add.s32 	%r163, %r152, %r14;
	add.s32 	%r164, %r152, %r13;
	st.shared.v4.u32 	[%r4+32], {%r164, %r163, %r162, %r161};
	add.s32 	%r165, %r152, %r19;
	add.s32 	%r166, %r152, %r18;
	add.s32 	%r167, %r152, %r17;
	st.shared.v4.u32 	[%r4+48], {%r167, %r166, %r165, %r36};
	bar.sync 	0;
	cvta.to.global.u64 	%rd25, %rd4;
	shl.b64 	%rd26, %rd1, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.shared.v4.u32 	{%r168, %r169, %r170, %r171}, [%r3];
	st.global.v4.u32 	[%rd27], {%r168, %r169, %r170, %r171};
	ld.shared.v4.u32 	{%r176, %r177, %r178, %r179}, [%r3+2048];
	st.global.v4.u32 	[%rd27+2048], {%r176, %r177, %r178, %r179};
	ld.shared.v4.u32 	{%r184, %r185, %r186, %r187}, [%r3+4096];
	st.global.v4.u32 	[%rd27+4096], {%r184, %r185, %r186, %r187};
	ld.shared.v4.u32 	{%r192, %r193, %r194, %r195}, [%r3+6144];
	st.global.v4.u32 	[%rd27+6144], {%r192, %r193, %r194, %r195};
	ret;
}

	// .globl	prefix_sum_inc_large_u64
.visible .entry prefix_sum_inc_large_u64(
	.param .u64 prefix_sum_inc_large_u64_param_0,
	.param .u64 prefix_sum_inc_large_u64_param_1,
	.param .u32 prefix_sum_inc_large_u64_param_2,
	.param .u64 prefix_sum_inc_large_u64_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<104>;
	.reg .b64 	%rd<118>;


	ld.param.u64 	%rd25, [prefix_sum_inc_large_u64_param_0];
	ld.param.u64 	%rd24, [prefix_sum_inc_large_u64_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r12, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r13, %r12, %r2;
	cvta.to.global.u64 	%rd26, %rd25;
	mul.wide.u32 	%rd27, %r13, 16;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.v2.u64 	{%rd29, %rd30}, [%rd28];
	ld.global.v2.u64 	{%rd33, %rd34}, [%rd28+2048];
	ld.global.v2.u64 	{%rd37, %rd38}, [%rd28+4096];
	ld.global.v2.u64 	{%rd41, %rd42}, [%rd28+6144];
	shl.b32 	%r14, %r2, 4;
	mov.u32 	%r15, shared;
	add.s32 	%r3, %r15, %r14;
	st.shared.v2.u64 	[%r3], {%rd29, %rd30};
	st.shared.v2.u64 	[%r3+2048], {%rd33, %rd34};
	st.shared.v2.u64 	[%r3+4096], {%rd37, %rd38};
	st.shared.v2.u64 	[%r3+6144], {%rd41, %rd42};
	bar.sync 	0;
	shl.b32 	%r16, %r2, 6;
	add.s32 	%r4, %r15, %r16;
	ld.shared.v2.u64 	{%rd45, %rd46}, [%r4];
	ld.shared.v2.u64 	{%rd48, %rd49}, [%r4+16];
	ld.shared.v2.u64 	{%rd52, %rd53}, [%r4+32];
	ld.shared.v2.u64 	{%rd56, %rd57}, [%r4+48];
	add.s64 	%rd3, %rd46, %rd45;
	add.s64 	%rd4, %rd48, %rd3;
	add.s64 	%rd5, %rd49, %rd4;
	add.s64 	%rd6, %rd52, %rd5;
	add.s64 	%rd7, %rd53, %rd6;
	add.s64 	%rd8, %rd56, %rd7;
	add.s64 	%rd9, %rd57, %rd8;
	bar.sync 	0;
	shl.b32 	%r18, %r2, 3;
	add.s32 	%r5, %r15, %r18;
	mov.u64 	%rd117, 0;
	st.shared.u64 	[%r5], %rd117;
	st.shared.u64 	[%r5+1024], %rd9;
	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r5+1016];
	ld.shared.u64 	%rd62, [%r5+1024];
	add.s64 	%rd10, %rd61, %rd62;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd10;
	bar.sync 	0;
	ld.shared.u64 	%rd63, [%r5+1008];
	ld.shared.u64 	%rd64, [%r5+1024];
	add.s64 	%rd11, %rd63, %rd64;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd11;
	bar.sync 	0;
	ld.shared.u64 	%rd65, [%r5+992];
	ld.shared.u64 	%rd66, [%r5+1024];
	add.s64 	%rd12, %rd65, %rd66;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd12;
	bar.sync 	0;
	ld.shared.u64 	%rd67, [%r5+960];
	ld.shared.u64 	%rd68, [%r5+1024];
	add.s64 	%rd13, %rd67, %rd68;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd13;
	bar.sync 	0;
	ld.shared.u64 	%rd69, [%r5+896];
	ld.shared.u64 	%rd70, [%r5+1024];
	add.s64 	%rd14, %rd69, %rd70;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd14;
	bar.sync 	0;
	ld.shared.u64 	%rd71, [%r5+768];
	ld.shared.u64 	%rd72, [%r5+1024];
	add.s64 	%rd15, %rd71, %rd72;
	bar.sync 	0;
	st.shared.u64 	[%r5+1024], %rd15;
	bar.sync 	0;
	ld.shared.u64 	%rd73, [%r5+512];
	ld.shared.u64 	%rd74, [%r5+1024];
	add.s64 	%rd16, %rd73, %rd74;
	bar.sync 	0;
	cvt.u64.u32	%rd17, %r1;
	mul.wide.u32 	%rd75, %r1, 8;
	add.s64 	%rd18, %rd24, %rd75;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB45_2;

	shl.b64 	%rd78, %rd16, 2;
	or.b64  	%rd77, %rd78, 1;
	// inline asm
	st.cg.u64 [%rd18], %rd77;
	// inline asm

BB45_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r103, %r6, -32;
	bra.uni 	BB45_3;

BB45_8:
	add.s64 	%rd117, %rd20, %rd117;
	add.s32 	%r103, %r103, -32;

BB45_3:
	cvt.s64.s32	%rd82, %r103;
	add.s64 	%rd83, %rd82, %rd17;
	shl.b64 	%rd84, %rd83, 3;
	add.s64 	%rd81, %rd24, %rd84;
	// inline asm
	ld.cg.u64 %rd80, [%rd81];
	// inline asm
	cvt.u32.u64	%r20, %rd80;
	and.b32  	%r9, %r20, 3;
	shr.u64 	%rd20, %rd80, 2;
	setp.eq.s32	%p2, %r9, 0;
	mov.u32 	%r21, -1;
	vote.sync.any.pred 	%p3, %p2, %r21;
	@%p3 bra 	BB45_3;

	setp.eq.s32	%p4, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p4, %r21;
	setp.eq.s32	%p6, %r10, 0;
	@%p6 bra 	BB45_8;

	clz.b32 	%r48, %r10;
	mov.u32 	%r49, 31;
	sub.s32 	%r50, %r49, %r48;
	setp.lt.u32	%p7, %r6, %r50;
	selp.b64	%rd97, 0, %rd20, %p7;
	add.s64 	%rd85, %rd97, %rd117;
	// inline asm
	mov.b64 {%r24,%r25}, %rd85;
	// inline asm
	mov.u32 	%r51, 2;
	mov.u32 	%r52, 16;
	shfl.sync.down.b32 	%r27|%p8, %r25, %r52, %r49, %r21;
	shfl.sync.down.b32 	%r26|%p9, %r24, %r52, %r49, %r21;
	// inline asm
	mov.b64 %rd86, {%r26,%r27};
	// inline asm
	add.s64 	%rd87, %rd86, %rd85;
	// inline asm
	mov.b64 {%r28,%r29}, %rd87;
	// inline asm
	mov.u32 	%r54, 8;
	shfl.sync.down.b32 	%r31|%p10, %r29, %r54, %r49, %r21;
	shfl.sync.down.b32 	%r30|%p11, %r28, %r54, %r49, %r21;
	// inline asm
	mov.b64 %rd88, {%r30,%r31};
	// inline asm
	add.s64 	%rd89, %rd88, %rd87;
	// inline asm
	mov.b64 {%r32,%r33}, %rd89;
	// inline asm
	mov.u32 	%r55, 4;
	shfl.sync.down.b32 	%r35|%p12, %r33, %r55, %r49, %r21;
	shfl.sync.down.b32 	%r34|%p13, %r32, %r55, %r49, %r21;
	// inline asm
	mov.b64 %rd90, {%r34,%r35};
	// inline asm
	add.s64 	%rd91, %rd90, %rd89;
	// inline asm
	mov.b64 {%r36,%r37}, %rd91;
	// inline asm
	shfl.sync.down.b32 	%r39|%p14, %r37, %r51, %r49, %r21;
	shfl.sync.down.b32 	%r38|%p15, %r36, %r51, %r49, %r21;
	// inline asm
	mov.b64 %rd92, {%r38,%r39};
	// inline asm
	add.s64 	%rd93, %rd92, %rd91;
	// inline asm
	mov.b64 {%r40,%r41}, %rd93;
	// inline asm
	mov.u32 	%r56, 1;
	shfl.sync.down.b32 	%r43|%p16, %r41, %r56, %r49, %r21;
	shfl.sync.down.b32 	%r42|%p17, %r40, %r56, %r49, %r21;
	// inline asm
	mov.b64 %rd94, {%r42,%r43};
	// inline asm
	add.s64 	%rd95, %rd94, %rd93;
	// inline asm
	mov.b64 {%r44,%r45}, %rd95;
	// inline asm
	mov.u32 	%r57, 0;
	shfl.sync.idx.b32 	%r47|%p18, %r45, %r57, %r49, %r21;
	shfl.sync.idx.b32 	%r46|%p19, %r44, %r57, %r49, %r21;
	// inline asm
	mov.b64 %rd96, {%r46,%r47};
	// inline asm
	add.s64 	%rd21, %rd96, %rd16;
	@%p1 bra 	BB45_7;

	mov.u32 	%r102, %ctaid.x;
	mul.wide.u32 	%rd116, %r102, 8;
	ld.param.u64 	%rd115, [prefix_sum_inc_large_u64_param_3];
	add.s64 	%rd114, %rd115, %rd116;
	shl.b64 	%rd100, %rd21, 2;
	or.b64  	%rd99, %rd100, 2;
	// inline asm
	st.cg.u64 [%rd114], %rd99;
	// inline asm

BB45_7:
	mov.u32 	%r93, %tid.x;
	shl.b32 	%r92, %r93, 6;
	mov.u32 	%r91, shared;
	add.s32 	%r90, %r91, %r92;
	sub.s64 	%rd101, %rd21, %rd9;
	add.s64 	%rd102, %rd101, %rd3;
	add.s64 	%rd103, %rd101, %rd45;
	st.shared.v2.u64 	[%r90], {%rd103, %rd102};
	add.s64 	%rd104, %rd101, %rd5;
	add.s64 	%rd105, %rd101, %rd4;
	st.shared.v2.u64 	[%r90+16], {%rd105, %rd104};
	add.s64 	%rd106, %rd101, %rd7;
	add.s64 	%rd107, %rd101, %rd6;
	st.shared.v2.u64 	[%r90+32], {%rd107, %rd106};
	add.s64 	%rd108, %rd101, %rd8;
	st.shared.v2.u64 	[%r90+48], {%rd108, %rd21};
	bar.sync 	0;
	mov.u32 	%r101, %tid.x;
	shl.b32 	%r100, %r101, 4;
	mov.u32 	%r99, shared;
	add.s32 	%r98, %r99, %r100;
	mov.u32 	%r97, %ctaid.x;
	mov.u32 	%r96, %tid.x;
	shl.b32 	%r95, %r97, 9;
	add.s32 	%r94, %r95, %r96;
	cvt.u64.u32	%rd113, %r94;
	ld.param.u64 	%rd112, [prefix_sum_inc_large_u64_param_1];
	cvta.to.global.u64 	%rd109, %rd112;
	shl.b64 	%rd110, %rd113, 4;
	add.s64 	%rd111, %rd109, %rd110;
	ld.shared.v4.u32 	{%r58, %r59, %r60, %r61}, [%r98];
	st.global.v4.u32 	[%rd111], {%r58, %r59, %r60, %r61};
	ld.shared.v4.u32 	{%r66, %r67, %r68, %r69}, [%r98+2048];
	st.global.v4.u32 	[%rd111+2048], {%r66, %r67, %r68, %r69};
	ld.shared.v4.u32 	{%r74, %r75, %r76, %r77}, [%r98+4096];
	st.global.v4.u32 	[%rd111+4096], {%r74, %r75, %r76, %r77};
	ld.shared.v4.u32 	{%r82, %r83, %r84, %r85}, [%r98+6144];
	st.global.v4.u32 	[%rd111+6144], {%r82, %r83, %r84, %r85};
	ret;
}

	// .globl	prefix_sum_inc_large_f32
.visible .entry prefix_sum_inc_large_f32(
	.param .u64 prefix_sum_inc_large_f32_param_0,
	.param .u64 prefix_sum_inc_large_f32_param_1,
	.param .u32 prefix_sum_inc_large_f32_param_2,
	.param .u64 prefix_sum_inc_large_f32_param_3
)
{
	.reg .pred 	%p<31>;
	.reg .f32 	%f<153>;
	.reg .b32 	%r<102>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd7, [prefix_sum_inc_large_f32_param_0];
	ld.param.u64 	%rd5, [prefix_sum_inc_large_f32_param_1];
	ld.param.u32 	%r12, [prefix_sum_inc_large_f32_param_2];
	ld.param.u64 	%rd6, [prefix_sum_inc_large_f32_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r13, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r14, %r13, %r2;
	cvt.u64.u32	%rd1, %r14;
	cvta.to.global.u64 	%rd8, %rd7;
	mul.wide.u32 	%rd9, %r14, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.f32 	{%f27, %f28, %f29, %f30}, [%rd10];
	shl.b32 	%r15, %r14, 2;
	setp.lt.u32	%p1, %r15, %r12;
	add.s32 	%r16, %r15, 1;
	setp.lt.u32	%p2, %r16, %r12;
	add.s32 	%r17, %r15, 2;
	setp.lt.u32	%p3, %r17, %r12;
	add.s32 	%r18, %r15, 3;
	setp.lt.u32	%p4, %r18, %r12;
	add.s32 	%r19, %r14, 128;
	ld.global.v4.f32 	{%f35, %f36, %f37, %f38}, [%rd10+2048];
	shl.b32 	%r20, %r19, 2;
	setp.lt.u32	%p5, %r20, %r12;
	add.s32 	%r21, %r20, 1;
	setp.lt.u32	%p6, %r21, %r12;
	add.s32 	%r22, %r20, 2;
	setp.lt.u32	%p7, %r22, %r12;
	add.s32 	%r23, %r20, 3;
	setp.lt.u32	%p8, %r23, %r12;
	add.s32 	%r24, %r14, 256;
	ld.global.v4.f32 	{%f43, %f44, %f45, %f46}, [%rd10+4096];
	shl.b32 	%r25, %r24, 2;
	setp.lt.u32	%p9, %r25, %r12;
	add.s32 	%r26, %r25, 1;
	setp.lt.u32	%p10, %r26, %r12;
	add.s32 	%r27, %r25, 2;
	setp.lt.u32	%p11, %r27, %r12;
	add.s32 	%r28, %r25, 3;
	setp.lt.u32	%p12, %r28, %r12;
	add.s32 	%r29, %r14, 384;
	ld.global.v4.f32 	{%f51, %f52, %f53, %f54}, [%rd10+6144];
	shl.b32 	%r30, %r29, 2;
	setp.lt.u32	%p13, %r30, %r12;
	add.s32 	%r31, %r30, 1;
	setp.lt.u32	%p14, %r31, %r12;
	add.s32 	%r32, %r30, 2;
	setp.lt.u32	%p15, %r32, %r12;
	add.s32 	%r33, %r30, 3;
	setp.lt.u32	%p16, %r33, %r12;
	shl.b32 	%r34, %r2, 4;
	mov.u32 	%r35, shared;
	add.s32 	%r3, %r35, %r34;
	selp.f32	%f59, %f30, 0f00000000, %p4;
	selp.f32	%f60, %f29, 0f00000000, %p3;
	selp.f32	%f61, %f28, 0f00000000, %p2;
	selp.f32	%f62, %f27, 0f00000000, %p1;
	st.shared.v4.f32 	[%r3], {%f62, %f61, %f60, %f59};
	selp.f32	%f63, %f38, 0f00000000, %p8;
	selp.f32	%f64, %f37, 0f00000000, %p7;
	selp.f32	%f65, %f36, 0f00000000, %p6;
	selp.f32	%f66, %f35, 0f00000000, %p5;
	st.shared.v4.f32 	[%r3+2048], {%f66, %f65, %f64, %f63};
	selp.f32	%f67, %f46, 0f00000000, %p12;
	selp.f32	%f68, %f45, 0f00000000, %p11;
	selp.f32	%f69, %f44, 0f00000000, %p10;
	selp.f32	%f70, %f43, 0f00000000, %p9;
	st.shared.v4.f32 	[%r3+4096], {%f70, %f69, %f68, %f67};
	selp.f32	%f71, %f54, 0f00000000, %p16;
	selp.f32	%f72, %f53, 0f00000000, %p15;
	selp.f32	%f73, %f52, 0f00000000, %p14;
	selp.f32	%f74, %f51, 0f00000000, %p13;
	st.shared.v4.f32 	[%r3+6144], {%f74, %f73, %f72, %f71};
	bar.sync 	0;
	shl.b32 	%r36, %r2, 6;
	add.s32 	%r4, %r35, %r36;
	ld.shared.v4.f32 	{%f75, %f76, %f77, %f78}, [%r4];
	ld.shared.v4.f32 	{%f83, %f84, %f85, %f86}, [%r4+16];
	ld.shared.v4.f32 	{%f91, %f92, %f93, %f94}, [%r4+32];
	ld.shared.v4.f32 	{%f99, %f100, %f101, %f102}, [%r4+48];
	add.f32 	%f1, %f75, 0f00000000;
	add.f32 	%f2, %f1, %f76;
	add.f32 	%f3, %f2, %f77;
	add.f32 	%f4, %f3, %f78;
	add.f32 	%f5, %f4, %f83;
	add.f32 	%f6, %f5, %f84;
	add.f32 	%f7, %f6, %f85;
	add.f32 	%f8, %f7, %f86;
	add.f32 	%f9, %f8, %f91;
	add.f32 	%f10, %f9, %f92;
	add.f32 	%f11, %f10, %f93;
	add.f32 	%f12, %f11, %f94;
	add.f32 	%f13, %f12, %f99;
	add.f32 	%f14, %f13, %f100;
	add.f32 	%f15, %f14, %f101;
	add.f32 	%f16, %f15, %f102;
	bar.sync 	0;
	shl.b32 	%r38, %r2, 2;
	add.s32 	%r5, %r35, %r38;
	mov.u32 	%r40, 0;
	st.shared.u32 	[%r5], %r40;
	st.shared.f32 	[%r5+512], %f16;
	bar.sync 	0;
	ld.shared.f32 	%f107, [%r5+508];
	ld.shared.f32 	%f108, [%r5+512];
	add.f32 	%f17, %f108, %f107;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f17;
	bar.sync 	0;
	ld.shared.f32 	%f109, [%r5+504];
	ld.shared.f32 	%f110, [%r5+512];
	add.f32 	%f18, %f110, %f109;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f18;
	bar.sync 	0;
	ld.shared.f32 	%f111, [%r5+496];
	ld.shared.f32 	%f112, [%r5+512];
	add.f32 	%f19, %f112, %f111;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f19;
	bar.sync 	0;
	ld.shared.f32 	%f113, [%r5+480];
	ld.shared.f32 	%f114, [%r5+512];
	add.f32 	%f20, %f114, %f113;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f20;
	bar.sync 	0;
	ld.shared.f32 	%f115, [%r5+448];
	ld.shared.f32 	%f116, [%r5+512];
	add.f32 	%f21, %f116, %f115;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f21;
	bar.sync 	0;
	ld.shared.f32 	%f117, [%r5+384];
	ld.shared.f32 	%f118, [%r5+512];
	add.f32 	%f22, %f118, %f117;
	bar.sync 	0;
	st.shared.f32 	[%r5+512], %f22;
	bar.sync 	0;
	ld.shared.f32 	%f119, [%r5+256];
	ld.shared.f32 	%f120, [%r5+512];
	add.f32 	%f23, %f120, %f119;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd11, %r1, 8;
	add.s64 	%rd3, %rd6, %rd11;
	setp.ne.s32	%p17, %r2, 127;
	@%p17 bra 	BB46_2;

	mov.b32 	 %r41, %f23;
	cvt.u64.u32	%rd14, %r41;
	shl.b64 	%rd15, %rd14, 32;
	or.b64  	%rd13, %rd15, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd13;
	// inline asm

BB46_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r101, %r6, -32;
	mov.f32 	%f152, 0f00000000;
	bra.uni 	BB46_3;

BB46_8:
	add.s32 	%r101, %r101, -32;
	mov.f32 	%f152, %f25;

BB46_3:
	cvt.s64.s32	%rd18, %r101;
	add.s64 	%rd19, %rd18, %rd2;
	shl.b64 	%rd20, %rd19, 3;
	add.s64 	%rd17, %rd6, %rd20;
	// inline asm
	ld.cg.u64 %rd16, [%rd17];
	// inline asm
	cvt.u32.u64	%r9, %rd16;
	setp.eq.s32	%p18, %r9, 0;
	mov.u32 	%r42, -1;
	vote.sync.any.pred 	%p19, %p18, %r42;
	@%p19 bra 	BB46_3;

	shr.u64 	%rd21, %rd16, 32;
	cvt.u32.u64	%r44, %rd21;
	mov.b32 	 %f122, %r44;
	setp.eq.s32	%p20, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p20, %r42;
	setp.eq.s32	%p22, %r10, 0;
	add.f32 	%f25, %f152, %f122;
	@%p22 bra 	BB46_8;

	clz.b32 	%r46, %r10;
	mov.u32 	%r47, 31;
	sub.s32 	%r48, %r47, %r46;
	setp.lt.u32	%p23, %r6, %r48;
	selp.f32	%f123, %f152, %f25, %p23;
	mov.b32 	 %r49, %f123;
	mov.u32 	%r50, 2;
	mov.u32 	%r51, 16;
	shfl.sync.down.b32 	%r53|%p24, %r49, %r51, %r47, %r42;
	mov.b32 	 %f124, %r53;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	 %r54, %f125;
	mov.u32 	%r55, 8;
	shfl.sync.down.b32 	%r56|%p25, %r54, %r55, %r47, %r42;
	mov.b32 	 %f126, %r56;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	 %r57, %f127;
	mov.u32 	%r58, 4;
	shfl.sync.down.b32 	%r59|%p26, %r57, %r58, %r47, %r42;
	mov.b32 	 %f128, %r59;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	 %r60, %f129;
	shfl.sync.down.b32 	%r61|%p27, %r60, %r50, %r47, %r42;
	mov.b32 	 %f130, %r61;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	 %r62, %f131;
	mov.u32 	%r63, 1;
	shfl.sync.down.b32 	%r64|%p28, %r62, %r63, %r47, %r42;
	mov.b32 	 %f132, %r64;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	 %r65, %f133;
	shfl.sync.idx.b32 	%r67|%p29, %r65, %r40, %r47, %r42;
	mov.b32 	 %f134, %r67;
	add.f32 	%f26, %f23, %f134;
	@%p17 bra 	BB46_7;

	mov.b32 	 %r68, %f26;
	cvt.u64.u32	%rd24, %r68;
	shl.b64 	%rd25, %rd24, 32;
	or.b64  	%rd23, %rd25, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd23;
	// inline asm

BB46_7:
	sub.f32 	%f135, %f26, %f16;
	add.f32 	%f136, %f135, %f4;
	add.f32 	%f137, %f135, %f3;
	add.f32 	%f138, %f135, %f2;
	add.f32 	%f139, %f135, %f1;
	st.shared.v4.f32 	[%r4], {%f139, %f138, %f137, %f136};
	add.f32 	%f140, %f135, %f8;
	add.f32 	%f141, %f135, %f7;
	add.f32 	%f142, %f135, %f6;
	add.f32 	%f143, %f135, %f5;
	st.shared.v4.f32 	[%r4+16], {%f143, %f142, %f141, %f140};
	add.f32 	%f144, %f135, %f12;
	add.f32 	%f145, %f135, %f11;
	add.f32 	%f146, %f135, %f10;
	add.f32 	%f147, %f135, %f9;
	st.shared.v4.f32 	[%r4+32], {%f147, %f146, %f145, %f144};
	add.f32 	%f148, %f135, %f16;
	add.f32 	%f149, %f135, %f15;
	add.f32 	%f150, %f135, %f14;
	add.f32 	%f151, %f135, %f13;
	st.shared.v4.f32 	[%r4+48], {%f151, %f150, %f149, %f148};
	bar.sync 	0;
	cvta.to.global.u64 	%rd26, %rd5;
	shl.b64 	%rd27, %rd1, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.shared.v4.u32 	{%r69, %r70, %r71, %r72}, [%r3];
	st.global.v4.u32 	[%rd28], {%r69, %r70, %r71, %r72};
	ld.shared.v4.u32 	{%r77, %r78, %r79, %r80}, [%r3+2048];
	st.global.v4.u32 	[%rd28+2048], {%r77, %r78, %r79, %r80};
	ld.shared.v4.u32 	{%r85, %r86, %r87, %r88}, [%r3+4096];
	st.global.v4.u32 	[%rd28+4096], {%r85, %r86, %r87, %r88};
	ld.shared.v4.u32 	{%r93, %r94, %r95, %r96}, [%r3+6144];
	st.global.v4.u32 	[%rd28+6144], {%r93, %r94, %r95, %r96};
	ret;
}

	// .globl	prefix_sum_inc_large_f64
.visible .entry prefix_sum_inc_large_f64(
	.param .u64 prefix_sum_inc_large_f64_param_0,
	.param .u64 prefix_sum_inc_large_f64_param_1,
	.param .u32 prefix_sum_inc_large_f64_param_2,
	.param .u64 prefix_sum_inc_large_f64_param_3
)
{
	.reg .pred 	%p<29>;
	.reg .b32 	%r<116>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd7, [prefix_sum_inc_large_f64_param_0];
	ld.param.u32 	%r12, [prefix_sum_inc_large_f64_param_2];
	ld.param.u64 	%rd6, [prefix_sum_inc_large_f64_param_3];
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r13, %r1, 9;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r14, %r13, %r2;
	cvta.to.global.u64 	%rd8, %rd7;
	mul.wide.u32 	%rd9, %r14, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v2.f64 	{%fd19, %fd20}, [%rd10];
	shl.b32 	%r15, %r14, 1;
	setp.lt.u32	%p1, %r15, %r12;
	add.s32 	%r16, %r15, 1;
	setp.lt.u32	%p2, %r16, %r12;
	add.s32 	%r17, %r14, 128;
	ld.global.v2.f64 	{%fd23, %fd24}, [%rd10+2048];
	shl.b32 	%r18, %r17, 1;
	setp.lt.u32	%p3, %r18, %r12;
	add.s32 	%r19, %r18, 1;
	setp.lt.u32	%p4, %r19, %r12;
	add.s32 	%r20, %r14, 256;
	ld.global.v2.f64 	{%fd27, %fd28}, [%rd10+4096];
	shl.b32 	%r21, %r20, 1;
	setp.lt.u32	%p5, %r21, %r12;
	add.s32 	%r22, %r21, 1;
	setp.lt.u32	%p6, %r22, %r12;
	add.s32 	%r23, %r14, 384;
	ld.global.v2.f64 	{%fd31, %fd32}, [%rd10+6144];
	shl.b32 	%r24, %r23, 1;
	setp.lt.u32	%p7, %r24, %r12;
	add.s32 	%r25, %r24, 1;
	setp.lt.u32	%p8, %r25, %r12;
	shl.b32 	%r26, %r2, 4;
	mov.u32 	%r27, shared_d;
	add.s32 	%r3, %r27, %r26;
	selp.f64	%fd35, %fd20, 0d0000000000000000, %p2;
	selp.f64	%fd36, %fd19, 0d0000000000000000, %p1;
	st.shared.v2.f64 	[%r3], {%fd36, %fd35};
	selp.f64	%fd37, %fd24, 0d0000000000000000, %p4;
	selp.f64	%fd38, %fd23, 0d0000000000000000, %p3;
	st.shared.v2.f64 	[%r3+2048], {%fd38, %fd37};
	selp.f64	%fd39, %fd28, 0d0000000000000000, %p6;
	selp.f64	%fd40, %fd27, 0d0000000000000000, %p5;
	st.shared.v2.f64 	[%r3+4096], {%fd40, %fd39};
	selp.f64	%fd41, %fd32, 0d0000000000000000, %p8;
	selp.f64	%fd42, %fd31, 0d0000000000000000, %p7;
	st.shared.v2.f64 	[%r3+6144], {%fd42, %fd41};
	bar.sync 	0;
	shl.b32 	%r28, %r2, 6;
	add.s32 	%r4, %r27, %r28;
	ld.shared.v2.f64 	{%fd43, %fd44}, [%r4];
	ld.shared.v2.f64 	{%fd47, %fd48}, [%r4+16];
	ld.shared.v2.f64 	{%fd51, %fd52}, [%r4+32];
	ld.shared.v2.f64 	{%fd55, %fd56}, [%r4+48];
	add.f64 	%fd1, %fd43, 0d0000000000000000;
	add.f64 	%fd2, %fd1, %fd44;
	add.f64 	%fd3, %fd2, %fd47;
	add.f64 	%fd4, %fd3, %fd48;
	add.f64 	%fd5, %fd4, %fd51;
	add.f64 	%fd6, %fd5, %fd52;
	add.f64 	%fd7, %fd6, %fd55;
	add.f64 	%fd8, %fd7, %fd56;
	bar.sync 	0;
	shl.b32 	%r30, %r2, 3;
	add.s32 	%r5, %r27, %r30;
	mov.u64 	%rd11, 0;
	st.shared.u64 	[%r5], %rd11;
	st.shared.f64 	[%r5+1024], %fd8;
	bar.sync 	0;
	ld.shared.f64 	%fd59, [%r5+1016];
	ld.shared.f64 	%fd60, [%r5+1024];
	add.f64 	%fd9, %fd60, %fd59;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd9;
	bar.sync 	0;
	ld.shared.f64 	%fd61, [%r5+1008];
	ld.shared.f64 	%fd62, [%r5+1024];
	add.f64 	%fd10, %fd62, %fd61;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd10;
	bar.sync 	0;
	ld.shared.f64 	%fd63, [%r5+992];
	ld.shared.f64 	%fd64, [%r5+1024];
	add.f64 	%fd11, %fd64, %fd63;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd11;
	bar.sync 	0;
	ld.shared.f64 	%fd65, [%r5+960];
	ld.shared.f64 	%fd66, [%r5+1024];
	add.f64 	%fd12, %fd66, %fd65;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd12;
	bar.sync 	0;
	ld.shared.f64 	%fd67, [%r5+896];
	ld.shared.f64 	%fd68, [%r5+1024];
	add.f64 	%fd13, %fd68, %fd67;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd13;
	bar.sync 	0;
	ld.shared.f64 	%fd69, [%r5+768];
	ld.shared.f64 	%fd70, [%r5+1024];
	add.f64 	%fd14, %fd70, %fd69;
	bar.sync 	0;
	st.shared.f64 	[%r5+1024], %fd14;
	bar.sync 	0;
	ld.shared.f64 	%fd71, [%r5+512];
	ld.shared.f64 	%fd72, [%r5+1024];
	add.f64 	%fd15, %fd72, %fd71;
	bar.sync 	0;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd12, %r1, 8;
	add.s64 	%rd3, %rd6, %rd12;
	setp.ne.s32	%p9, %r2, 127;
	@%p9 bra 	BB47_2;

	mov.b64 	 %rd15, %fd15;
	and.b64  	%rd16, %rd15, -4;
	or.b64  	%rd14, %rd16, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd14;
	// inline asm

BB47_2:
	and.b32  	%r6, %r2, 31;
	add.s32 	%r115, %r6, -32;
	mov.f64 	%fd96, 0d0000000000000000;
	bra.uni 	BB47_3;

BB47_8:
	add.s32 	%r115, %r115, -32;
	mov.f64 	%fd96, %fd17;

BB47_3:
	cvt.s64.s32	%rd19, %r115;
	add.s64 	%rd20, %rd19, %rd2;
	shl.b64 	%rd21, %rd20, 3;
	add.s64 	%rd18, %rd6, %rd21;
	// inline asm
	ld.cg.u64 %rd17, [%rd18];
	// inline asm
	cvt.u32.u64	%r32, %rd17;
	and.b32  	%r9, %r32, 3;
	setp.eq.s32	%p10, %r9, 0;
	mov.u32 	%r33, -1;
	vote.sync.any.pred 	%p11, %p10, %r33;
	@%p11 bra 	BB47_3;

	and.b64  	%rd22, %rd17, -4;
	mov.b64 	 %fd74, %rd22;
	setp.eq.s32	%p12, %r9, 2;
	vote.sync.ballot.b32 	%r10, %p12, %r33;
	setp.eq.s32	%p14, %r10, 0;
	add.f64 	%fd17, %fd96, %fd74;
	@%p14 bra 	BB47_8;

	clz.b32 	%r60, %r10;
	mov.u32 	%r61, 31;
	sub.s32 	%r62, %r61, %r60;
	setp.lt.u32	%p15, %r6, %r62;
	selp.f64	%fd75, %fd96, %fd17, %p15;
	// inline asm
	mov.b64 {%r36,%r37}, %fd75;
	// inline asm
	mov.u32 	%r63, 2;
	mov.u32 	%r64, 16;
	shfl.sync.down.b32 	%r39|%p16, %r37, %r64, %r61, %r33;
	shfl.sync.down.b32 	%r38|%p17, %r36, %r64, %r61, %r33;
	// inline asm
	mov.b64 %fd76, {%r38,%r39};
	// inline asm
	add.f64 	%fd77, %fd75, %fd76;
	// inline asm
	mov.b64 {%r40,%r41}, %fd77;
	// inline asm
	mov.u32 	%r66, 8;
	shfl.sync.down.b32 	%r43|%p18, %r41, %r66, %r61, %r33;
	shfl.sync.down.b32 	%r42|%p19, %r40, %r66, %r61, %r33;
	// inline asm
	mov.b64 %fd78, {%r42,%r43};
	// inline asm
	add.f64 	%fd79, %fd77, %fd78;
	// inline asm
	mov.b64 {%r44,%r45}, %fd79;
	// inline asm
	mov.u32 	%r67, 4;
	shfl.sync.down.b32 	%r47|%p20, %r45, %r67, %r61, %r33;
	shfl.sync.down.b32 	%r46|%p21, %r44, %r67, %r61, %r33;
	// inline asm
	mov.b64 %fd80, {%r46,%r47};
	// inline asm
	add.f64 	%fd81, %fd79, %fd80;
	// inline asm
	mov.b64 {%r48,%r49}, %fd81;
	// inline asm
	shfl.sync.down.b32 	%r51|%p22, %r49, %r63, %r61, %r33;
	shfl.sync.down.b32 	%r50|%p23, %r48, %r63, %r61, %r33;
	// inline asm
	mov.b64 %fd82, {%r50,%r51};
	// inline asm
	add.f64 	%fd83, %fd81, %fd82;
	// inline asm
	mov.b64 {%r52,%r53}, %fd83;
	// inline asm
	mov.u32 	%r68, 1;
	shfl.sync.down.b32 	%r55|%p24, %r53, %r68, %r61, %r33;
	shfl.sync.down.b32 	%r54|%p25, %r52, %r68, %r61, %r33;
	// inline asm
	mov.b64 %fd84, {%r54,%r55};
	// inline asm
	add.f64 	%fd85, %fd83, %fd84;
	// inline asm
	mov.b64 {%r56,%r57}, %fd85;
	// inline asm
	mov.u32 	%r69, 0;
	shfl.sync.idx.b32 	%r59|%p26, %r57, %r69, %r61, %r33;
	shfl.sync.idx.b32 	%r58|%p27, %r56, %r69, %r61, %r33;
	// inline asm
	mov.b64 %fd86, {%r58,%r59};
	// inline asm
	add.f64 	%fd18, %fd15, %fd86;
	@%p9 bra 	BB47_7;

	mov.u32 	%r114, %ctaid.x;
	mul.wide.u32 	%rd34, %r114, 8;
	ld.param.u64 	%rd33, [prefix_sum_inc_large_f64_param_3];
	add.s64 	%rd32, %rd33, %rd34;
	mov.b64 	 %rd25, %fd18;
	and.b64  	%rd26, %rd25, -4;
	or.b64  	%rd24, %rd26, 2;
	// inline asm
	st.cg.u64 [%rd32], %rd24;
	// inline asm

BB47_7:
	mov.u32 	%r105, %tid.x;
	shl.b32 	%r104, %r105, 6;
	mov.u32 	%r103, shared_d;
	add.s32 	%r102, %r103, %r104;
	sub.f64 	%fd87, %fd18, %fd8;
	add.f64 	%fd88, %fd87, %fd2;
	add.f64 	%fd89, %fd87, %fd1;
	st.shared.v2.f64 	[%r102], {%fd89, %fd88};
	add.f64 	%fd90, %fd87, %fd4;
	add.f64 	%fd91, %fd87, %fd3;
	st.shared.v2.f64 	[%r102+16], {%fd91, %fd90};
	add.f64 	%fd92, %fd87, %fd6;
	add.f64 	%fd93, %fd87, %fd5;
	st.shared.v2.f64 	[%r102+32], {%fd93, %fd92};
	add.f64 	%fd94, %fd87, %fd8;
	add.f64 	%fd95, %fd87, %fd7;
	st.shared.v2.f64 	[%r102+48], {%fd95, %fd94};
	bar.sync 	0;
	mov.u32 	%r113, %tid.x;
	shl.b32 	%r112, %r113, 4;
	mov.u32 	%r111, shared_d;
	add.s32 	%r110, %r111, %r112;
	mov.u32 	%r109, %ctaid.x;
	mov.u32 	%r108, %tid.x;
	shl.b32 	%r107, %r109, 9;
	add.s32 	%r106, %r107, %r108;
	cvt.u64.u32	%rd31, %r106;
	ld.param.u64 	%rd30, [prefix_sum_inc_large_f64_param_1];
	cvta.to.global.u64 	%rd27, %rd30;
	shl.b64 	%rd28, %rd31, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.shared.v4.u32 	{%r70, %r71, %r72, %r73}, [%r110];
	st.global.v4.u32 	[%rd29], {%r70, %r71, %r72, %r73};
	ld.shared.v4.u32 	{%r78, %r79, %r80, %r81}, [%r110+2048];
	st.global.v4.u32 	[%rd29+2048], {%r78, %r79, %r80, %r81};
	ld.shared.v4.u32 	{%r86, %r87, %r88, %r89}, [%r110+4096];
	st.global.v4.u32 	[%rd29+4096], {%r86, %r87, %r88, %r89};
	ld.shared.v4.u32 	{%r94, %r95, %r96, %r97}, [%r110+6144];
	st.global.v4.u32 	[%rd29+6144], {%r94, %r95, %r96, %r97};
	ret;
}

	// .globl	compress_small
.visible .entry compress_small(
	.param .u64 compress_small_param_0,
	.param .u64 compress_small_param_1,
	.param .u32 compress_small_param_2,
	.param .u64 compress_small_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd3, [compress_small_param_0];
	ld.param.u64 	%rd1, [compress_small_param_1];
	ld.param.u64 	%rd2, [compress_small_param_3];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r16, %tid.x;
	mul.wide.u32 	%rd5, %r16, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r17, [%rd6];
	and.b32  	%r1, %r17, 255;
	bfe.u32 	%r18, %r17, 8, 8;
	add.s32 	%r2, %r18, %r1;
	bfe.u32 	%r19, %r17, 16, 8;
	add.s32 	%r3, %r19, %r2;
	shr.u32 	%r20, %r17, 24;
	add.s32 	%r4, %r20, %r3;
	shl.b32 	%r21, %r16, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	mov.u32 	%r24, 0;
	st.shared.u32 	[%r23], %r24;
	mov.u32 	%r25, %ntid.x;
	add.s32 	%r26, %r25, %r16;
	shl.b32 	%r27, %r26, 2;
	add.s32 	%r28, %r22, %r27;
	st.shared.u32 	[%r28], %r4;
	mov.u32 	%r52, 1;
	setp.lt.u32	%p1, %r25, 2;
	mov.u32 	%r53, %r4;
	@%p1 bra 	BB48_2;

BB48_1:
	bar.sync 	0;
	sub.s32 	%r33, %r26, %r52;
	shl.b32 	%r34, %r33, 2;
	add.s32 	%r35, %r22, %r34;
	ld.shared.u32 	%r36, [%r35];
	ld.shared.u32 	%r37, [%r28];
	add.s32 	%r53, %r36, %r37;
	bar.sync 	0;
	st.shared.u32 	[%r28], %r53;
	shl.b32 	%r52, %r52, 1;
	setp.lt.u32	%p2, %r52, %r25;
	@%p2 bra 	BB48_1;

BB48_2:
	add.s32 	%r39, %r25, -1;
	setp.ne.s32	%p3, %r16, %r39;
	@%p3 bra 	BB48_4;

	cvta.to.global.u64 	%rd7, %rd2;
	st.global.u32 	[%rd7], %r53;

BB48_4:
	sub.s32 	%r11, %r53, %r4;
	add.s32 	%r12, %r11, %r1;
	add.s32 	%r13, %r11, %r2;
	add.s32 	%r14, %r11, %r3;
	setp.eq.s32	%p4, %r1, 0;
	@%p4 bra 	BB48_6;

	cvta.to.global.u64 	%rd8, %rd1;
	mul.wide.u32 	%rd9, %r11, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

BB48_6:
	setp.eq.s32	%p5, %r12, %r13;
	@%p5 bra 	BB48_8;

	add.s32 	%r45, %r21, 1;
	cvta.to.global.u64 	%rd11, %rd1;
	mul.wide.u32 	%rd12, %r12, 4;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %r45;

BB48_8:
	setp.eq.s32	%p6, %r13, %r14;
	@%p6 bra 	BB48_10;

	add.s32 	%r48, %r21, 2;
	cvta.to.global.u64 	%rd14, %rd1;
	mul.wide.u32 	%rd15, %r13, 4;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u32 	[%rd16], %r48;

BB48_10:
	setp.eq.s32	%p7, %r14, %r53;
	@%p7 bra 	BB48_12;

	add.s32 	%r51, %r21, 3;
	cvta.to.global.u64 	%rd17, %rd1;
	mul.wide.u32 	%rd18, %r14, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %r51;

BB48_12:
	ret;
}

	// .globl	compress_large
.visible .entry compress_large(
	.param .u64 compress_large_param_0,
	.param .u64 compress_large_param_1,
	.param .u64 compress_large_param_2,
	.param .u64 compress_large_param_3
)
{
	.reg .pred 	%p<32>;
	.reg .b32 	%r<145>;
	.reg .b64 	%rd<60>;


	ld.param.u64 	%rd8, [compress_large_param_0];
	ld.param.u64 	%rd9, [compress_large_param_1];
	ld.param.u64 	%rd6, [compress_large_param_2];
	ld.param.u64 	%rd7, [compress_large_param_3];
	cvta.to.global.u64 	%rd1, %rd9;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r56, %r1, 7;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r56, %r2;
	cvta.to.global.u64 	%rd10, %rd8;
	mul.wide.u32 	%rd11, %r3, 16;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.v4.u32 	{%r57, %r58, %r59, %r60}, [%rd12];
	mov.u32 	%r143, 0;
	and.b32  	%r4, %r57, 255;
	bfe.u32 	%r66, %r57, 8, 8;
	add.s32 	%r5, %r66, %r4;
	bfe.u32 	%r67, %r57, 16, 8;
	add.s32 	%r6, %r67, %r5;
	shr.u32 	%r68, %r57, 24;
	add.s32 	%r7, %r68, %r6;
	and.b32  	%r69, %r58, 255;
	add.s32 	%r8, %r69, %r7;
	bfe.u32 	%r70, %r58, 8, 8;
	add.s32 	%r9, %r70, %r8;
	bfe.u32 	%r71, %r58, 16, 8;
	add.s32 	%r10, %r71, %r9;
	shr.u32 	%r72, %r58, 24;
	add.s32 	%r11, %r72, %r10;
	and.b32  	%r73, %r59, 255;
	add.s32 	%r12, %r73, %r11;
	bfe.u32 	%r74, %r59, 8, 8;
	add.s32 	%r13, %r74, %r12;
	bfe.u32 	%r75, %r59, 16, 8;
	add.s32 	%r14, %r75, %r13;
	shr.u32 	%r76, %r59, 24;
	add.s32 	%r15, %r76, %r14;
	and.b32  	%r77, %r60, 255;
	add.s32 	%r16, %r77, %r15;
	bfe.u32 	%r78, %r60, 8, 8;
	add.s32 	%r17, %r78, %r16;
	bfe.u32 	%r79, %r60, 16, 8;
	add.s32 	%r18, %r79, %r17;
	shr.u32 	%r80, %r60, 24;
	add.s32 	%r19, %r80, %r18;
	shl.b32 	%r81, %r2, 2;
	mov.u32 	%r82, shared;
	add.s32 	%r20, %r82, %r81;
	st.shared.u32 	[%r20], %r143;
	st.shared.u32 	[%r20+512], %r19;
	bar.sync 	0;
	ld.shared.u32 	%r83, [%r20+508];
	ld.shared.u32 	%r84, [%r20+512];
	add.s32 	%r21, %r83, %r84;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r21;
	bar.sync 	0;
	ld.shared.u32 	%r85, [%r20+504];
	ld.shared.u32 	%r86, [%r20+512];
	add.s32 	%r22, %r85, %r86;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r22;
	bar.sync 	0;
	ld.shared.u32 	%r87, [%r20+496];
	ld.shared.u32 	%r88, [%r20+512];
	add.s32 	%r23, %r87, %r88;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r23;
	bar.sync 	0;
	ld.shared.u32 	%r89, [%r20+480];
	ld.shared.u32 	%r90, [%r20+512];
	add.s32 	%r24, %r89, %r90;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r24;
	bar.sync 	0;
	ld.shared.u32 	%r91, [%r20+448];
	ld.shared.u32 	%r92, [%r20+512];
	add.s32 	%r25, %r91, %r92;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r25;
	bar.sync 	0;
	ld.shared.u32 	%r93, [%r20+384];
	ld.shared.u32 	%r94, [%r20+512];
	add.s32 	%r26, %r93, %r94;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r26;
	bar.sync 	0;
	ld.shared.u32 	%r95, [%r20+256];
	ld.shared.u32 	%r96, [%r20+512];
	add.s32 	%r27, %r95, %r96;
	bar.sync 	0;
	st.shared.u32 	[%r20+512], %r27;
	cvt.u64.u32	%rd2, %r1;
	mul.wide.u32 	%rd13, %r1, 8;
	add.s64 	%rd3, %rd6, %rd13;
	setp.ne.s32	%p1, %r2, 127;
	@%p1 bra 	BB49_2;

	cvt.u64.u32	%rd16, %r27;
	shl.b64 	%rd17, %rd16, 32;
	or.b64  	%rd15, %rd17, 1;
	// inline asm
	st.cg.u64 [%rd3], %rd15;
	// inline asm

BB49_2:
	mov.u32 	%r28, WARP_SZ;
	add.s32 	%r98, %r28, -1;
	and.b32  	%r29, %r98, %r2;
	sub.s32 	%r144, %r29, %r28;
	cvta.to.global.u64 	%rd4, %rd7;
	bra.uni 	BB49_3;

BB49_41:
	add.s32 	%r143, %r34, %r143;
	sub.s32 	%r144, %r144, %r28;

BB49_3:
	cvt.s64.s32	%rd20, %r144;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 3;
	add.s64 	%rd19, %rd6, %rd22;
	// inline asm
	ld.cg.u64 %rd18, [%rd19];
	// inline asm
	cvt.u32.u64	%r33, %rd18;
	setp.eq.s32	%p2, %r33, 0;
	mov.u32 	%r99, -1;
	vote.sync.any.pred 	%p3, %p2, %r99;
	@%p3 bra 	BB49_3;

	setp.eq.s32	%p4, %r33, 2;
	vote.sync.ballot.b32 	%r35, %p4, %r99;
	shr.u64 	%rd23, %rd18, 32;
	cvt.u32.u64	%r34, %rd23;
	setp.eq.s32	%p6, %r35, 0;
	@%p6 bra 	BB49_41;

	clz.b32 	%r102, %r35;
	mov.u32 	%r103, 31;
	sub.s32 	%r104, %r103, %r102;
	setp.lt.u32	%p7, %r29, %r104;
	selp.b32	%r105, 0, %r34, %p7;
	mov.u32 	%r106, 0;
	add.s32 	%r107, %r105, %r143;
	mov.u32 	%r108, 2;
	mov.u32 	%r109, 16;
	shfl.sync.down.b32 	%r111|%p8, %r107, %r109, %r103, %r99;
	add.s32 	%r112, %r111, %r107;
	mov.u32 	%r113, 8;
	shfl.sync.down.b32 	%r114|%p9, %r112, %r113, %r103, %r99;
	add.s32 	%r115, %r114, %r112;
	mov.u32 	%r116, 4;
	shfl.sync.down.b32 	%r117|%p10, %r115, %r116, %r103, %r99;
	add.s32 	%r118, %r117, %r115;
	shfl.sync.down.b32 	%r119|%p11, %r118, %r108, %r103, %r99;
	add.s32 	%r120, %r119, %r118;
	mov.u32 	%r121, 1;
	shfl.sync.down.b32 	%r122|%p12, %r120, %r121, %r103, %r99;
	add.s32 	%r123, %r122, %r120;
	shfl.sync.idx.b32 	%r124|%p13, %r123, %r106, %r103, %r99;
	add.s32 	%r36, %r124, %r27;
	@%p1 bra 	BB49_8;

	cvt.u32.u64	%r125, %rd2;
	cvt.u64.u32	%rd26, %r36;
	shl.b64 	%rd27, %rd26, 32;
	or.b64  	%rd25, %rd27, 2;
	// inline asm
	st.cg.u64 [%rd3], %rd25;
	// inline asm
	mov.u32 	%r126, %nctaid.x;
	add.s32 	%r127, %r126, -1;
	setp.ne.s32	%p15, %r125, %r127;
	@%p15 bra 	BB49_8;

	st.global.u32 	[%rd4], %r36;

BB49_8:
	sub.s32 	%r37, %r36, %r19;
	add.s32 	%r38, %r37, %r4;
	add.s32 	%r39, %r37, %r5;
	add.s32 	%r40, %r37, %r6;
	add.s32 	%r41, %r37, %r7;
	add.s32 	%r42, %r37, %r8;
	add.s32 	%r43, %r37, %r9;
	add.s32 	%r44, %r37, %r10;
	add.s32 	%r45, %r37, %r11;
	add.s32 	%r46, %r37, %r12;
	add.s32 	%r47, %r37, %r13;
	add.s32 	%r48, %r37, %r14;
	add.s32 	%r49, %r37, %r15;
	add.s32 	%r50, %r37, %r16;
	add.s32 	%r51, %r37, %r17;
	add.s32 	%r52, %r37, %r18;
	shl.b32 	%r53, %r3, 4;
	setp.eq.s32	%p16, %r4, 0;
	@%p16 bra 	BB49_10;

	mul.wide.u32 	%rd28, %r37, 4;
	add.s64 	%rd29, %rd1, %rd28;
	st.global.u32 	[%rd29], %r53;

BB49_10:
	setp.eq.s32	%p17, %r38, %r39;
	@%p17 bra 	BB49_12;

	add.s32 	%r128, %r53, 1;
	mul.wide.u32 	%rd30, %r38, 4;
	add.s64 	%rd31, %rd1, %rd30;
	st.global.u32 	[%rd31], %r128;

BB49_12:
	setp.eq.s32	%p18, %r39, %r40;
	@%p18 bra 	BB49_14;

	add.s32 	%r129, %r53, 2;
	mul.wide.u32 	%rd32, %r39, 4;
	add.s64 	%rd33, %rd1, %rd32;
	st.global.u32 	[%rd33], %r129;

BB49_14:
	setp.eq.s32	%p19, %r40, %r41;
	@%p19 bra 	BB49_16;

	add.s32 	%r130, %r53, 3;
	mul.wide.u32 	%rd34, %r40, 4;
	add.s64 	%rd35, %rd1, %rd34;
	st.global.u32 	[%rd35], %r130;

BB49_16:
	setp.eq.s32	%p20, %r41, %r42;
	@%p20 bra 	BB49_18;

	add.s32 	%r131, %r53, 4;
	mul.wide.u32 	%rd36, %r41, 4;
	add.s64 	%rd37, %rd1, %rd36;
	st.global.u32 	[%rd37], %r131;

BB49_18:
	setp.eq.s32	%p21, %r42, %r43;
	@%p21 bra 	BB49_20;

	add.s32 	%r132, %r53, 5;
	mul.wide.u32 	%rd38, %r42, 4;
	add.s64 	%rd39, %rd1, %rd38;
	st.global.u32 	[%rd39], %r132;

BB49_20:
	setp.eq.s32	%p22, %r43, %r44;
	@%p22 bra 	BB49_22;

	add.s32 	%r133, %r53, 6;
	mul.wide.u32 	%rd40, %r43, 4;
	add.s64 	%rd41, %rd1, %rd40;
	st.global.u32 	[%rd41], %r133;

BB49_22:
	setp.eq.s32	%p23, %r44, %r45;
	@%p23 bra 	BB49_24;

	add.s32 	%r134, %r53, 7;
	mul.wide.u32 	%rd42, %r44, 4;
	add.s64 	%rd43, %rd1, %rd42;
	st.global.u32 	[%rd43], %r134;

BB49_24:
	setp.eq.s32	%p24, %r45, %r46;
	@%p24 bra 	BB49_26;

	add.s32 	%r135, %r53, 8;
	mul.wide.u32 	%rd44, %r45, 4;
	add.s64 	%rd45, %rd1, %rd44;
	st.global.u32 	[%rd45], %r135;

BB49_26:
	setp.eq.s32	%p25, %r46, %r47;
	@%p25 bra 	BB49_28;

	add.s32 	%r136, %r53, 9;
	mul.wide.u32 	%rd46, %r46, 4;
	add.s64 	%rd47, %rd1, %rd46;
	st.global.u32 	[%rd47], %r136;

BB49_28:
	setp.eq.s32	%p26, %r47, %r48;
	@%p26 bra 	BB49_30;

	add.s32 	%r137, %r53, 10;
	mul.wide.u32 	%rd48, %r47, 4;
	add.s64 	%rd49, %rd1, %rd48;
	st.global.u32 	[%rd49], %r137;

BB49_30:
	setp.eq.s32	%p27, %r48, %r49;
	@%p27 bra 	BB49_32;

	add.s32 	%r138, %r53, 11;
	mul.wide.u32 	%rd50, %r48, 4;
	add.s64 	%rd51, %rd1, %rd50;
	st.global.u32 	[%rd51], %r138;

BB49_32:
	setp.eq.s32	%p28, %r49, %r50;
	@%p28 bra 	BB49_34;

	add.s32 	%r139, %r53, 12;
	mul.wide.u32 	%rd52, %r49, 4;
	add.s64 	%rd53, %rd1, %rd52;
	st.global.u32 	[%rd53], %r139;

BB49_34:
	setp.eq.s32	%p29, %r50, %r51;
	@%p29 bra 	BB49_36;

	add.s32 	%r140, %r53, 13;
	mul.wide.u32 	%rd54, %r50, 4;
	add.s64 	%rd55, %rd1, %rd54;
	st.global.u32 	[%rd55], %r140;

BB49_36:
	setp.eq.s32	%p30, %r51, %r52;
	@%p30 bra 	BB49_38;

	add.s32 	%r141, %r53, 14;
	mul.wide.u32 	%rd56, %r51, 4;
	add.s64 	%rd57, %rd1, %rd56;
	st.global.u32 	[%rd57], %r141;

BB49_38:
	setp.eq.s32	%p31, %r52, %r36;
	@%p31 bra 	BB49_40;

	add.s32 	%r142, %r53, 15;
	mul.wide.u32 	%rd58, %r52, 4;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u32 	[%rd59], %r142;

BB49_40:
	ret;
}

	// .globl	mkperm_phase_1_tiny
.visible .entry mkperm_phase_1_tiny(
	.param .u64 mkperm_phase_1_tiny_param_0,
	.param .u64 mkperm_phase_1_tiny_param_1,
	.param .u32 mkperm_phase_1_tiny_param_2,
	.param .u32 mkperm_phase_1_tiny_param_3,
	.param .u32 mkperm_phase_1_tiny_param_4
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd3, [mkperm_phase_1_tiny_param_0];
	ld.param.u64 	%rd4, [mkperm_phase_1_tiny_param_1];
	ld.param.u32 	%r26, [mkperm_phase_1_tiny_param_2];
	ld.param.u32 	%r28, [mkperm_phase_1_tiny_param_3];
	ld.param.u32 	%r27, [mkperm_phase_1_tiny_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r29, %ctaid.x;
	mul.lo.s32 	%r2, %r29, %r28;
	add.s32 	%r3, %r2, %r28;
	mov.u32 	%r30, WARP_SZ;
	mov.u32 	%r4, %ntid.x;
	div.u32 	%r31, %r4, %r30;
	div.u32 	%r5, %r1, %r30;
	mul.lo.s32 	%r6, %r31, %r27;
	setp.ge.u32	%p3, %r1, %r6;
	@%p3 bra 	BB50_3;

	mov.u32 	%r64, %r1;

BB50_2:
	shl.b32 	%r32, %r64, 2;
	mov.u32 	%r33, shared;
	add.s32 	%r34, %r33, %r32;
	mov.u32 	%r35, 0;
	st.shared.u32 	[%r34], %r35;
	add.s32 	%r64, %r64, %r4;
	setp.lt.u32	%p4, %r64, %r6;
	@%p4 bra 	BB50_2;

BB50_3:
	bar.sync 	0;
	add.s32 	%r65, %r2, %r1;
	setp.ge.u32	%p5, %r65, %r3;
	@%p5 bra 	BB50_12;

	add.s32 	%r37, %r30, -1;
	and.b32  	%r10, %r37, %r1;
	mul.lo.s32 	%r11, %r5, %r27;
	cvta.to.global.u64 	%rd5, %rd3;

BB50_5:
	setp.lt.u32	%p6, %r65, %r26;
	mov.u32 	%r38, -1;
	vote.sync.ballot.b32 	%r66, %p6, %r38;
	setp.ge.u32	%p7, %r65, %r26;
	@%p7 bra 	BB50_11;

	mul.wide.u32 	%rd6, %r65, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r15, [%rd7];

BB50_7:
	neg.s32 	%r39, %r66;
	and.b32  	%r40, %r66, %r39;
	clz.b32 	%r41, %r40;
	mov.u32 	%r42, 31;
	sub.s32 	%r43, %r42, %r41;
	shfl.sync.idx.b32 	%r44|%p8, %r15, %r43, %r42, %r66;
	setp.eq.s32	%p9, %r44, %r15;
	vote.sync.ballot.b32 	%r17, %p9, %r66;
	xor.b32  	%r66, %r17, %r66;
	setp.ne.s32	%p11, %r44, %r15;
	@%p11 bra 	BB50_7;

	neg.s32 	%r46, %r17;
	mov.u32 	%r67, 0;
	and.b32  	%r47, %r17, %r46;
	clz.b32 	%r48, %r47;
	sub.s32 	%r19, %r42, %r48;
	setp.ne.s32	%p12, %r10, %r19;
	@%p12 bra 	BB50_10;

	add.s32 	%r50, %r15, %r11;
	shl.b32 	%r51, %r50, 2;
	mov.u32 	%r52, shared;
	add.s32 	%r53, %r52, %r51;
	popc.b32 	%r54, %r17;
	ld.shared.u32 	%r67, [%r53];
	add.s32 	%r55, %r54, %r67;
	st.shared.u32 	[%r53], %r55;

BB50_10:
	shfl.sync.idx.b32 	%r57|%p13, %r67, %r19, %r42, %r17;

BB50_11:
	bar.warp.sync 	-1;
	add.s32 	%r65, %r65, %r4;
	setp.lt.u32	%p14, %r65, %r3;
	@%p14 bra 	BB50_5;

BB50_12:
	mov.u32 	%r68, %tid.x;
	setp.lt.u32	%p2, %r68, %r6;
	bar.sync 	0;
	mul.lo.s32 	%r59, %r6, %r29;
	cvt.u64.u32	%rd1, %r59;
	cvta.to.global.u64 	%rd2, %rd4;
	@!%p2 bra 	BB50_14;
	bra.uni 	BB50_13;

BB50_13:
	shl.b32 	%r60, %r68, 2;
	mov.u32 	%r61, shared;
	add.s32 	%r62, %r61, %r60;
	ld.shared.u32 	%r63, [%r62];
	cvt.u64.u32	%rd8, %r68;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	st.global.u32 	[%rd11], %r63;
	add.s32 	%r68, %r68, %r4;
	setp.lt.u32	%p15, %r68, %r6;
	@%p15 bra 	BB50_13;

BB50_14:
	ret;
}

	// .globl	mkperm_phase_1_small
.visible .entry mkperm_phase_1_small(
	.param .u64 mkperm_phase_1_small_param_0,
	.param .u64 mkperm_phase_1_small_param_1,
	.param .u32 mkperm_phase_1_small_param_2,
	.param .u32 mkperm_phase_1_small_param_3,
	.param .u32 mkperm_phase_1_small_param_4
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_small_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_small_param_1];
	ld.param.u32 	%r23, [mkperm_phase_1_small_param_2];
	ld.param.u32 	%r25, [mkperm_phase_1_small_param_3];
	ld.param.u32 	%r24, [mkperm_phase_1_small_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r26, %ctaid.x;
	mul.lo.s32 	%r2, %r26, %r25;
	add.s32 	%r3, %r2, %r25;
	mov.u32 	%r4, %tid.x;
	setp.ge.u32	%p3, %r4, %r24;
	@%p3 bra 	BB51_3;

	mov.u32 	%r57, %r4;

BB51_2:
	shl.b32 	%r27, %r57, 2;
	mov.u32 	%r28, shared;
	add.s32 	%r29, %r28, %r27;
	mov.u32 	%r30, 0;
	st.shared.u32 	[%r29], %r30;
	add.s32 	%r57, %r57, %r1;
	setp.lt.u32	%p4, %r57, %r24;
	@%p4 bra 	BB51_2;

BB51_3:
	bar.sync 	0;
	add.s32 	%r58, %r2, %r4;
	setp.ge.u32	%p5, %r58, %r3;
	@%p5 bra 	BB51_12;

	mov.u32 	%r31, WARP_SZ;
	add.s32 	%r32, %r31, -1;
	and.b32  	%r8, %r32, %r4;
	cvta.to.global.u64 	%rd1, %rd4;

BB51_5:
	setp.lt.u32	%p6, %r58, %r23;
	mov.u32 	%r33, -1;
	vote.sync.ballot.b32 	%r59, %p6, %r33;
	setp.ge.u32	%p7, %r58, %r23;
	@%p7 bra 	BB51_11;

	mul.wide.u32 	%rd6, %r58, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r12, [%rd7];

BB51_7:
	neg.s32 	%r34, %r59;
	and.b32  	%r35, %r59, %r34;
	clz.b32 	%r36, %r35;
	mov.u32 	%r37, 31;
	sub.s32 	%r38, %r37, %r36;
	shfl.sync.idx.b32 	%r39|%p8, %r12, %r38, %r37, %r59;
	setp.eq.s32	%p9, %r39, %r12;
	vote.sync.ballot.b32 	%r14, %p9, %r59;
	xor.b32  	%r59, %r14, %r59;
	setp.ne.s32	%p11, %r39, %r12;
	@%p11 bra 	BB51_7;

	neg.s32 	%r41, %r14;
	mov.u32 	%r60, 0;
	and.b32  	%r42, %r14, %r41;
	clz.b32 	%r43, %r42;
	sub.s32 	%r16, %r37, %r43;
	setp.ne.s32	%p12, %r8, %r16;
	@%p12 bra 	BB51_10;

	popc.b32 	%r45, %r14;
	shl.b32 	%r46, %r12, 2;
	mov.u32 	%r47, shared;
	add.s32 	%r48, %r47, %r46;
	atom.shared.add.u32 	%r60, [%r48], %r45;

BB51_10:
	shfl.sync.idx.b32 	%r50|%p13, %r60, %r16, %r37, %r14;

BB51_11:
	add.s32 	%r58, %r58, %r1;
	setp.lt.u32	%p14, %r58, %r3;
	@%p14 bra 	BB51_5;

BB51_12:
	mov.u32 	%r61, %tid.x;
	setp.lt.u32	%p2, %r61, %r24;
	bar.sync 	0;
	mul.lo.s32 	%r52, %r26, %r24;
	cvt.u64.u32	%rd2, %r52;
	cvta.to.global.u64 	%rd3, %rd5;
	@!%p2 bra 	BB51_14;
	bra.uni 	BB51_13;

BB51_13:
	shl.b32 	%r53, %r61, 2;
	mov.u32 	%r54, shared;
	add.s32 	%r55, %r54, %r53;
	ld.shared.u32 	%r56, [%r55];
	cvt.u64.u32	%rd8, %r61;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd3, %rd10;
	st.global.u32 	[%rd11], %r56;
	add.s32 	%r61, %r61, %r1;
	setp.lt.u32	%p15, %r61, %r24;
	@%p15 bra 	BB51_13;

BB51_14:
	ret;
}

	// .globl	mkperm_phase_1_large
.visible .entry mkperm_phase_1_large(
	.param .u64 mkperm_phase_1_large_param_0,
	.param .u64 mkperm_phase_1_large_param_1,
	.param .u32 mkperm_phase_1_large_param_2,
	.param .u32 mkperm_phase_1_large_param_3,
	.param .u32 mkperm_phase_1_large_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_large_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_large_param_1];
	ld.param.u32 	%r18, [mkperm_phase_1_large_param_2];
	ld.param.u32 	%r20, [mkperm_phase_1_large_param_3];
	ld.param.u32 	%r19, [mkperm_phase_1_large_param_4];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r21, %r1, %r20;
	add.s32 	%r2, %r21, %r20;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r41, %r21, %r3;
	setp.ge.u32	%p2, %r41, %r2;
	@%p2 bra 	BB52_9;

	mov.u32 	%r22, WARP_SZ;
	add.s32 	%r23, %r22, -1;
	and.b32  	%r5, %r23, %r3;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.u64.u32	%rd1, %r24;
	mov.u32 	%r6, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd6, %rd4;

BB52_2:
	setp.lt.u32	%p3, %r41, %r18;
	mov.u32 	%r25, -1;
	vote.sync.ballot.b32 	%r42, %p3, %r25;
	setp.ge.u32	%p4, %r41, %r18;
	@%p4 bra 	BB52_8;

	mul.wide.u32 	%rd7, %r41, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r10, [%rd8];
	cvt.u64.u32	%rd3, %r10;

BB52_4:
	neg.s32 	%r26, %r42;
	and.b32  	%r27, %r42, %r26;
	clz.b32 	%r28, %r27;
	mov.u32 	%r29, 31;
	sub.s32 	%r30, %r29, %r28;
	shfl.sync.idx.b32 	%r31|%p5, %r10, %r30, %r29, %r42;
	cvt.u32.u64	%r32, %rd3;
	setp.eq.s32	%p6, %r31, %r32;
	vote.sync.ballot.b32 	%r12, %p6, %r42;
	xor.b32  	%r42, %r12, %r42;
	setp.ne.s32	%p8, %r31, %r32;
	@%p8 bra 	BB52_4;

	neg.s32 	%r34, %r12;
	mov.u32 	%r43, 0;
	and.b32  	%r35, %r12, %r34;
	clz.b32 	%r36, %r35;
	sub.s32 	%r14, %r29, %r36;
	setp.ne.s32	%p9, %r5, %r14;
	@%p9 bra 	BB52_7;

	popc.b32 	%r38, %r12;
	add.s64 	%rd9, %rd3, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	atom.global.add.u32 	%r43, [%rd11], %r38;

BB52_7:
	shfl.sync.idx.b32 	%r40|%p10, %r43, %r14, %r29, %r12;

BB52_8:
	add.s32 	%r41, %r41, %r6;
	setp.lt.u32	%p11, %r41, %r2;
	@%p11 bra 	BB52_2;

BB52_9:
	ret;
}

	// .globl	mkperm_phase_3
.visible .entry mkperm_phase_3(
	.param .u64 mkperm_phase_3_param_0,
	.param .u32 mkperm_phase_3_param_1,
	.param .u32 mkperm_phase_3_param_2,
	.param .u32 mkperm_phase_3_param_3,
	.param .u64 mkperm_phase_3_param_4,
	.param .u64 mkperm_phase_3_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [mkperm_phase_3_param_0];
	ld.param.u32 	%r20, [mkperm_phase_3_param_1];
	ld.param.u32 	%r21, [mkperm_phase_3_param_2];
	ld.param.u32 	%r22, [mkperm_phase_3_param_3];
	ld.param.u64 	%rd2, [mkperm_phase_3_param_4];
	ld.param.u64 	%rd3, [mkperm_phase_3_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mad.lo.s32 	%r52, %r2, %r23, %r1;
	setp.ge.u32	%p3, %r52, %r21;
	@%p3 bra 	BB53_13;

	shl.b32 	%r24, %r1, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r4, %r25, %r24;
	mov.u32 	%r26, WARP_SZ;
	add.s32 	%r27, %r26, -1;
	and.b32  	%r28, %r27, %r1;
	mov.u32 	%r29, 32;
	sub.s32 	%r5, %r29, %r28;
	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r6, %r30, %r2;
	cvta.to.global.u64 	%rd10, %rd2;
	cvta.to.global.u64 	%rd11, %rd3;

BB53_2:
	setp.ge.u32	%p4, %r52, %r20;
	mov.u32 	%r53, %r22;
	@%p4 bra 	BB53_4;

	cvta.to.global.u64 	%rd4, %rd1;
	mul.wide.u32 	%rd5, %r52, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u32 	%r53, [%rd6];

BB53_4:
	add.s32 	%r32, %r1, 1;
	setp.lt.u32	%p1, %r32, %r2;
	st.shared.u32 	[%r4], %r53;
	bar.sync 	0;
	@%p1 bra 	BB53_7;
	bra.uni 	BB53_5;

BB53_7:
	ld.shared.u32 	%r54, [%r4+4];
	bra.uni 	BB53_8;

BB53_5:
	add.s32 	%r10, %r52, 1;
	setp.ge.u32	%p5, %r10, %r20;
	mov.u32 	%r54, %r22;
	@%p5 bra 	BB53_8;

	cvta.to.global.u64 	%rd7, %rd1;
	mul.wide.u32 	%rd8, %r10, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%r54, [%rd9];

BB53_8:
	setp.ne.s32	%p6, %r54, %r53;
	mov.u32 	%r34, -1;
	vote.sync.ballot.b32 	%r14, %p6, %r34;
	setp.eq.s32	%p7, %r54, %r53;
	@%p7 bra 	BB53_12;

	neg.s32 	%r36, %r14;
	mov.u32 	%r35, 0;
	and.b32  	%r37, %r14, %r36;
	clz.b32 	%r38, %r37;
	mov.u32 	%r39, 31;
	sub.s32 	%r16, %r39, %r38;
	setp.ne.s32	%p8, %r28, %r16;
	mov.u32 	%r55, %r35;
	@%p8 bra 	BB53_11;

	popc.b32 	%r44, %r14;
	atom.global.add.u32 	%r55, [%rd10], %r44;

BB53_11:
	shfl.sync.idx.b32 	%r46|%p9, %r55, %r16, %r39, %r14;
	shl.b32 	%r47, %r14, %r5;
	popc.b32 	%r48, %r47;
	add.s32 	%r49, %r46, %r48;
	mul.wide.u32 	%rd12, %r49, 16;
	add.s64 	%rd13, %rd11, %rd12;
	sub.s32 	%r50, %r54, %r53;
	st.global.v4.u32 	[%rd13], {%r52, %r53, %r50, %r35};

BB53_12:
	add.s32 	%r52, %r6, %r52;
	setp.lt.u32	%p10, %r52, %r21;
	@%p10 bra 	BB53_2;

BB53_13:
	ret;
}

	// .globl	mkperm_phase_4_tiny
.visible .entry mkperm_phase_4_tiny(
	.param .u64 mkperm_phase_4_tiny_param_0,
	.param .u64 mkperm_phase_4_tiny_param_1,
	.param .u64 mkperm_phase_4_tiny_param_2,
	.param .u32 mkperm_phase_4_tiny_param_3,
	.param .u32 mkperm_phase_4_tiny_param_4,
	.param .u32 mkperm_phase_4_tiny_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd4, [mkperm_phase_4_tiny_param_0];
	ld.param.u64 	%rd6, [mkperm_phase_4_tiny_param_1];
	ld.param.u64 	%rd5, [mkperm_phase_4_tiny_param_2];
	ld.param.u32 	%r25, [mkperm_phase_4_tiny_param_3];
	ld.param.u32 	%r27, [mkperm_phase_4_tiny_param_4];
	ld.param.u32 	%r26, [mkperm_phase_4_tiny_param_5];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r28, %ctaid.x;
	mul.lo.s32 	%r1, %r28, %r27;
	add.s32 	%r2, %r1, %r27;
	mov.u32 	%r3, WARP_SZ;
	mov.u32 	%r4, %ntid.x;
	div.u32 	%r29, %r4, %r3;
	mov.u32 	%r5, %tid.x;
	div.u32 	%r6, %r5, %r3;
	mul.lo.s32 	%r30, %r28, %r26;
	mul.lo.s32 	%r31, %r30, %r29;
	cvt.u64.u32	%rd2, %r31;
	mul.lo.s32 	%r7, %r29, %r26;
	setp.ge.u32	%p2, %r5, %r7;
	@%p2 bra 	BB54_3;

	mov.u32 	%r61, %r5;

BB54_2:
	cvt.u64.u32	%rd7, %r61;
	add.s64 	%rd8, %rd7, %rd2;
	shl.b64 	%rd9, %rd8, 2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r32, [%rd10];
	shl.b32 	%r33, %r61, 2;
	mov.u32 	%r34, shared;
	add.s32 	%r35, %r34, %r33;
	st.shared.u32 	[%r35], %r32;
	add.s32 	%r61, %r61, %r4;
	setp.lt.u32	%p3, %r61, %r7;
	@%p3 bra 	BB54_2;

BB54_3:
	bar.sync 	0;
	add.s32 	%r62, %r1, %r5;
	setp.ge.u32	%p4, %r62, %r2;
	@%p4 bra 	BB54_12;

	add.s32 	%r36, %r3, -1;
	and.b32  	%r11, %r36, %r5;
	mov.u32 	%r37, 32;
	sub.s32 	%r12, %r37, %r11;
	cvta.to.global.u64 	%rd3, %rd5;
	mul.lo.s32 	%r13, %r6, %r26;
	cvta.to.global.u64 	%rd11, %rd4;

BB54_5:
	setp.lt.u32	%p5, %r62, %r25;
	mov.u32 	%r38, -1;
	vote.sync.ballot.b32 	%r63, %p5, %r38;
	setp.ge.u32	%p6, %r62, %r25;
	@%p6 bra 	BB54_11;

	mul.wide.u32 	%rd12, %r62, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%r17, [%rd13];

BB54_7:
	neg.s32 	%r39, %r63;
	and.b32  	%r40, %r63, %r39;
	clz.b32 	%r41, %r40;
	mov.u32 	%r42, 31;
	sub.s32 	%r43, %r42, %r41;
	shfl.sync.idx.b32 	%r44|%p7, %r17, %r43, %r42, %r63;
	setp.eq.s32	%p8, %r44, %r17;
	vote.sync.ballot.b32 	%r19, %p8, %r63;
	xor.b32  	%r63, %r19, %r63;
	setp.ne.s32	%p10, %r44, %r17;
	@%p10 bra 	BB54_7;

	neg.s32 	%r46, %r19;
	mov.u32 	%r64, 0;
	and.b32  	%r47, %r19, %r46;
	clz.b32 	%r48, %r47;
	sub.s32 	%r21, %r42, %r48;
	setp.ne.s32	%p11, %r11, %r21;
	@%p11 bra 	BB54_10;

	add.s32 	%r50, %r17, %r13;
	shl.b32 	%r51, %r50, 2;
	mov.u32 	%r52, shared;
	add.s32 	%r53, %r52, %r51;
	popc.b32 	%r54, %r19;
	ld.shared.u32 	%r64, [%r53];
	add.s32 	%r55, %r54, %r64;
	st.shared.u32 	[%r53], %r55;

BB54_10:
	shfl.sync.idx.b32 	%r57|%p12, %r64, %r21, %r42, %r19;
	shl.b32 	%r58, %r19, %r12;
	popc.b32 	%r59, %r58;
	add.s32 	%r60, %r57, %r59;
	mul.wide.u32 	%rd14, %r60, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r62;

BB54_11:
	add.s32 	%r62, %r62, %r4;
	setp.lt.u32	%p13, %r62, %r2;
	@%p13 bra 	BB54_5;

BB54_12:
	ret;
}

	// .globl	mkperm_phase_4_small
.visible .entry mkperm_phase_4_small(
	.param .u64 mkperm_phase_4_small_param_0,
	.param .u64 mkperm_phase_4_small_param_1,
	.param .u64 mkperm_phase_4_small_param_2,
	.param .u32 mkperm_phase_4_small_param_3,
	.param .u32 mkperm_phase_4_small_param_4,
	.param .u32 mkperm_phase_4_small_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<58>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [mkperm_phase_4_small_param_0];
	ld.param.u64 	%rd7, [mkperm_phase_4_small_param_1];
	ld.param.u64 	%rd6, [mkperm_phase_4_small_param_2];
	ld.param.u32 	%r21, [mkperm_phase_4_small_param_3];
	ld.param.u32 	%r23, [mkperm_phase_4_small_param_4];
	ld.param.u32 	%r22, [mkperm_phase_4_small_param_5];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mul.lo.s32 	%r2, %r24, %r23;
	add.s32 	%r3, %r2, %r23;
	mul.lo.s32 	%r25, %r24, %r22;
	cvt.u64.u32	%rd2, %r25;
	mov.u32 	%r4, %tid.x;
	setp.ge.u32	%p2, %r4, %r22;
	@%p2 bra 	BB55_3;

	mov.u32 	%r54, %r4;

BB55_2:
	cvt.u64.u32	%rd8, %r54;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.u32 	%r26, [%rd11];
	shl.b32 	%r27, %r54, 2;
	mov.u32 	%r28, shared;
	add.s32 	%r29, %r28, %r27;
	st.shared.u32 	[%r29], %r26;
	add.s32 	%r54, %r54, %r1;
	setp.lt.u32	%p3, %r54, %r22;
	@%p3 bra 	BB55_2;

BB55_3:
	bar.sync 	0;
	add.s32 	%r55, %r2, %r4;
	setp.ge.u32	%p4, %r55, %r3;
	@%p4 bra 	BB55_12;

	mov.u32 	%r30, WARP_SZ;
	add.s32 	%r31, %r30, -1;
	and.b32  	%r8, %r31, %r4;
	mov.u32 	%r32, 32;
	sub.s32 	%r9, %r32, %r8;
	cvta.to.global.u64 	%rd3, %rd6;
	cvta.to.global.u64 	%rd4, %rd5;

BB55_5:
	setp.lt.u32	%p5, %r55, %r21;
	mov.u32 	%r33, -1;
	vote.sync.ballot.b32 	%r56, %p5, %r33;
	setp.ge.u32	%p6, %r55, %r21;
	@%p6 bra 	BB55_11;

	mul.wide.u32 	%rd12, %r55, 4;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.u32 	%r13, [%rd13];

BB55_7:
	neg.s32 	%r34, %r56;
	and.b32  	%r35, %r56, %r34;
	clz.b32 	%r36, %r35;
	mov.u32 	%r37, 31;
	sub.s32 	%r38, %r37, %r36;
	shfl.sync.idx.b32 	%r39|%p7, %r13, %r38, %r37, %r56;
	setp.eq.s32	%p8, %r39, %r13;
	vote.sync.ballot.b32 	%r15, %p8, %r56;
	xor.b32  	%r56, %r15, %r56;
	setp.ne.s32	%p10, %r39, %r13;
	@%p10 bra 	BB55_7;

	neg.s32 	%r41, %r15;
	mov.u32 	%r57, 0;
	and.b32  	%r42, %r15, %r41;
	clz.b32 	%r43, %r42;
	sub.s32 	%r17, %r37, %r43;
	setp.ne.s32	%p11, %r8, %r17;
	@%p11 bra 	BB55_10;

	popc.b32 	%r45, %r15;
	shl.b32 	%r46, %r13, 2;
	mov.u32 	%r47, shared;
	add.s32 	%r48, %r47, %r46;
	atom.shared.add.u32 	%r57, [%r48], %r45;

BB55_10:
	shfl.sync.idx.b32 	%r50|%p12, %r57, %r17, %r37, %r15;
	shl.b32 	%r51, %r15, %r9;
	popc.b32 	%r52, %r51;
	add.s32 	%r53, %r50, %r52;
	mul.wide.u32 	%rd14, %r53, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r55;

BB55_11:
	add.s32 	%r55, %r55, %r1;
	setp.lt.u32	%p13, %r55, %r3;
	@%p13 bra 	BB55_5;

BB55_12:
	ret;
}

	// .globl	mkperm_phase_4_large
.visible .entry mkperm_phase_4_large(
	.param .u64 mkperm_phase_4_large_param_0,
	.param .u64 mkperm_phase_4_large_param_1,
	.param .u64 mkperm_phase_4_large_param_2,
	.param .u32 mkperm_phase_4_large_param_3,
	.param .u32 mkperm_phase_4_large_param_4,
	.param .u32 mkperm_phase_4_large_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<49>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd3, [mkperm_phase_4_large_param_0];
	ld.param.u64 	%rd4, [mkperm_phase_4_large_param_1];
	ld.param.u64 	%rd5, [mkperm_phase_4_large_param_2];
	ld.param.u32 	%r18, [mkperm_phase_4_large_param_3];
	ld.param.u32 	%r20, [mkperm_phase_4_large_param_4];
	ld.param.u32 	%r19, [mkperm_phase_4_large_param_5];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r21, %r2, %r20;
	add.s32 	%r3, %r21, %r20;
	add.s32 	%r46, %r21, %r1;
	setp.ge.u32	%p2, %r46, %r3;
	@%p2 bra 	BB56_9;

	mov.u32 	%r22, WARP_SZ;
	add.s32 	%r23, %r22, -1;
	and.b32  	%r5, %r23, %r1;
	mov.u32 	%r24, 32;
	sub.s32 	%r6, %r24, %r5;
	mul.lo.s32 	%r25, %r2, %r19;
	cvt.u64.u32	%rd1, %r25;
	cvta.to.global.u64 	%rd6, %rd3;
	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd13, %rd5;

BB56_2:
	setp.lt.u32	%p3, %r46, %r18;
	mov.u32 	%r26, -1;
	vote.sync.ballot.b32 	%r47, %p3, %r26;
	setp.ge.u32	%p4, %r46, %r18;
	@%p4 bra 	BB56_8;

	mul.wide.u32 	%rd7, %r46, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r10, [%rd8];
	cvt.u64.u32	%rd2, %r10;

BB56_4:
	neg.s32 	%r27, %r47;
	and.b32  	%r28, %r47, %r27;
	clz.b32 	%r29, %r28;
	mov.u32 	%r30, 31;
	sub.s32 	%r31, %r30, %r29;
	shfl.sync.idx.b32 	%r32|%p5, %r10, %r31, %r30, %r47;
	cvt.u32.u64	%r33, %rd2;
	setp.eq.s32	%p6, %r32, %r33;
	vote.sync.ballot.b32 	%r12, %p6, %r47;
	xor.b32  	%r47, %r12, %r47;
	setp.ne.s32	%p8, %r32, %r33;
	@%p8 bra 	BB56_4;

	neg.s32 	%r35, %r12;
	mov.u32 	%r48, 0;
	and.b32  	%r36, %r12, %r35;
	clz.b32 	%r37, %r36;
	sub.s32 	%r14, %r30, %r37;
	setp.ne.s32	%p9, %r5, %r14;
	@%p9 bra 	BB56_7;

	popc.b32 	%r39, %r12;
	add.s64 	%rd10, %rd2, %rd1;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd9, %rd11;
	atom.global.add.u32 	%r48, [%rd12], %r39;

BB56_7:
	shfl.sync.idx.b32 	%r41|%p10, %r48, %r14, %r30, %r12;
	shl.b32 	%r42, %r12, %r6;
	popc.b32 	%r43, %r42;
	add.s32 	%r44, %r41, %r43;
	mul.wide.u32 	%rd14, %r44, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %r46;

BB56_8:
	mov.u32 	%r45, %ntid.x;
	add.s32 	%r46, %r46, %r45;
	setp.lt.u32	%p11, %r46, %r3;
	@%p11 bra 	BB56_2;

BB56_9:
	ret;
}

	// .globl	transpose
.visible .entry transpose(
	.param .u64 transpose_param_0,
	.param .u64 transpose_param_1,
	.param .u32 transpose_param_2,
	.param .u32 transpose_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r9, [transpose_param_2];
	ld.param.u32 	%r10, [transpose_param_3];
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r1, %r11, 4;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r12, %ctaid.y;
	shl.b32 	%r4, %r12, 4;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.u32	%p1, %r6, %r9;
	setp.ge.u32	%p2, %r3, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB57_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r6, %r10, %r3;
	mul.wide.u32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r14, [%rd5];
	mad.lo.s32 	%r15, %r5, 17, %r2;
	shl.b32 	%r16, %r15, 2;
	mov.u32 	%r17, shared;
	add.s32 	%r18, %r17, %r16;
	st.shared.u32 	[%r18], %r14;

BB57_2:
	bar.sync 	0;
	add.s32 	%r7, %r5, %r1;
	setp.ge.u32	%p4, %r7, %r10;
	add.s32 	%r8, %r4, %r2;
	setp.ge.u32	%p5, %r8, %r9;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB57_4;

	mad.lo.s32 	%r19, %r2, 17, %r5;
	shl.b32 	%r20, %r19, 2;
	mov.u32 	%r21, shared;
	add.s32 	%r22, %r21, %r20;
	ld.shared.u32 	%r23, [%r22];
	mad.lo.s32 	%r24, %r7, %r9, %r8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r23;

BB57_4:
	ret;
}

	// .globl	poke_u8
.visible .entry poke_u8(
	.param .u64 poke_u8_param_0,
	.param .u8 poke_u8_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u8_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.param.u8 	%rs1, [poke_u8_param_1];
	st.global.u8 	[%rd2], %rs1;
	ret;
}

	// .globl	poke_u16
.visible .entry poke_u16(
	.param .u64 poke_u16_param_0,
	.param .u16 poke_u16_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u16_param_0];
	ld.param.u16 	%rs1, [poke_u16_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u16 	[%rd2], %rs1;
	ret;
}

	// .globl	poke_u32
.visible .entry poke_u32(
	.param .u64 poke_u32_param_0,
	.param .u32 poke_u32_param_1
)
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u32_param_0];
	ld.param.u32 	%r1, [poke_u32_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u32 	[%rd2], %r1;
	ret;
}

	// .globl	poke_u64
.visible .entry poke_u64(
	.param .u64 poke_u64_param_0,
	.param .u64 poke_u64_param_1
)
{
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [poke_u64_param_0];
	ld.param.u64 	%rd2, [poke_u64_param_1];
	cvta.to.global.u64 	%rd3, %rd1;
	st.global.u64 	[%rd3], %rd2;
	ret;
}

	// .globl	fill_64
.visible .entry fill_64(
	.param .u64 fill_64_param_0,
	.param .u32 fill_64_param_1,
	.param .u64 fill_64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [fill_64_param_0];
	ld.param.u32 	%r6, [fill_64_param_1];
	ld.param.u64 	%rd3, [fill_64_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r1, %r7, %r8;
	setp.ge.u32	%p1, %r10, %r6;
	@%p1 bra 	BB62_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r9, %r1;
	cvta.to.global.u64 	%rd1, %rd2;

BB62_2:
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r3, %r10;
	setp.lt.u32	%p2, %r10, %r6;
	@%p2 bra 	BB62_2;

BB62_3:
	ret;
}

	// .globl	aggregate
.visible .entry aggregate(
	.param .u64 aggregate_param_0,
	.param .u64 aggregate_param_1,
	.param .u32 aggregate_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [aggregate_param_0];
	ld.param.u64 	%rd5, [aggregate_param_1];
	ld.param.u32 	%r2, [aggregate_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.u32	%p1, %r1, %r2;
	@%p1 bra 	BB63_20;

	cvta.to.global.u64 	%rd6, %rd5;
	mul.wide.u32 	%rd7, %r1, 16;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v2.u32 	{%r7, %r8}, [%rd8];
	cvt.u64.u32	%rd9, %r8;
	cvta.to.global.u64 	%rd10, %rd4;
	add.s64 	%rd1, %rd10, %rd9;
	ld.global.u64 	%rd2, [%rd8+8];
	setp.gt.s32	%p2, %r7, 0;
	@%p2 bra 	BB63_9;

	setp.gt.s32	%p8, %r7, -3;
	@%p8 bra 	BB63_6;

	setp.eq.s32	%p11, %r7, -8;
	@%p11 bra 	BB63_19;
	bra.uni 	BB63_4;

BB63_19:
	ld.u64 	%rd11, [%rd2];
	st.global.u64 	[%rd1], %rd11;
	bra.uni 	BB63_20;

BB63_9:
	setp.gt.s32	%p3, %r7, 3;
	@%p3 bra 	BB63_13;

	setp.eq.s32	%p6, %r7, 1;
	@%p6 bra 	BB63_17;
	bra.uni 	BB63_11;

BB63_17:
	st.global.u8 	[%rd1], %rd2;
	bra.uni 	BB63_20;

BB63_6:
	setp.eq.s32	%p9, %r7, -2;
	@%p9 bra 	BB63_18;
	bra.uni 	BB63_7;

BB63_18:
	ld.u16 	%rs2, [%rd2];
	st.global.u16 	[%rd1], %rs2;
	bra.uni 	BB63_20;

BB63_13:
	setp.eq.s32	%p4, %r7, 4;
	@%p4 bra 	BB63_16;
	bra.uni 	BB63_14;

BB63_16:
	st.global.u32 	[%rd1], %rd2;
	bra.uni 	BB63_20;

BB63_4:
	setp.eq.s32	%p12, %r7, -4;
	@%p12 bra 	BB63_5;
	bra.uni 	BB63_20;

BB63_5:
	ld.u32 	%r10, [%rd2];
	st.global.u32 	[%rd1], %r10;
	bra.uni 	BB63_20;

BB63_11:
	setp.eq.s32	%p7, %r7, 2;
	@%p7 bra 	BB63_12;
	bra.uni 	BB63_20;

BB63_12:
	st.global.u16 	[%rd1], %rd2;
	bra.uni 	BB63_20;

BB63_7:
	setp.eq.s32	%p10, %r7, -1;
	@%p10 bra 	BB63_8;
	bra.uni 	BB63_20;

BB63_8:
	ld.u8 	%rs1, [%rd2];
	st.global.u8 	[%rd1], %rs1;
	bra.uni 	BB63_20;

BB63_14:
	setp.ne.s32	%p5, %r7, 8;
	@%p5 bra 	BB63_20;

	st.global.u64 	[%rd1], %rd2;

BB63_20:
	ret;
}

	// .globl	block_reduce_add_f16_1024
.visible .entry block_reduce_add_f16_1024(
	.param .u64 block_reduce_add_f16_1024_param_0,
	.param .u64 block_reduce_add_f16_1024_param_1,
	.param .u32 block_reduce_add_f16_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_add_f16_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB64_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f29, %rs1;}

	// inline asm
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f29;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB64_3;

	ld.shared.f32 	%f12, [%r13+2048];
	add.f32 	%f29, %f29, %f12;
	st.shared.f32 	[%r13], %f29;

BB64_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB64_5;

	ld.shared.f32 	%f13, [%r13+1024];
	add.f32 	%f29, %f29, %f13;
	st.shared.f32 	[%r13], %f29;

BB64_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB64_7;

	ld.shared.f32 	%f14, [%r13+512];
	add.f32 	%f29, %f29, %f14;
	st.shared.f32 	[%r13], %f29;

BB64_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB64_9;

	ld.shared.f32 	%f15, [%r13+256];
	add.f32 	%f29, %f29, %f15;
	st.shared.f32 	[%r13], %f29;

BB64_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB64_12;

	ld.shared.f32 	%f16, [%r13+128];
	add.f32 	%f17, %f29, %f16;
	mov.b32 	 %r42, %f17;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f18, %r46;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	 %r47, %f19;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f20, %r49;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	 %r50, %f21;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f22, %r52;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	 %r53, %f23;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f24, %r55;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	 %r56, %f25;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f26, %r58;
	add.f32 	%f10, %f25, %f26;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB64_12;

	shr.u32 	%r64, %r6, 10;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f10;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB64_12:
	ret;
}

	// .globl	block_reduce_add_f16_512
.visible .entry block_reduce_add_f16_512(
	.param .u64 block_reduce_add_f16_512_param_0,
	.param .u64 block_reduce_add_f16_512_param_1,
	.param .u32 block_reduce_add_f16_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_512_param_1];
	ld.param.u32 	%r4, [block_reduce_add_f16_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB65_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f26, %rs1;}

	// inline asm
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f26;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB65_3;

	ld.shared.f32 	%f10, [%r3+1024];
	add.f32 	%f26, %f26, %f10;
	st.shared.f32 	[%r3], %f26;

BB65_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB65_5;

	ld.shared.f32 	%f11, [%r3+512];
	add.f32 	%f26, %f26, %f11;
	st.shared.f32 	[%r3], %f26;

BB65_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB65_7;

	ld.shared.f32 	%f12, [%r3+256];
	add.f32 	%f26, %f26, %f12;
	st.shared.f32 	[%r3], %f26;

BB65_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB65_10;

	ld.shared.f32 	%f13, [%r3+128];
	add.f32 	%f14, %f26, %f13;
	mov.b32 	 %r17, %f14;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f15, %r21;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	 %r22, %f16;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f17, %r24;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	 %r25, %f18;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f19, %r27;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	 %r28, %f20;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f21, %r30;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	 %r31, %f22;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f23, %r33;
	add.f32 	%f8, %f22, %f23;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB65_10;

	shr.u32 	%r40, %r1, 9;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB65_10:
	ret;
}

	// .globl	block_reduce_add_f16_256
.visible .entry block_reduce_add_f16_256(
	.param .u64 block_reduce_add_f16_256_param_0,
	.param .u64 block_reduce_add_f16_256_param_1,
	.param .u32 block_reduce_add_f16_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_256_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f16_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB66_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f22, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f22;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB66_3;

	ld.shared.f32 	%f8, [%r3+512];
	add.f32 	%f22, %f22, %f8;
	st.shared.f32 	[%r3], %f22;

BB66_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB66_5;

	ld.shared.f32 	%f9, [%r3+256];
	add.f32 	%f22, %f22, %f9;
	st.shared.f32 	[%r3], %f22;

BB66_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB66_8;

	ld.shared.f32 	%f10, [%r3+128];
	add.f32 	%f11, %f22, %f10;
	mov.b32 	 %r10, %f11;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f12, %r14;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	 %r15, %f13;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f14, %r17;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	 %r18, %f15;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f16, %r20;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	 %r21, %f17;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f18, %r23;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	 %r24, %f19;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f20, %r26;
	add.f32 	%f6, %f19, %f20;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB66_8;

	shr.u32 	%r31, %r2, 8;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB66_8:
	ret;
}

	// .globl	block_reduce_add_f16_128
.visible .entry block_reduce_add_f16_128(
	.param .u64 block_reduce_add_f16_128_param_0,
	.param .u64 block_reduce_add_f16_128_param_1,
	.param .u32 block_reduce_add_f16_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_128_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f16_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB67_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f19, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f19;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB67_3;

	ld.shared.f32 	%f6, [%r3+256];
	add.f32 	%f19, %f19, %f6;
	st.shared.f32 	[%r3], %f19;

BB67_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB67_6;

	ld.shared.f32 	%f7, [%r3+128];
	add.f32 	%f8, %f19, %f7;
	mov.b32 	 %r10, %f8;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f9, %r14;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	 %r15, %f10;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f11, %r17;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	 %r18, %f12;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f13, %r20;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	 %r21, %f14;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f15, %r23;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	 %r24, %f16;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f17, %r26;
	add.f32 	%f4, %f16, %f17;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB67_6;

	shr.u32 	%r27, %r2, 7;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB67_6:
	ret;
}

	// .globl	block_reduce_add_f16_64
.visible .entry block_reduce_add_f16_64(
	.param .u64 block_reduce_add_f16_64_param_0,
	.param .u64 block_reduce_add_f16_64_param_1,
	.param .u32 block_reduce_add_f16_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_64_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f16_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB68_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f3, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f3;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB68_4;

	ld.shared.f32 	%f4, [%r3+128];
	add.f32 	%f5, %f3, %f4;
	mov.b32 	 %r10, %f5;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f6, %r14;
	add.f32 	%f7, %f5, %f6;
	mov.b32 	 %r15, %f7;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f8, %r17;
	add.f32 	%f9, %f7, %f8;
	mov.b32 	 %r18, %f9;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f10, %r20;
	add.f32 	%f11, %f9, %f10;
	mov.b32 	 %r21, %f11;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f12, %r23;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	 %r24, %f13;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f14, %r26;
	add.f32 	%f2, %f13, %f14;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB68_4;

	shr.u32 	%r27, %r2, 6;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB68_4:
	ret;
}

	// .globl	block_reduce_add_f16_32
.visible .entry block_reduce_add_f16_32(
	.param .u64 block_reduce_add_f16_32_param_0,
	.param .u64 block_reduce_add_f16_32_param_1,
	.param .u32 block_reduce_add_f16_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_32_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f16_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB69_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	add.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB69_3;

	shr.u32 	%r24, %r2, 5;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB69_3:
	ret;
}

	// .globl	block_reduce_add_f16_16
.visible .entry block_reduce_add_f16_16(
	.param .u64 block_reduce_add_f16_16_param_0,
	.param .u64 block_reduce_add_f16_16_param_1,
	.param .u32 block_reduce_add_f16_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_16_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f16_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB70_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	add.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB70_3;

	shr.u32 	%r21, %r2, 4;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB70_3:
	ret;
}

	// .globl	block_reduce_add_f16_8
.visible .entry block_reduce_add_f16_8(
	.param .u64 block_reduce_add_f16_8_param_0,
	.param .u64 block_reduce_add_f16_8_param_1,
	.param .u32 block_reduce_add_f16_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_8_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f16_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB71_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB71_3;

	shr.u32 	%r18, %r2, 3;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB71_3:
	ret;
}

	// .globl	block_reduce_add_f16_4
.visible .entry block_reduce_add_f16_4(
	.param .u64 block_reduce_add_f16_4_param_0,
	.param .u64 block_reduce_add_f16_4_param_1,
	.param .u32 block_reduce_add_f16_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_4_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f16_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB72_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB72_3;

	shr.u32 	%r15, %r2, 2;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB72_3:
	ret;
}

	// .globl	block_reduce_add_f16_2
.visible .entry block_reduce_add_f16_2(
	.param .u64 block_reduce_add_f16_2_param_0,
	.param .u64 block_reduce_add_f16_2_param_1,
	.param .u32 block_reduce_add_f16_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f16_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f16_2_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f16_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB73_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB73_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r12, %r2, -2;
	cvt.u64.u32	%rd7, %r12;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB73_3:
	ret;
}

	// .globl	block_reduce_add_f32_1024
.visible .entry block_reduce_add_f32_1024(
	.param .u64 block_reduce_add_f32_1024_param_0,
	.param .u64 block_reduce_add_f32_1024_param_1,
	.param .u32 block_reduce_add_f32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_add_f32_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB74_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f27;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB74_3;

	ld.shared.f32 	%f11, [%r13+2048];
	add.f32 	%f27, %f27, %f11;
	st.shared.f32 	[%r13], %f27;

BB74_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB74_5;

	ld.shared.f32 	%f12, [%r13+1024];
	add.f32 	%f27, %f27, %f12;
	st.shared.f32 	[%r13], %f27;

BB74_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB74_7;

	ld.shared.f32 	%f13, [%r13+512];
	add.f32 	%f27, %f27, %f13;
	st.shared.f32 	[%r13], %f27;

BB74_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB74_9;

	ld.shared.f32 	%f14, [%r13+256];
	add.f32 	%f27, %f27, %f14;
	st.shared.f32 	[%r13], %f27;

BB74_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB74_12;

	ld.shared.f32 	%f15, [%r13+128];
	add.f32 	%f16, %f27, %f15;
	mov.b32 	 %r42, %f16;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f17, %r46;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	 %r47, %f18;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f19, %r49;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	 %r50, %f20;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f21, %r52;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	 %r53, %f22;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f23, %r55;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	 %r56, %f24;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f25, %r58;
	add.f32 	%f10, %f24, %f25;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB74_12;

	shr.u32 	%r64, %r6, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

BB74_12:
	ret;
}

	// .globl	block_reduce_add_f32_512
.visible .entry block_reduce_add_f32_512(
	.param .u64 block_reduce_add_f32_512_param_0,
	.param .u64 block_reduce_add_f32_512_param_1,
	.param .u32 block_reduce_add_f32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_512_param_1];
	ld.param.u32 	%r4, [block_reduce_add_f32_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB75_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f24, [%rd5];
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f24;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB75_3;

	ld.shared.f32 	%f9, [%r3+1024];
	add.f32 	%f24, %f24, %f9;
	st.shared.f32 	[%r3], %f24;

BB75_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB75_5;

	ld.shared.f32 	%f10, [%r3+512];
	add.f32 	%f24, %f24, %f10;
	st.shared.f32 	[%r3], %f24;

BB75_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB75_7;

	ld.shared.f32 	%f11, [%r3+256];
	add.f32 	%f24, %f24, %f11;
	st.shared.f32 	[%r3], %f24;

BB75_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB75_10;

	ld.shared.f32 	%f12, [%r3+128];
	add.f32 	%f13, %f24, %f12;
	mov.b32 	 %r17, %f13;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f14, %r21;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	 %r22, %f15;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f16, %r24;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	 %r25, %f17;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f18, %r27;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	 %r28, %f19;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f20, %r30;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	 %r31, %f21;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f22, %r33;
	add.f32 	%f8, %f21, %f22;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB75_10;

	shr.u32 	%r40, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f8;

BB75_10:
	ret;
}

	// .globl	block_reduce_add_f32_256
.visible .entry block_reduce_add_f32_256(
	.param .u64 block_reduce_add_f32_256_param_0,
	.param .u64 block_reduce_add_f32_256_param_1,
	.param .u32 block_reduce_add_f32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_256_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB76_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f20, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB76_3;

	ld.shared.f32 	%f7, [%r3+512];
	add.f32 	%f20, %f20, %f7;
	st.shared.f32 	[%r3], %f20;

BB76_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB76_5;

	ld.shared.f32 	%f8, [%r3+256];
	add.f32 	%f20, %f20, %f8;
	st.shared.f32 	[%r3], %f20;

BB76_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB76_8;

	ld.shared.f32 	%f9, [%r3+128];
	add.f32 	%f10, %f20, %f9;
	mov.b32 	 %r10, %f10;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f11, %r14;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	 %r15, %f12;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f13, %r17;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	 %r18, %f14;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f15, %r20;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	 %r21, %f16;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f17, %r23;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	 %r24, %f18;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f19, %r26;
	add.f32 	%f6, %f18, %f19;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB76_8;

	shr.u32 	%r31, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB76_8:
	ret;
}

	// .globl	block_reduce_add_f32_128
.visible .entry block_reduce_add_f32_128(
	.param .u64 block_reduce_add_f32_128_param_0,
	.param .u64 block_reduce_add_f32_128_param_1,
	.param .u32 block_reduce_add_f32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_128_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB77_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB77_3;

	ld.shared.f32 	%f5, [%r3+256];
	add.f32 	%f17, %f17, %f5;
	st.shared.f32 	[%r3], %f17;

BB77_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB77_6;

	ld.shared.f32 	%f6, [%r3+128];
	add.f32 	%f7, %f17, %f6;
	mov.b32 	 %r10, %f7;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f8, %r14;
	add.f32 	%f9, %f7, %f8;
	mov.b32 	 %r15, %f9;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f10, %r17;
	add.f32 	%f11, %f9, %f10;
	mov.b32 	 %r18, %f11;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f12, %r20;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	 %r21, %f13;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f14, %r23;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	 %r24, %f15;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f16, %r26;
	add.f32 	%f4, %f15, %f16;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB77_6;

	shr.u32 	%r27, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB77_6:
	ret;
}

	// .globl	block_reduce_add_f32_64
.visible .entry block_reduce_add_f32_64(
	.param .u64 block_reduce_add_f32_64_param_0,
	.param .u64 block_reduce_add_f32_64_param_1,
	.param .u32 block_reduce_add_f32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_64_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB78_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB78_4;

	ld.shared.f32 	%f3, [%r3+128];
	add.f32 	%f4, %f1, %f3;
	mov.b32 	 %r10, %f4;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f5, %r14;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r15, %f6;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f7, %r17;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	 %r18, %f8;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f9, %r20;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	 %r21, %f10;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f11, %r23;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	 %r24, %f12;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f13, %r26;
	add.f32 	%f2, %f12, %f13;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB78_4;

	shr.u32 	%r27, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

BB78_4:
	ret;
}

	// .globl	block_reduce_add_f32_32
.visible .entry block_reduce_add_f32_32(
	.param .u64 block_reduce_add_f32_32_param_0,
	.param .u64 block_reduce_add_f32_32_param_1,
	.param .u32 block_reduce_add_f32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_32_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f32_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB79_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	add.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB79_3;

	shr.u32 	%r24, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB79_3:
	ret;
}

	// .globl	block_reduce_add_f32_16
.visible .entry block_reduce_add_f32_16(
	.param .u64 block_reduce_add_f32_16_param_0,
	.param .u64 block_reduce_add_f32_16_param_1,
	.param .u32 block_reduce_add_f32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_16_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f32_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB80_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	add.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB80_3;

	shr.u32 	%r21, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB80_3:
	ret;
}

	// .globl	block_reduce_add_f32_8
.visible .entry block_reduce_add_f32_8(
	.param .u64 block_reduce_add_f32_8_param_0,
	.param .u64 block_reduce_add_f32_8_param_1,
	.param .u32 block_reduce_add_f32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_8_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f32_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB81_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	add.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB81_3;

	shr.u32 	%r18, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB81_3:
	ret;
}

	// .globl	block_reduce_add_f32_4
.visible .entry block_reduce_add_f32_4(
	.param .u64 block_reduce_add_f32_4_param_0,
	.param .u64 block_reduce_add_f32_4_param_1,
	.param .u32 block_reduce_add_f32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_4_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f32_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB82_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	add.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB82_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r15, %r2, -4;
	cvt.u64.u32	%rd7, %r15;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB82_3:
	ret;
}

	// .globl	block_reduce_add_f32_2
.visible .entry block_reduce_add_f32_2(
	.param .u64 block_reduce_add_f32_2_param_0,
	.param .u64 block_reduce_add_f32_2_param_1,
	.param .u32 block_reduce_add_f32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f32_2_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f32_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB83_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	add.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB83_3;

	shr.u32 	%r12, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB83_3:
	ret;
}

	// .globl	block_reduce_add_f64_1024
.visible .entry block_reduce_add_f64_1024(
	.param .u64 block_reduce_add_f64_1024_param_0,
	.param .u64 block_reduce_add_f64_1024_param_1,
	.param .u32 block_reduce_add_f64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB84_12;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd27, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd27;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB84_3;

	ld.shared.f64 	%fd11, [%r3+4096];
	add.f64 	%fd27, %fd27, %fd11;
	st.shared.f64 	[%r3], %fd27;

BB84_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB84_5;

	ld.shared.f64 	%fd12, [%r3+2048];
	add.f64 	%fd27, %fd27, %fd12;
	st.shared.f64 	[%r3], %fd27;

BB84_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB84_7;

	ld.shared.f64 	%fd13, [%r3+1024];
	add.f64 	%fd27, %fd27, %fd13;
	st.shared.f64 	[%r3], %fd27;

BB84_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB84_9;

	ld.shared.f64 	%fd14, [%r3+512];
	add.f64 	%fd27, %fd27, %fd14;
	st.shared.f64 	[%r3], %fd27;

BB84_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB84_12;

	ld.shared.f64 	%fd25, [%r3+256];
	add.f64 	%fd15, %fd27, %fd25;
	// inline asm
	mov.b64 {%r10,%r11}, %fd15;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r12,%r13};
	// inline asm
	add.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r14,%r15}, %fd17;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r16,%r17};
	// inline asm
	add.f64 	%fd19, %fd17, %fd18;
	// inline asm
	mov.b64 {%r18,%r19}, %fd19;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd20, {%r20,%r21};
	// inline asm
	add.f64 	%fd21, %fd19, %fd20;
	// inline asm
	mov.b64 {%r22,%r23}, %fd21;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd22, {%r24,%r25};
	// inline asm
	add.f64 	%fd23, %fd21, %fd22;
	// inline asm
	mov.b64 {%r26,%r27}, %fd23;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd24, {%r28,%r29};
	// inline asm
	add.f64 	%fd10, %fd23, %fd24;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB84_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd10;

BB84_12:
	ret;
}

	// .globl	block_reduce_add_f64_512
.visible .entry block_reduce_add_f64_512(
	.param .u64 block_reduce_add_f64_512_param_0,
	.param .u64 block_reduce_add_f64_512_param_1,
	.param .u32 block_reduce_add_f64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB85_10;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd24, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd24;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB85_3;

	ld.shared.f64 	%fd9, [%r3+2048];
	add.f64 	%fd24, %fd24, %fd9;
	st.shared.f64 	[%r3], %fd24;

BB85_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB85_5;

	ld.shared.f64 	%fd10, [%r3+1024];
	add.f64 	%fd24, %fd24, %fd10;
	st.shared.f64 	[%r3], %fd24;

BB85_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB85_7;

	ld.shared.f64 	%fd11, [%r3+512];
	add.f64 	%fd24, %fd24, %fd11;
	st.shared.f64 	[%r3], %fd24;

BB85_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB85_10;

	ld.shared.f64 	%fd22, [%r3+256];
	add.f64 	%fd12, %fd24, %fd22;
	// inline asm
	mov.b64 {%r10,%r11}, %fd12;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r12,%r13};
	// inline asm
	add.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r14,%r15}, %fd14;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r16,%r17};
	// inline asm
	add.f64 	%fd16, %fd14, %fd15;
	// inline asm
	mov.b64 {%r18,%r19}, %fd16;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd17, {%r20,%r21};
	// inline asm
	add.f64 	%fd18, %fd16, %fd17;
	// inline asm
	mov.b64 {%r22,%r23}, %fd18;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd19, {%r24,%r25};
	// inline asm
	add.f64 	%fd20, %fd18, %fd19;
	// inline asm
	mov.b64 {%r26,%r27}, %fd20;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd21, {%r28,%r29};
	// inline asm
	add.f64 	%fd8, %fd20, %fd21;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB85_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd8;

BB85_10:
	ret;
}

	// .globl	block_reduce_add_f64_256
.visible .entry block_reduce_add_f64_256(
	.param .u64 block_reduce_add_f64_256_param_0,
	.param .u64 block_reduce_add_f64_256_param_1,
	.param .u32 block_reduce_add_f64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB86_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd20, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB86_3;

	ld.shared.f64 	%fd7, [%r3+1024];
	add.f64 	%fd20, %fd20, %fd7;
	st.shared.f64 	[%r3], %fd20;

BB86_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB86_5;

	ld.shared.f64 	%fd8, [%r3+512];
	add.f64 	%fd20, %fd20, %fd8;
	st.shared.f64 	[%r3], %fd20;

BB86_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB86_8;

	ld.shared.f64 	%fd19, [%r3+256];
	add.f64 	%fd9, %fd20, %fd19;
	// inline asm
	mov.b64 {%r10,%r11}, %fd9;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r12,%r13};
	// inline asm
	add.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r14,%r15}, %fd11;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r16,%r17};
	// inline asm
	add.f64 	%fd13, %fd11, %fd12;
	// inline asm
	mov.b64 {%r18,%r19}, %fd13;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd14, {%r20,%r21};
	// inline asm
	add.f64 	%fd15, %fd13, %fd14;
	// inline asm
	mov.b64 {%r22,%r23}, %fd15;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r24,%r25};
	// inline asm
	add.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r26,%r27}, %fd17;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r28,%r29};
	// inline asm
	add.f64 	%fd6, %fd17, %fd18;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB86_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd6;

BB86_8:
	ret;
}

	// .globl	block_reduce_add_f64_128
.visible .entry block_reduce_add_f64_128(
	.param .u64 block_reduce_add_f64_128_param_0,
	.param .u64 block_reduce_add_f64_128_param_1,
	.param .u32 block_reduce_add_f64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<18>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB87_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB87_3;

	ld.shared.f64 	%fd5, [%r3+512];
	add.f64 	%fd17, %fd17, %fd5;
	st.shared.f64 	[%r3], %fd17;

BB87_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB87_6;

	ld.shared.f64 	%fd16, [%r3+256];
	add.f64 	%fd6, %fd17, %fd16;
	// inline asm
	mov.b64 {%r10,%r11}, %fd6;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd7, {%r12,%r13};
	// inline asm
	add.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r14,%r15}, %fd8;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd9, {%r16,%r17};
	// inline asm
	add.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r18,%r19}, %fd10;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd11, {%r20,%r21};
	// inline asm
	add.f64 	%fd12, %fd10, %fd11;
	// inline asm
	mov.b64 {%r22,%r23}, %fd12;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r24,%r25};
	// inline asm
	add.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r26,%r27}, %fd14;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r28,%r29};
	// inline asm
	add.f64 	%fd4, %fd14, %fd15;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB87_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd4;

BB87_6:
	ret;
}

	// .globl	block_reduce_add_f64_64
.visible .entry block_reduce_add_f64_64(
	.param .u64 block_reduce_add_f64_64_param_0,
	.param .u64 block_reduce_add_f64_64_param_1,
	.param .u32 block_reduce_add_f64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_add_f64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB88_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB88_4;

	ld.shared.f64 	%fd13, [%r3+256];
	add.f64 	%fd3, %fd1, %fd13;
	// inline asm
	mov.b64 {%r10,%r11}, %fd3;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd4, {%r12,%r13};
	// inline asm
	add.f64 	%fd5, %fd3, %fd4;
	// inline asm
	mov.b64 {%r14,%r15}, %fd5;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd6, {%r16,%r17};
	// inline asm
	add.f64 	%fd7, %fd5, %fd6;
	// inline asm
	mov.b64 {%r18,%r19}, %fd7;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd8, {%r20,%r21};
	// inline asm
	add.f64 	%fd9, %fd7, %fd8;
	// inline asm
	mov.b64 {%r22,%r23}, %fd9;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r24,%r25};
	// inline asm
	add.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r26,%r27}, %fd11;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r28,%r29};
	// inline asm
	add.f64 	%fd2, %fd11, %fd12;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB88_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd2;

BB88_4:
	ret;
}

	// .globl	block_reduce_add_f64_32
.visible .entry block_reduce_add_f64_32(
	.param .u64 block_reduce_add_f64_32_param_0,
	.param .u64 block_reduce_add_f64_32_param_1,
	.param .u32 block_reduce_add_f64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB89_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	add.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	add.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	add.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	add.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r22,%r23}, %fd10;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %fd11, {%r24,%r25};
	// inline asm
	add.f64 	%fd1, %fd10, %fd11;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB89_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r34, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB89_3:
	ret;
}

	// .globl	block_reduce_add_f64_16
.visible .entry block_reduce_add_f64_16(
	.param .u64 block_reduce_add_f64_16_param_0,
	.param .u64 block_reduce_add_f64_16_param_1,
	.param .u32 block_reduce_add_f64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB90_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	add.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	add.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	add.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	add.f64 	%fd1, %fd8, %fd9;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB90_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r29, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB90_3:
	ret;
}

	// .globl	block_reduce_add_f64_8
.visible .entry block_reduce_add_f64_8(
	.param .u64 block_reduce_add_f64_8_param_0,
	.param .u64 block_reduce_add_f64_8_param_1,
	.param .u32 block_reduce_add_f64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB91_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	add.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	add.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	add.f64 	%fd1, %fd6, %fd7;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB91_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd7, %r24;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB91_3:
	ret;
}

	// .globl	block_reduce_add_f64_4
.visible .entry block_reduce_add_f64_4(
	.param .u64 block_reduce_add_f64_4_param_0,
	.param .u64 block_reduce_add_f64_4_param_1,
	.param .u32 block_reduce_add_f64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB92_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	add.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	add.f64 	%fd1, %fd4, %fd5;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB92_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB92_3:
	ret;
}

	// .globl	block_reduce_add_f64_2
.visible .entry block_reduce_add_f64_2(
	.param .u64 block_reduce_add_f64_2_param_0,
	.param .u64 block_reduce_add_f64_2_param_1,
	.param .u32 block_reduce_add_f64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_f64_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_f64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_add_f64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB93_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	add.f64 	%fd1, %fd2, %fd3;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB93_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB93_3:
	ret;
}

	// .globl	block_reduce_add_u32_1024
.visible .entry block_reduce_add_u32_1024(
	.param .u64 block_reduce_add_u32_1024_param_0,
	.param .u64 block_reduce_add_u32_1024_param_1,
	.param .u32 block_reduce_add_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_add_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB94_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB94_3;

	ld.shared.u32 	%r28, [%r23+2048];
	add.s32 	%r81, %r28, %r81;
	st.shared.u32 	[%r23], %r81;

BB94_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB94_5;

	ld.shared.u32 	%r35, [%r23+1024];
	add.s32 	%r81, %r35, %r81;
	st.shared.u32 	[%r23], %r81;

BB94_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB94_7;

	ld.shared.u32 	%r42, [%r23+512];
	add.s32 	%r81, %r42, %r81;
	st.shared.u32 	[%r23], %r81;

BB94_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB94_9;

	ld.shared.u32 	%r49, [%r23+256];
	add.s32 	%r81, %r49, %r81;
	st.shared.u32 	[%r23], %r81;

BB94_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB94_12;

	ld.shared.u32 	%r56, [%r23+128];
	add.s32 	%r57, %r56, %r81;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	add.s32 	%r62, %r61, %r57;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	add.s32 	%r65, %r64, %r62;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	add.s32 	%r68, %r67, %r65;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	add.s32 	%r71, %r70, %r68;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	add.s32 	%r11, %r73, %r71;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB94_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB94_12:
	ret;
}

	// .globl	block_reduce_add_u32_512
.visible .entry block_reduce_add_u32_512(
	.param .u64 block_reduce_add_u32_512_param_0,
	.param .u64 block_reduce_add_u32_512_param_1,
	.param .u32 block_reduce_add_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_add_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB95_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB95_3;

	ld.shared.u32 	%r19, [%r4+1024];
	add.s32 	%r54, %r19, %r54;
	st.shared.u32 	[%r4], %r54;

BB95_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB95_5;

	ld.shared.u32 	%r22, [%r4+512];
	add.s32 	%r54, %r22, %r54;
	st.shared.u32 	[%r4], %r54;

BB95_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB95_7;

	ld.shared.u32 	%r25, [%r4+256];
	add.s32 	%r54, %r25, %r54;
	st.shared.u32 	[%r4], %r54;

BB95_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB95_10;

	ld.shared.u32 	%r28, [%r4+128];
	add.s32 	%r29, %r28, %r54;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	add.s32 	%r34, %r33, %r29;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	add.s32 	%r37, %r36, %r34;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	add.s32 	%r40, %r39, %r37;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	add.s32 	%r43, %r42, %r40;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	add.s32 	%r11, %r45, %r43;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB95_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB95_10:
	ret;
}

	// .globl	block_reduce_add_u32_256
.visible .entry block_reduce_add_u32_256(
	.param .u64 block_reduce_add_u32_256_param_0,
	.param .u64 block_reduce_add_u32_256_param_1,
	.param .u32 block_reduce_add_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_add_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB96_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB96_3;

	ld.shared.u32 	%r16, [%r4+512];
	add.s32 	%r41, %r16, %r41;
	st.shared.u32 	[%r4], %r41;

BB96_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB96_5;

	ld.shared.u32 	%r17, [%r4+256];
	add.s32 	%r41, %r17, %r41;
	st.shared.u32 	[%r4], %r41;

BB96_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB96_8;

	ld.shared.u32 	%r18, [%r4+128];
	add.s32 	%r19, %r18, %r41;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	add.s32 	%r24, %r23, %r19;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	add.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	add.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	add.s32 	%r33, %r32, %r30;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	add.s32 	%r10, %r35, %r33;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB96_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB96_8:
	ret;
}

	// .globl	block_reduce_add_u32_128
.visible .entry block_reduce_add_u32_128(
	.param .u64 block_reduce_add_u32_128_param_0,
	.param .u64 block_reduce_add_u32_128_param_1,
	.param .u32 block_reduce_add_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_add_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB97_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB97_3;

	ld.shared.u32 	%r14, [%r4+256];
	add.s32 	%r34, %r14, %r34;
	st.shared.u32 	[%r4], %r34;

BB97_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB97_6;

	ld.shared.u32 	%r15, [%r4+128];
	add.s32 	%r16, %r15, %r34;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	add.s32 	%r21, %r20, %r16;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	add.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	add.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	add.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	add.s32 	%r8, %r32, %r30;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB97_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB97_6:
	ret;
}

	// .globl	block_reduce_add_u32_64
.visible .entry block_reduce_add_u32_64(
	.param .u64 block_reduce_add_u32_64_param_0,
	.param .u64 block_reduce_add_u32_64_param_1,
	.param .u32 block_reduce_add_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_add_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB98_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB98_4;

	ld.shared.u32 	%r12, [%r4+128];
	add.s32 	%r13, %r12, %r3;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	add.s32 	%r18, %r17, %r13;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	add.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	add.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	add.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	add.s32 	%r6, %r29, %r27;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB98_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB98_4:
	ret;
}

	// .globl	block_reduce_add_u32_32
.visible .entry block_reduce_add_u32_32(
	.param .u64 block_reduce_add_u32_32_param_0,
	.param .u64 block_reduce_add_u32_32_param_1,
	.param .u32 block_reduce_add_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_add_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB99_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	add.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	add.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	add.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	add.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	add.s32 	%r3, %r23, %r21;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB99_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB99_3:
	ret;
}

	// .globl	block_reduce_add_u32_16
.visible .entry block_reduce_add_u32_16(
	.param .u64 block_reduce_add_u32_16_param_0,
	.param .u64 block_reduce_add_u32_16_param_1,
	.param .u32 block_reduce_add_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_add_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB100_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	add.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	add.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	add.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	add.s32 	%r3, %r20, %r18;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB100_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB100_3:
	ret;
}

	// .globl	block_reduce_add_u32_8
.visible .entry block_reduce_add_u32_8(
	.param .u64 block_reduce_add_u32_8_param_0,
	.param .u64 block_reduce_add_u32_8_param_1,
	.param .u32 block_reduce_add_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_add_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB101_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	add.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	add.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	add.s32 	%r3, %r17, %r15;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB101_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB101_3:
	ret;
}

	// .globl	block_reduce_add_u32_4
.visible .entry block_reduce_add_u32_4(
	.param .u64 block_reduce_add_u32_4_param_0,
	.param .u64 block_reduce_add_u32_4_param_1,
	.param .u32 block_reduce_add_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_add_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB102_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	add.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	add.s32 	%r3, %r14, %r12;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB102_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB102_3:
	ret;
}

	// .globl	block_reduce_add_u32_2
.visible .entry block_reduce_add_u32_2(
	.param .u64 block_reduce_add_u32_2_param_0,
	.param .u64 block_reduce_add_u32_2_param_1,
	.param .u32 block_reduce_add_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_add_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_add_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_add_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB103_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	add.s32 	%r3, %r11, %r7;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB103_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB103_3:
	ret;
}

	// .globl	block_reduce_add_u64_1024
.visible .entry block_reduce_add_u64_1024(
	.param .u64 block_reduce_add_u64_1024_param_0,
	.param .u64 block_reduce_add_u64_1024_param_1,
	.param .u32 block_reduce_add_u64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_add_u64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_add_u64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_add_u64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB104_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u32 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB104_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	add.s64 	%rd35, %rd16, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB104_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB104_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	add.s64 	%rd35, %rd17, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB104_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB104_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	add.s64 	%rd35, %rd18, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB104_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB104_9;

	ld.shared.u64 	%rd19, [%r3+512];
	add.s64 	%rd35, %rd19, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB104_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB104_12;

	ld.shared.u64 	%rd30, [%r3+256];
	add.s64 	%rd20, %rd30, %rd35;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	add.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	add.s64 	%rd24, %rd23, %rd22;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	add.s64 	%rd26, %rd25, %rd24;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	add.s64 	%rd28, %rd27, %rd26;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	add.s64 	%rd10, %rd29, %rd28;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB104_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 4;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u32 	[%rd33], %rd10;

BB104_12:
	ret;
}

	// .globl	block_reduce_add_u64_512
.visible .entry block_reduce_add_u64_512(
	.param .u64 block_reduce_add_u64_512_param_0,
	.param .u64 block_reduce_add_u64_512_param_1,
	.param .u32 block_reduce_add_u64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_add_u64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_add_u64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_add_u64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB105_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB105_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	add.s64 	%rd32, %rd14, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB105_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB105_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	add.s64 	%rd32, %rd15, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB105_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB105_7;

	ld.shared.u64 	%rd16, [%r3+512];
	add.s64 	%rd32, %rd16, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB105_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB105_10;

	ld.shared.u64 	%rd27, [%r3+256];
	add.s64 	%rd17, %rd27, %rd32;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	add.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	add.s64 	%rd21, %rd20, %rd19;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	add.s64 	%rd23, %rd22, %rd21;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	add.s64 	%rd25, %rd24, %rd23;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	add.s64 	%rd8, %rd26, %rd25;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB105_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u32 	[%rd30], %rd8;

BB105_10:
	ret;
}

	// .globl	block_reduce_add_u64_256
.visible .entry block_reduce_add_u64_256(
	.param .u64 block_reduce_add_u64_256_param_0,
	.param .u64 block_reduce_add_u64_256_param_1,
	.param .u32 block_reduce_add_u64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_add_u64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_add_u64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_add_u64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB106_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB106_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	add.s64 	%rd28, %rd12, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB106_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB106_5;

	ld.shared.u64 	%rd13, [%r3+512];
	add.s64 	%rd28, %rd13, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB106_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB106_8;

	ld.shared.u64 	%rd24, [%r3+256];
	add.s64 	%rd14, %rd24, %rd28;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	add.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	add.s64 	%rd18, %rd17, %rd16;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	add.s64 	%rd20, %rd19, %rd18;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	add.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	add.s64 	%rd6, %rd23, %rd22;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB106_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %rd6;

BB106_8:
	ret;
}

	// .globl	block_reduce_add_u64_128
.visible .entry block_reduce_add_u64_128(
	.param .u64 block_reduce_add_u64_128_param_0,
	.param .u64 block_reduce_add_u64_128_param_1,
	.param .u32 block_reduce_add_u64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_add_u64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_add_u64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_add_u64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB107_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB107_3;

	ld.shared.u64 	%rd10, [%r3+512];
	add.s64 	%rd25, %rd10, %rd25;
	st.shared.u64 	[%r3], %rd25;

BB107_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB107_6;

	ld.shared.u64 	%rd21, [%r3+256];
	add.s64 	%rd11, %rd21, %rd25;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	add.s64 	%rd13, %rd12, %rd11;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	add.s64 	%rd15, %rd14, %rd13;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	add.s64 	%rd17, %rd16, %rd15;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	add.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	add.s64 	%rd4, %rd20, %rd19;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB107_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 4;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u32 	[%rd24], %rd4;

BB107_6:
	ret;
}

	// .globl	block_reduce_add_u64_64
.visible .entry block_reduce_add_u64_64(
	.param .u64 block_reduce_add_u64_64_param_0,
	.param .u64 block_reduce_add_u64_64_param_1,
	.param .u32 block_reduce_add_u64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_add_u64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_add_u64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_add_u64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB108_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB108_4;

	ld.shared.u64 	%rd18, [%r3+256];
	add.s64 	%rd8, %rd18, %rd1;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	add.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	add.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	add.s64 	%rd14, %rd13, %rd12;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	add.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	add.s64 	%rd2, %rd17, %rd16;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB108_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 4;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %rd2;

BB108_4:
	ret;
}

	// .globl	block_reduce_add_u64_32
.visible .entry block_reduce_add_u64_32(
	.param .u64 block_reduce_add_u64_32_param_0,
	.param .u64 block_reduce_add_u64_32_param_1,
	.param .u32 block_reduce_add_u64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_add_u64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_add_u64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_add_u64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB109_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u32 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	add.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	add.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	add.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	add.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	add.s64 	%rd1, %rd13, %rd12;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB109_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %rd1;

BB109_3:
	ret;
}

	// .globl	block_reduce_add_u64_16
.visible .entry block_reduce_add_u64_16(
	.param .u64 block_reduce_add_u64_16_param_0,
	.param .u64 block_reduce_add_u64_16_param_1,
	.param .u32 block_reduce_add_u64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_add_u64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_add_u64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_add_u64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB110_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u32 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	add.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	add.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	add.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	add.s64 	%rd1, %rd11, %rd10;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB110_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %rd1;

BB110_3:
	ret;
}

	// .globl	block_reduce_add_u64_8
.visible .entry block_reduce_add_u64_8(
	.param .u64 block_reduce_add_u64_8_param_0,
	.param .u64 block_reduce_add_u64_8_param_1,
	.param .u32 block_reduce_add_u64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_add_u64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_add_u64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_add_u64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB111_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	add.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	add.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	add.s64 	%rd1, %rd9, %rd8;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB111_3;

	shr.u32 	%r24, %r2, 3;
	cvta.to.global.u64 	%rd13, %rd3;
	mul.wide.u32 	%rd14, %r24, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %rd1;

BB111_3:
	ret;
}

	// .globl	block_reduce_add_u64_4
.visible .entry block_reduce_add_u64_4(
	.param .u64 block_reduce_add_u64_4_param_0,
	.param .u64 block_reduce_add_u64_4_param_1,
	.param .u32 block_reduce_add_u64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_add_u64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_add_u64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_add_u64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB112_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	add.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	add.s64 	%rd1, %rd7, %rd6;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB112_3;

	cvta.to.global.u64 	%rd11, %rd3;
	and.b32  	%r19, %r2, -4;
	cvt.u64.u32	%rd12, %r19;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %rd1;

BB112_3:
	ret;
}

	// .globl	block_reduce_add_u64_2
.visible .entry block_reduce_add_u64_2(
	.param .u64 block_reduce_add_u64_2_param_0,
	.param .u64 block_reduce_add_u64_2_param_1,
	.param .u32 block_reduce_add_u64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_add_u64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_add_u64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_add_u64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB113_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	add.s64 	%rd1, %rd5, %rd4;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB113_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %rd1;

BB113_3:
	ret;
}

	// .globl	block_reduce_min_f16_1024
.visible .entry block_reduce_min_f16_1024(
	.param .u64 block_reduce_min_f16_1024_param_0,
	.param .u64 block_reduce_min_f16_1024_param_1,
	.param .u32 block_reduce_min_f16_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_min_f16_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB114_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f29, %rs1;}

	// inline asm
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f29;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB114_3;

	ld.shared.f32 	%f12, [%r13+2048];
	min.f32 	%f29, %f29, %f12;
	st.shared.f32 	[%r13], %f29;

BB114_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB114_5;

	ld.shared.f32 	%f13, [%r13+1024];
	min.f32 	%f29, %f29, %f13;
	st.shared.f32 	[%r13], %f29;

BB114_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB114_7;

	ld.shared.f32 	%f14, [%r13+512];
	min.f32 	%f29, %f29, %f14;
	st.shared.f32 	[%r13], %f29;

BB114_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB114_9;

	ld.shared.f32 	%f15, [%r13+256];
	min.f32 	%f29, %f29, %f15;
	st.shared.f32 	[%r13], %f29;

BB114_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB114_12;

	ld.shared.f32 	%f16, [%r13+128];
	min.f32 	%f17, %f29, %f16;
	mov.b32 	 %r42, %f17;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f18, %r46;
	min.f32 	%f19, %f17, %f18;
	mov.b32 	 %r47, %f19;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f20, %r49;
	min.f32 	%f21, %f19, %f20;
	mov.b32 	 %r50, %f21;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f22, %r52;
	min.f32 	%f23, %f21, %f22;
	mov.b32 	 %r53, %f23;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f24, %r55;
	min.f32 	%f25, %f23, %f24;
	mov.b32 	 %r56, %f25;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f26, %r58;
	min.f32 	%f10, %f25, %f26;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB114_12;

	shr.u32 	%r64, %r6, 10;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f10;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB114_12:
	ret;
}

	// .globl	block_reduce_min_f16_512
.visible .entry block_reduce_min_f16_512(
	.param .u64 block_reduce_min_f16_512_param_0,
	.param .u64 block_reduce_min_f16_512_param_1,
	.param .u32 block_reduce_min_f16_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_512_param_1];
	ld.param.u32 	%r4, [block_reduce_min_f16_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB115_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f26, %rs1;}

	// inline asm
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f26;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB115_3;

	ld.shared.f32 	%f10, [%r3+1024];
	min.f32 	%f26, %f26, %f10;
	st.shared.f32 	[%r3], %f26;

BB115_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB115_5;

	ld.shared.f32 	%f11, [%r3+512];
	min.f32 	%f26, %f26, %f11;
	st.shared.f32 	[%r3], %f26;

BB115_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB115_7;

	ld.shared.f32 	%f12, [%r3+256];
	min.f32 	%f26, %f26, %f12;
	st.shared.f32 	[%r3], %f26;

BB115_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB115_10;

	ld.shared.f32 	%f13, [%r3+128];
	min.f32 	%f14, %f26, %f13;
	mov.b32 	 %r17, %f14;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f15, %r21;
	min.f32 	%f16, %f14, %f15;
	mov.b32 	 %r22, %f16;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f17, %r24;
	min.f32 	%f18, %f16, %f17;
	mov.b32 	 %r25, %f18;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f19, %r27;
	min.f32 	%f20, %f18, %f19;
	mov.b32 	 %r28, %f20;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f21, %r30;
	min.f32 	%f22, %f20, %f21;
	mov.b32 	 %r31, %f22;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f23, %r33;
	min.f32 	%f8, %f22, %f23;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB115_10;

	shr.u32 	%r40, %r1, 9;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB115_10:
	ret;
}

	// .globl	block_reduce_min_f16_256
.visible .entry block_reduce_min_f16_256(
	.param .u64 block_reduce_min_f16_256_param_0,
	.param .u64 block_reduce_min_f16_256_param_1,
	.param .u32 block_reduce_min_f16_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_256_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f16_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB116_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f22, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f22;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB116_3;

	ld.shared.f32 	%f8, [%r3+512];
	min.f32 	%f22, %f22, %f8;
	st.shared.f32 	[%r3], %f22;

BB116_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB116_5;

	ld.shared.f32 	%f9, [%r3+256];
	min.f32 	%f22, %f22, %f9;
	st.shared.f32 	[%r3], %f22;

BB116_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB116_8;

	ld.shared.f32 	%f10, [%r3+128];
	min.f32 	%f11, %f22, %f10;
	mov.b32 	 %r10, %f11;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f12, %r14;
	min.f32 	%f13, %f11, %f12;
	mov.b32 	 %r15, %f13;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f14, %r17;
	min.f32 	%f15, %f13, %f14;
	mov.b32 	 %r18, %f15;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f16, %r20;
	min.f32 	%f17, %f15, %f16;
	mov.b32 	 %r21, %f17;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f18, %r23;
	min.f32 	%f19, %f17, %f18;
	mov.b32 	 %r24, %f19;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f20, %r26;
	min.f32 	%f6, %f19, %f20;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB116_8;

	shr.u32 	%r31, %r2, 8;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB116_8:
	ret;
}

	// .globl	block_reduce_min_f16_128
.visible .entry block_reduce_min_f16_128(
	.param .u64 block_reduce_min_f16_128_param_0,
	.param .u64 block_reduce_min_f16_128_param_1,
	.param .u32 block_reduce_min_f16_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_128_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f16_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB117_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f19, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f19;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB117_3;

	ld.shared.f32 	%f6, [%r3+256];
	min.f32 	%f19, %f19, %f6;
	st.shared.f32 	[%r3], %f19;

BB117_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB117_6;

	ld.shared.f32 	%f7, [%r3+128];
	min.f32 	%f8, %f19, %f7;
	mov.b32 	 %r10, %f8;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f9, %r14;
	min.f32 	%f10, %f8, %f9;
	mov.b32 	 %r15, %f10;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f11, %r17;
	min.f32 	%f12, %f10, %f11;
	mov.b32 	 %r18, %f12;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f13, %r20;
	min.f32 	%f14, %f12, %f13;
	mov.b32 	 %r21, %f14;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f15, %r23;
	min.f32 	%f16, %f14, %f15;
	mov.b32 	 %r24, %f16;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f17, %r26;
	min.f32 	%f4, %f16, %f17;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB117_6;

	shr.u32 	%r27, %r2, 7;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB117_6:
	ret;
}

	// .globl	block_reduce_min_f16_64
.visible .entry block_reduce_min_f16_64(
	.param .u64 block_reduce_min_f16_64_param_0,
	.param .u64 block_reduce_min_f16_64_param_1,
	.param .u32 block_reduce_min_f16_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_64_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f16_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB118_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f3, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f3;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB118_4;

	ld.shared.f32 	%f4, [%r3+128];
	min.f32 	%f5, %f3, %f4;
	mov.b32 	 %r10, %f5;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f6, %r14;
	min.f32 	%f7, %f5, %f6;
	mov.b32 	 %r15, %f7;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f8, %r17;
	min.f32 	%f9, %f7, %f8;
	mov.b32 	 %r18, %f9;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f10, %r20;
	min.f32 	%f11, %f9, %f10;
	mov.b32 	 %r21, %f11;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f12, %r23;
	min.f32 	%f13, %f11, %f12;
	mov.b32 	 %r24, %f13;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f14, %r26;
	min.f32 	%f2, %f13, %f14;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB118_4;

	shr.u32 	%r27, %r2, 6;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB118_4:
	ret;
}

	// .globl	block_reduce_min_f16_32
.visible .entry block_reduce_min_f16_32(
	.param .u64 block_reduce_min_f16_32_param_0,
	.param .u64 block_reduce_min_f16_32_param_1,
	.param .u32 block_reduce_min_f16_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_32_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f16_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB119_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	min.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	min.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB119_3;

	shr.u32 	%r24, %r2, 5;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB119_3:
	ret;
}

	// .globl	block_reduce_min_f16_16
.visible .entry block_reduce_min_f16_16(
	.param .u64 block_reduce_min_f16_16_param_0,
	.param .u64 block_reduce_min_f16_16_param_1,
	.param .u32 block_reduce_min_f16_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_16_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f16_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB120_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	min.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB120_3;

	shr.u32 	%r21, %r2, 4;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB120_3:
	ret;
}

	// .globl	block_reduce_min_f16_8
.visible .entry block_reduce_min_f16_8(
	.param .u64 block_reduce_min_f16_8_param_0,
	.param .u64 block_reduce_min_f16_8_param_1,
	.param .u32 block_reduce_min_f16_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_8_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f16_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB121_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB121_3;

	shr.u32 	%r18, %r2, 3;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB121_3:
	ret;
}

	// .globl	block_reduce_min_f16_4
.visible .entry block_reduce_min_f16_4(
	.param .u64 block_reduce_min_f16_4_param_0,
	.param .u64 block_reduce_min_f16_4_param_1,
	.param .u32 block_reduce_min_f16_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_4_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f16_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB122_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB122_3;

	shr.u32 	%r15, %r2, 2;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB122_3:
	ret;
}

	// .globl	block_reduce_min_f16_2
.visible .entry block_reduce_min_f16_2(
	.param .u64 block_reduce_min_f16_2_param_0,
	.param .u64 block_reduce_min_f16_2_param_1,
	.param .u32 block_reduce_min_f16_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f16_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f16_2_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f16_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB123_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB123_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r12, %r2, -2;
	cvt.u64.u32	%rd7, %r12;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB123_3:
	ret;
}

	// .globl	block_reduce_min_f32_1024
.visible .entry block_reduce_min_f32_1024(
	.param .u64 block_reduce_min_f32_1024_param_0,
	.param .u64 block_reduce_min_f32_1024_param_1,
	.param .u32 block_reduce_min_f32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_min_f32_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB124_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f27;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB124_3;

	ld.shared.f32 	%f11, [%r13+2048];
	min.f32 	%f27, %f27, %f11;
	st.shared.f32 	[%r13], %f27;

BB124_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB124_5;

	ld.shared.f32 	%f12, [%r13+1024];
	min.f32 	%f27, %f27, %f12;
	st.shared.f32 	[%r13], %f27;

BB124_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB124_7;

	ld.shared.f32 	%f13, [%r13+512];
	min.f32 	%f27, %f27, %f13;
	st.shared.f32 	[%r13], %f27;

BB124_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB124_9;

	ld.shared.f32 	%f14, [%r13+256];
	min.f32 	%f27, %f27, %f14;
	st.shared.f32 	[%r13], %f27;

BB124_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB124_12;

	ld.shared.f32 	%f15, [%r13+128];
	min.f32 	%f16, %f27, %f15;
	mov.b32 	 %r42, %f16;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f17, %r46;
	min.f32 	%f18, %f16, %f17;
	mov.b32 	 %r47, %f18;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f19, %r49;
	min.f32 	%f20, %f18, %f19;
	mov.b32 	 %r50, %f20;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f21, %r52;
	min.f32 	%f22, %f20, %f21;
	mov.b32 	 %r53, %f22;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f23, %r55;
	min.f32 	%f24, %f22, %f23;
	mov.b32 	 %r56, %f24;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f25, %r58;
	min.f32 	%f10, %f24, %f25;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB124_12;

	shr.u32 	%r64, %r6, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

BB124_12:
	ret;
}

	// .globl	block_reduce_min_f32_512
.visible .entry block_reduce_min_f32_512(
	.param .u64 block_reduce_min_f32_512_param_0,
	.param .u64 block_reduce_min_f32_512_param_1,
	.param .u32 block_reduce_min_f32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_512_param_1];
	ld.param.u32 	%r4, [block_reduce_min_f32_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB125_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f24, [%rd5];
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f24;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB125_3;

	ld.shared.f32 	%f9, [%r3+1024];
	min.f32 	%f24, %f24, %f9;
	st.shared.f32 	[%r3], %f24;

BB125_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB125_5;

	ld.shared.f32 	%f10, [%r3+512];
	min.f32 	%f24, %f24, %f10;
	st.shared.f32 	[%r3], %f24;

BB125_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB125_7;

	ld.shared.f32 	%f11, [%r3+256];
	min.f32 	%f24, %f24, %f11;
	st.shared.f32 	[%r3], %f24;

BB125_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB125_10;

	ld.shared.f32 	%f12, [%r3+128];
	min.f32 	%f13, %f24, %f12;
	mov.b32 	 %r17, %f13;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f14, %r21;
	min.f32 	%f15, %f13, %f14;
	mov.b32 	 %r22, %f15;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f16, %r24;
	min.f32 	%f17, %f15, %f16;
	mov.b32 	 %r25, %f17;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f18, %r27;
	min.f32 	%f19, %f17, %f18;
	mov.b32 	 %r28, %f19;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f20, %r30;
	min.f32 	%f21, %f19, %f20;
	mov.b32 	 %r31, %f21;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f22, %r33;
	min.f32 	%f8, %f21, %f22;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB125_10;

	shr.u32 	%r40, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f8;

BB125_10:
	ret;
}

	// .globl	block_reduce_min_f32_256
.visible .entry block_reduce_min_f32_256(
	.param .u64 block_reduce_min_f32_256_param_0,
	.param .u64 block_reduce_min_f32_256_param_1,
	.param .u32 block_reduce_min_f32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_256_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB126_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f20, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB126_3;

	ld.shared.f32 	%f7, [%r3+512];
	min.f32 	%f20, %f20, %f7;
	st.shared.f32 	[%r3], %f20;

BB126_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB126_5;

	ld.shared.f32 	%f8, [%r3+256];
	min.f32 	%f20, %f20, %f8;
	st.shared.f32 	[%r3], %f20;

BB126_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB126_8;

	ld.shared.f32 	%f9, [%r3+128];
	min.f32 	%f10, %f20, %f9;
	mov.b32 	 %r10, %f10;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f11, %r14;
	min.f32 	%f12, %f10, %f11;
	mov.b32 	 %r15, %f12;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f13, %r17;
	min.f32 	%f14, %f12, %f13;
	mov.b32 	 %r18, %f14;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f15, %r20;
	min.f32 	%f16, %f14, %f15;
	mov.b32 	 %r21, %f16;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f17, %r23;
	min.f32 	%f18, %f16, %f17;
	mov.b32 	 %r24, %f18;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f19, %r26;
	min.f32 	%f6, %f18, %f19;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB126_8;

	shr.u32 	%r31, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB126_8:
	ret;
}

	// .globl	block_reduce_min_f32_128
.visible .entry block_reduce_min_f32_128(
	.param .u64 block_reduce_min_f32_128_param_0,
	.param .u64 block_reduce_min_f32_128_param_1,
	.param .u32 block_reduce_min_f32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_128_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB127_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB127_3;

	ld.shared.f32 	%f5, [%r3+256];
	min.f32 	%f17, %f17, %f5;
	st.shared.f32 	[%r3], %f17;

BB127_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB127_6;

	ld.shared.f32 	%f6, [%r3+128];
	min.f32 	%f7, %f17, %f6;
	mov.b32 	 %r10, %f7;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f8, %r14;
	min.f32 	%f9, %f7, %f8;
	mov.b32 	 %r15, %f9;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f10, %r17;
	min.f32 	%f11, %f9, %f10;
	mov.b32 	 %r18, %f11;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f12, %r20;
	min.f32 	%f13, %f11, %f12;
	mov.b32 	 %r21, %f13;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f14, %r23;
	min.f32 	%f15, %f13, %f14;
	mov.b32 	 %r24, %f15;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f16, %r26;
	min.f32 	%f4, %f15, %f16;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB127_6;

	shr.u32 	%r27, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB127_6:
	ret;
}

	// .globl	block_reduce_min_f32_64
.visible .entry block_reduce_min_f32_64(
	.param .u64 block_reduce_min_f32_64_param_0,
	.param .u64 block_reduce_min_f32_64_param_1,
	.param .u32 block_reduce_min_f32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_64_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB128_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB128_4;

	ld.shared.f32 	%f3, [%r3+128];
	min.f32 	%f4, %f1, %f3;
	mov.b32 	 %r10, %f4;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f5, %r14;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r15, %f6;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f7, %r17;
	min.f32 	%f8, %f6, %f7;
	mov.b32 	 %r18, %f8;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f9, %r20;
	min.f32 	%f10, %f8, %f9;
	mov.b32 	 %r21, %f10;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f11, %r23;
	min.f32 	%f12, %f10, %f11;
	mov.b32 	 %r24, %f12;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f13, %r26;
	min.f32 	%f2, %f12, %f13;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB128_4;

	shr.u32 	%r27, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

BB128_4:
	ret;
}

	// .globl	block_reduce_min_f32_32
.visible .entry block_reduce_min_f32_32(
	.param .u64 block_reduce_min_f32_32_param_0,
	.param .u64 block_reduce_min_f32_32_param_1,
	.param .u32 block_reduce_min_f32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_32_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f32_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB129_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	min.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	min.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB129_3;

	shr.u32 	%r24, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB129_3:
	ret;
}

	// .globl	block_reduce_min_f32_16
.visible .entry block_reduce_min_f32_16(
	.param .u64 block_reduce_min_f32_16_param_0,
	.param .u64 block_reduce_min_f32_16_param_1,
	.param .u32 block_reduce_min_f32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_16_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f32_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB130_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	min.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB130_3;

	shr.u32 	%r21, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB130_3:
	ret;
}

	// .globl	block_reduce_min_f32_8
.visible .entry block_reduce_min_f32_8(
	.param .u64 block_reduce_min_f32_8_param_0,
	.param .u64 block_reduce_min_f32_8_param_1,
	.param .u32 block_reduce_min_f32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_8_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f32_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB131_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	min.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB131_3;

	shr.u32 	%r18, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB131_3:
	ret;
}

	// .globl	block_reduce_min_f32_4
.visible .entry block_reduce_min_f32_4(
	.param .u64 block_reduce_min_f32_4_param_0,
	.param .u64 block_reduce_min_f32_4_param_1,
	.param .u32 block_reduce_min_f32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_4_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f32_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB132_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	min.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB132_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r15, %r2, -4;
	cvt.u64.u32	%rd7, %r15;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB132_3:
	ret;
}

	// .globl	block_reduce_min_f32_2
.visible .entry block_reduce_min_f32_2(
	.param .u64 block_reduce_min_f32_2_param_0,
	.param .u64 block_reduce_min_f32_2_param_1,
	.param .u32 block_reduce_min_f32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f32_2_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f32_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB133_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	min.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB133_3;

	shr.u32 	%r12, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB133_3:
	ret;
}

	// .globl	block_reduce_min_f64_1024
.visible .entry block_reduce_min_f64_1024(
	.param .u64 block_reduce_min_f64_1024_param_0,
	.param .u64 block_reduce_min_f64_1024_param_1,
	.param .u32 block_reduce_min_f64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB134_12;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd27, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd27;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB134_3;

	ld.shared.f64 	%fd11, [%r3+4096];
	min.f64 	%fd27, %fd27, %fd11;
	st.shared.f64 	[%r3], %fd27;

BB134_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB134_5;

	ld.shared.f64 	%fd12, [%r3+2048];
	min.f64 	%fd27, %fd27, %fd12;
	st.shared.f64 	[%r3], %fd27;

BB134_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB134_7;

	ld.shared.f64 	%fd13, [%r3+1024];
	min.f64 	%fd27, %fd27, %fd13;
	st.shared.f64 	[%r3], %fd27;

BB134_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB134_9;

	ld.shared.f64 	%fd14, [%r3+512];
	min.f64 	%fd27, %fd27, %fd14;
	st.shared.f64 	[%r3], %fd27;

BB134_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB134_12;

	ld.shared.f64 	%fd25, [%r3+256];
	min.f64 	%fd15, %fd27, %fd25;
	// inline asm
	mov.b64 {%r10,%r11}, %fd15;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r12,%r13};
	// inline asm
	min.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r14,%r15}, %fd17;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r16,%r17};
	// inline asm
	min.f64 	%fd19, %fd17, %fd18;
	// inline asm
	mov.b64 {%r18,%r19}, %fd19;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd20, {%r20,%r21};
	// inline asm
	min.f64 	%fd21, %fd19, %fd20;
	// inline asm
	mov.b64 {%r22,%r23}, %fd21;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd22, {%r24,%r25};
	// inline asm
	min.f64 	%fd23, %fd21, %fd22;
	// inline asm
	mov.b64 {%r26,%r27}, %fd23;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd24, {%r28,%r29};
	// inline asm
	min.f64 	%fd10, %fd23, %fd24;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB134_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd10;

BB134_12:
	ret;
}

	// .globl	block_reduce_min_f64_512
.visible .entry block_reduce_min_f64_512(
	.param .u64 block_reduce_min_f64_512_param_0,
	.param .u64 block_reduce_min_f64_512_param_1,
	.param .u32 block_reduce_min_f64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB135_10;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd24, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd24;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB135_3;

	ld.shared.f64 	%fd9, [%r3+2048];
	min.f64 	%fd24, %fd24, %fd9;
	st.shared.f64 	[%r3], %fd24;

BB135_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB135_5;

	ld.shared.f64 	%fd10, [%r3+1024];
	min.f64 	%fd24, %fd24, %fd10;
	st.shared.f64 	[%r3], %fd24;

BB135_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB135_7;

	ld.shared.f64 	%fd11, [%r3+512];
	min.f64 	%fd24, %fd24, %fd11;
	st.shared.f64 	[%r3], %fd24;

BB135_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB135_10;

	ld.shared.f64 	%fd22, [%r3+256];
	min.f64 	%fd12, %fd24, %fd22;
	// inline asm
	mov.b64 {%r10,%r11}, %fd12;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r12,%r13};
	// inline asm
	min.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r14,%r15}, %fd14;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r16,%r17};
	// inline asm
	min.f64 	%fd16, %fd14, %fd15;
	// inline asm
	mov.b64 {%r18,%r19}, %fd16;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd17, {%r20,%r21};
	// inline asm
	min.f64 	%fd18, %fd16, %fd17;
	// inline asm
	mov.b64 {%r22,%r23}, %fd18;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd19, {%r24,%r25};
	// inline asm
	min.f64 	%fd20, %fd18, %fd19;
	// inline asm
	mov.b64 {%r26,%r27}, %fd20;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd21, {%r28,%r29};
	// inline asm
	min.f64 	%fd8, %fd20, %fd21;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB135_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd8;

BB135_10:
	ret;
}

	// .globl	block_reduce_min_f64_256
.visible .entry block_reduce_min_f64_256(
	.param .u64 block_reduce_min_f64_256_param_0,
	.param .u64 block_reduce_min_f64_256_param_1,
	.param .u32 block_reduce_min_f64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB136_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd20, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB136_3;

	ld.shared.f64 	%fd7, [%r3+1024];
	min.f64 	%fd20, %fd20, %fd7;
	st.shared.f64 	[%r3], %fd20;

BB136_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB136_5;

	ld.shared.f64 	%fd8, [%r3+512];
	min.f64 	%fd20, %fd20, %fd8;
	st.shared.f64 	[%r3], %fd20;

BB136_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB136_8;

	ld.shared.f64 	%fd19, [%r3+256];
	min.f64 	%fd9, %fd20, %fd19;
	// inline asm
	mov.b64 {%r10,%r11}, %fd9;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r12,%r13};
	// inline asm
	min.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r14,%r15}, %fd11;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r16,%r17};
	// inline asm
	min.f64 	%fd13, %fd11, %fd12;
	// inline asm
	mov.b64 {%r18,%r19}, %fd13;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd14, {%r20,%r21};
	// inline asm
	min.f64 	%fd15, %fd13, %fd14;
	// inline asm
	mov.b64 {%r22,%r23}, %fd15;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r24,%r25};
	// inline asm
	min.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r26,%r27}, %fd17;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r28,%r29};
	// inline asm
	min.f64 	%fd6, %fd17, %fd18;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB136_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd6;

BB136_8:
	ret;
}

	// .globl	block_reduce_min_f64_128
.visible .entry block_reduce_min_f64_128(
	.param .u64 block_reduce_min_f64_128_param_0,
	.param .u64 block_reduce_min_f64_128_param_1,
	.param .u32 block_reduce_min_f64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<18>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB137_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB137_3;

	ld.shared.f64 	%fd5, [%r3+512];
	min.f64 	%fd17, %fd17, %fd5;
	st.shared.f64 	[%r3], %fd17;

BB137_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB137_6;

	ld.shared.f64 	%fd16, [%r3+256];
	min.f64 	%fd6, %fd17, %fd16;
	// inline asm
	mov.b64 {%r10,%r11}, %fd6;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd7, {%r12,%r13};
	// inline asm
	min.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r14,%r15}, %fd8;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd9, {%r16,%r17};
	// inline asm
	min.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r18,%r19}, %fd10;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd11, {%r20,%r21};
	// inline asm
	min.f64 	%fd12, %fd10, %fd11;
	// inline asm
	mov.b64 {%r22,%r23}, %fd12;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r24,%r25};
	// inline asm
	min.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r26,%r27}, %fd14;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r28,%r29};
	// inline asm
	min.f64 	%fd4, %fd14, %fd15;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB137_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd4;

BB137_6:
	ret;
}

	// .globl	block_reduce_min_f64_64
.visible .entry block_reduce_min_f64_64(
	.param .u64 block_reduce_min_f64_64_param_0,
	.param .u64 block_reduce_min_f64_64_param_1,
	.param .u32 block_reduce_min_f64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_min_f64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB138_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB138_4;

	ld.shared.f64 	%fd13, [%r3+256];
	min.f64 	%fd3, %fd1, %fd13;
	// inline asm
	mov.b64 {%r10,%r11}, %fd3;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd4, {%r12,%r13};
	// inline asm
	min.f64 	%fd5, %fd3, %fd4;
	// inline asm
	mov.b64 {%r14,%r15}, %fd5;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd6, {%r16,%r17};
	// inline asm
	min.f64 	%fd7, %fd5, %fd6;
	// inline asm
	mov.b64 {%r18,%r19}, %fd7;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd8, {%r20,%r21};
	// inline asm
	min.f64 	%fd9, %fd7, %fd8;
	// inline asm
	mov.b64 {%r22,%r23}, %fd9;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r24,%r25};
	// inline asm
	min.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r26,%r27}, %fd11;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r28,%r29};
	// inline asm
	min.f64 	%fd2, %fd11, %fd12;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB138_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd2;

BB138_4:
	ret;
}

	// .globl	block_reduce_min_f64_32
.visible .entry block_reduce_min_f64_32(
	.param .u64 block_reduce_min_f64_32_param_0,
	.param .u64 block_reduce_min_f64_32_param_1,
	.param .u32 block_reduce_min_f64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB139_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	min.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	min.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	min.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	min.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r22,%r23}, %fd10;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %fd11, {%r24,%r25};
	// inline asm
	min.f64 	%fd1, %fd10, %fd11;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB139_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r34, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB139_3:
	ret;
}

	// .globl	block_reduce_min_f64_16
.visible .entry block_reduce_min_f64_16(
	.param .u64 block_reduce_min_f64_16_param_0,
	.param .u64 block_reduce_min_f64_16_param_1,
	.param .u32 block_reduce_min_f64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB140_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	min.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	min.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	min.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	min.f64 	%fd1, %fd8, %fd9;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB140_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r29, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB140_3:
	ret;
}

	// .globl	block_reduce_min_f64_8
.visible .entry block_reduce_min_f64_8(
	.param .u64 block_reduce_min_f64_8_param_0,
	.param .u64 block_reduce_min_f64_8_param_1,
	.param .u32 block_reduce_min_f64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB141_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	min.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	min.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	min.f64 	%fd1, %fd6, %fd7;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB141_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd7, %r24;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB141_3:
	ret;
}

	// .globl	block_reduce_min_f64_4
.visible .entry block_reduce_min_f64_4(
	.param .u64 block_reduce_min_f64_4_param_0,
	.param .u64 block_reduce_min_f64_4_param_1,
	.param .u32 block_reduce_min_f64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB142_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	min.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	min.f64 	%fd1, %fd4, %fd5;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB142_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB142_3:
	ret;
}

	// .globl	block_reduce_min_f64_2
.visible .entry block_reduce_min_f64_2(
	.param .u64 block_reduce_min_f64_2_param_0,
	.param .u64 block_reduce_min_f64_2_param_1,
	.param .u32 block_reduce_min_f64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_f64_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_f64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_min_f64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB143_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	min.f64 	%fd1, %fd2, %fd3;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB143_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB143_3:
	ret;
}

	// .globl	block_reduce_min_u32_1024
.visible .entry block_reduce_min_u32_1024(
	.param .u64 block_reduce_min_u32_1024_param_0,
	.param .u64 block_reduce_min_u32_1024_param_1,
	.param .u32 block_reduce_min_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_min_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB144_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB144_3;

	ld.shared.u32 	%r28, [%r23+2048];
	min.u32 	%r81, %r81, %r28;
	st.shared.u32 	[%r23], %r81;

BB144_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB144_5;

	ld.shared.u32 	%r35, [%r23+1024];
	min.u32 	%r81, %r81, %r35;
	st.shared.u32 	[%r23], %r81;

BB144_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB144_7;

	ld.shared.u32 	%r42, [%r23+512];
	min.u32 	%r81, %r81, %r42;
	st.shared.u32 	[%r23], %r81;

BB144_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB144_9;

	ld.shared.u32 	%r49, [%r23+256];
	min.u32 	%r81, %r81, %r49;
	st.shared.u32 	[%r23], %r81;

BB144_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB144_12;

	ld.shared.u32 	%r56, [%r23+128];
	min.u32 	%r57, %r81, %r56;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	min.u32 	%r62, %r57, %r61;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	min.u32 	%r65, %r62, %r64;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	min.u32 	%r68, %r65, %r67;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	min.u32 	%r71, %r68, %r70;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	min.u32 	%r11, %r71, %r73;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB144_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB144_12:
	ret;
}

	// .globl	block_reduce_min_u32_512
.visible .entry block_reduce_min_u32_512(
	.param .u64 block_reduce_min_u32_512_param_0,
	.param .u64 block_reduce_min_u32_512_param_1,
	.param .u32 block_reduce_min_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_min_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB145_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB145_3;

	ld.shared.u32 	%r19, [%r4+1024];
	min.u32 	%r54, %r54, %r19;
	st.shared.u32 	[%r4], %r54;

BB145_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB145_5;

	ld.shared.u32 	%r22, [%r4+512];
	min.u32 	%r54, %r54, %r22;
	st.shared.u32 	[%r4], %r54;

BB145_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB145_7;

	ld.shared.u32 	%r25, [%r4+256];
	min.u32 	%r54, %r54, %r25;
	st.shared.u32 	[%r4], %r54;

BB145_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB145_10;

	ld.shared.u32 	%r28, [%r4+128];
	min.u32 	%r29, %r54, %r28;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	min.u32 	%r34, %r29, %r33;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	min.u32 	%r37, %r34, %r36;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	min.u32 	%r40, %r37, %r39;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	min.u32 	%r43, %r40, %r42;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	min.u32 	%r11, %r43, %r45;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB145_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB145_10:
	ret;
}

	// .globl	block_reduce_min_u32_256
.visible .entry block_reduce_min_u32_256(
	.param .u64 block_reduce_min_u32_256_param_0,
	.param .u64 block_reduce_min_u32_256_param_1,
	.param .u32 block_reduce_min_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_min_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB146_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB146_3;

	ld.shared.u32 	%r16, [%r4+512];
	min.u32 	%r41, %r41, %r16;
	st.shared.u32 	[%r4], %r41;

BB146_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB146_5;

	ld.shared.u32 	%r17, [%r4+256];
	min.u32 	%r41, %r41, %r17;
	st.shared.u32 	[%r4], %r41;

BB146_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB146_8;

	ld.shared.u32 	%r18, [%r4+128];
	min.u32 	%r19, %r41, %r18;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	min.u32 	%r24, %r19, %r23;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	min.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	min.u32 	%r30, %r27, %r29;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	min.u32 	%r33, %r30, %r32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	min.u32 	%r10, %r33, %r35;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB146_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB146_8:
	ret;
}

	// .globl	block_reduce_min_u32_128
.visible .entry block_reduce_min_u32_128(
	.param .u64 block_reduce_min_u32_128_param_0,
	.param .u64 block_reduce_min_u32_128_param_1,
	.param .u32 block_reduce_min_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_min_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB147_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB147_3;

	ld.shared.u32 	%r14, [%r4+256];
	min.u32 	%r34, %r34, %r14;
	st.shared.u32 	[%r4], %r34;

BB147_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB147_6;

	ld.shared.u32 	%r15, [%r4+128];
	min.u32 	%r16, %r34, %r15;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	min.u32 	%r21, %r16, %r20;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	min.u32 	%r24, %r21, %r23;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	min.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	min.u32 	%r30, %r27, %r29;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	min.u32 	%r8, %r30, %r32;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB147_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB147_6:
	ret;
}

	// .globl	block_reduce_min_u32_64
.visible .entry block_reduce_min_u32_64(
	.param .u64 block_reduce_min_u32_64_param_0,
	.param .u64 block_reduce_min_u32_64_param_1,
	.param .u32 block_reduce_min_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_min_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB148_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB148_4;

	ld.shared.u32 	%r12, [%r4+128];
	min.u32 	%r13, %r3, %r12;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	min.u32 	%r18, %r13, %r17;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	min.u32 	%r21, %r18, %r20;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	min.u32 	%r24, %r21, %r23;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	min.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	min.u32 	%r6, %r27, %r29;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB148_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB148_4:
	ret;
}

	// .globl	block_reduce_min_u32_32
.visible .entry block_reduce_min_u32_32(
	.param .u64 block_reduce_min_u32_32_param_0,
	.param .u64 block_reduce_min_u32_32_param_1,
	.param .u32 block_reduce_min_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_min_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB149_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.u32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	min.u32 	%r21, %r18, %r20;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	min.u32 	%r3, %r21, %r23;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB149_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB149_3:
	ret;
}

	// .globl	block_reduce_min_u32_16
.visible .entry block_reduce_min_u32_16(
	.param .u64 block_reduce_min_u32_16_param_0,
	.param .u64 block_reduce_min_u32_16_param_1,
	.param .u32 block_reduce_min_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_min_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB150_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.u32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	min.u32 	%r3, %r18, %r20;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB150_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB150_3:
	ret;
}

	// .globl	block_reduce_min_u32_8
.visible .entry block_reduce_min_u32_8(
	.param .u64 block_reduce_min_u32_8_param_0,
	.param .u64 block_reduce_min_u32_8_param_1,
	.param .u32 block_reduce_min_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_min_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB151_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.u32 	%r3, %r15, %r17;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB151_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB151_3:
	ret;
}

	// .globl	block_reduce_min_u32_4
.visible .entry block_reduce_min_u32_4(
	.param .u64 block_reduce_min_u32_4_param_0,
	.param .u64 block_reduce_min_u32_4_param_1,
	.param .u32 block_reduce_min_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_min_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB152_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.u32 	%r3, %r12, %r14;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB152_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB152_3:
	ret;
}

	// .globl	block_reduce_min_u32_2
.visible .entry block_reduce_min_u32_2(
	.param .u64 block_reduce_min_u32_2_param_0,
	.param .u64 block_reduce_min_u32_2_param_1,
	.param .u32 block_reduce_min_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_min_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB153_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.u32 	%r3, %r7, %r11;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB153_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB153_3:
	ret;
}

	// .globl	block_reduce_min_u64_1024
.visible .entry block_reduce_min_u64_1024(
	.param .u64 block_reduce_min_u64_1024_param_0,
	.param .u64 block_reduce_min_u64_1024_param_1,
	.param .u32 block_reduce_min_u64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_min_u64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_min_u64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_min_u64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB154_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u32 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB154_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	min.u64 	%rd35, %rd35, %rd16;
	st.shared.u64 	[%r3], %rd35;

BB154_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB154_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	min.u64 	%rd35, %rd35, %rd17;
	st.shared.u64 	[%r3], %rd35;

BB154_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB154_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	min.u64 	%rd35, %rd35, %rd18;
	st.shared.u64 	[%r3], %rd35;

BB154_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB154_9;

	ld.shared.u64 	%rd19, [%r3+512];
	min.u64 	%rd35, %rd35, %rd19;
	st.shared.u64 	[%r3], %rd35;

BB154_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB154_12;

	ld.shared.u64 	%rd30, [%r3+256];
	min.u64 	%rd20, %rd35, %rd30;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	min.u64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	min.u64 	%rd24, %rd22, %rd23;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	min.u64 	%rd26, %rd24, %rd25;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	min.u64 	%rd28, %rd26, %rd27;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	min.u64 	%rd10, %rd28, %rd29;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB154_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 4;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u32 	[%rd33], %rd10;

BB154_12:
	ret;
}

	// .globl	block_reduce_min_u64_512
.visible .entry block_reduce_min_u64_512(
	.param .u64 block_reduce_min_u64_512_param_0,
	.param .u64 block_reduce_min_u64_512_param_1,
	.param .u32 block_reduce_min_u64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_min_u64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_min_u64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_min_u64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB155_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB155_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	min.u64 	%rd32, %rd32, %rd14;
	st.shared.u64 	[%r3], %rd32;

BB155_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB155_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	min.u64 	%rd32, %rd32, %rd15;
	st.shared.u64 	[%r3], %rd32;

BB155_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB155_7;

	ld.shared.u64 	%rd16, [%r3+512];
	min.u64 	%rd32, %rd32, %rd16;
	st.shared.u64 	[%r3], %rd32;

BB155_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB155_10;

	ld.shared.u64 	%rd27, [%r3+256];
	min.u64 	%rd17, %rd32, %rd27;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	min.u64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	min.u64 	%rd21, %rd19, %rd20;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	min.u64 	%rd23, %rd21, %rd22;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	min.u64 	%rd25, %rd23, %rd24;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	min.u64 	%rd8, %rd25, %rd26;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB155_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u32 	[%rd30], %rd8;

BB155_10:
	ret;
}

	// .globl	block_reduce_min_u64_256
.visible .entry block_reduce_min_u64_256(
	.param .u64 block_reduce_min_u64_256_param_0,
	.param .u64 block_reduce_min_u64_256_param_1,
	.param .u32 block_reduce_min_u64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_min_u64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_min_u64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_min_u64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB156_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB156_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	min.u64 	%rd28, %rd28, %rd12;
	st.shared.u64 	[%r3], %rd28;

BB156_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB156_5;

	ld.shared.u64 	%rd13, [%r3+512];
	min.u64 	%rd28, %rd28, %rd13;
	st.shared.u64 	[%r3], %rd28;

BB156_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB156_8;

	ld.shared.u64 	%rd24, [%r3+256];
	min.u64 	%rd14, %rd28, %rd24;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	min.u64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	min.u64 	%rd18, %rd16, %rd17;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	min.u64 	%rd20, %rd18, %rd19;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	min.u64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	min.u64 	%rd6, %rd22, %rd23;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB156_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %rd6;

BB156_8:
	ret;
}

	// .globl	block_reduce_min_u64_128
.visible .entry block_reduce_min_u64_128(
	.param .u64 block_reduce_min_u64_128_param_0,
	.param .u64 block_reduce_min_u64_128_param_1,
	.param .u32 block_reduce_min_u64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_min_u64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_min_u64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_min_u64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB157_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB157_3;

	ld.shared.u64 	%rd10, [%r3+512];
	min.u64 	%rd25, %rd25, %rd10;
	st.shared.u64 	[%r3], %rd25;

BB157_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB157_6;

	ld.shared.u64 	%rd21, [%r3+256];
	min.u64 	%rd11, %rd25, %rd21;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	min.u64 	%rd13, %rd11, %rd12;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	min.u64 	%rd15, %rd13, %rd14;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	min.u64 	%rd17, %rd15, %rd16;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	min.u64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	min.u64 	%rd4, %rd19, %rd20;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB157_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 4;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u32 	[%rd24], %rd4;

BB157_6:
	ret;
}

	// .globl	block_reduce_min_u64_64
.visible .entry block_reduce_min_u64_64(
	.param .u64 block_reduce_min_u64_64_param_0,
	.param .u64 block_reduce_min_u64_64_param_1,
	.param .u32 block_reduce_min_u64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_min_u64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_min_u64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_min_u64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB158_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB158_4;

	ld.shared.u64 	%rd18, [%r3+256];
	min.u64 	%rd8, %rd1, %rd18;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	min.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	min.u64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	min.u64 	%rd14, %rd12, %rd13;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	min.u64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	min.u64 	%rd2, %rd16, %rd17;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB158_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 4;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %rd2;

BB158_4:
	ret;
}

	// .globl	block_reduce_min_u64_32
.visible .entry block_reduce_min_u64_32(
	.param .u64 block_reduce_min_u64_32_param_0,
	.param .u64 block_reduce_min_u64_32_param_1,
	.param .u32 block_reduce_min_u64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_min_u64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_u64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_min_u64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB159_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u32 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	min.u64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	min.u64 	%rd1, %rd12, %rd13;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB159_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %rd1;

BB159_3:
	ret;
}

	// .globl	block_reduce_min_u64_16
.visible .entry block_reduce_min_u64_16(
	.param .u64 block_reduce_min_u64_16_param_0,
	.param .u64 block_reduce_min_u64_16_param_1,
	.param .u32 block_reduce_min_u64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_min_u64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_u64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_min_u64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB160_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u32 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	min.u64 	%rd1, %rd10, %rd11;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB160_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %rd1;

BB160_3:
	ret;
}

	// .globl	block_reduce_min_u64_8
.visible .entry block_reduce_min_u64_8(
	.param .u64 block_reduce_min_u64_8_param_0,
	.param .u64 block_reduce_min_u64_8_param_1,
	.param .u32 block_reduce_min_u64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_min_u64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_u64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_min_u64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB161_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.u64 	%rd1, %rd8, %rd9;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB161_3;

	shr.u32 	%r24, %r2, 3;
	cvta.to.global.u64 	%rd13, %rd3;
	mul.wide.u32 	%rd14, %r24, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %rd1;

BB161_3:
	ret;
}

	// .globl	block_reduce_min_u64_4
.visible .entry block_reduce_min_u64_4(
	.param .u64 block_reduce_min_u64_4_param_0,
	.param .u64 block_reduce_min_u64_4_param_1,
	.param .u32 block_reduce_min_u64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_min_u64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_u64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_min_u64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB162_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.u64 	%rd1, %rd6, %rd7;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB162_3;

	cvta.to.global.u64 	%rd11, %rd3;
	and.b32  	%r19, %r2, -4;
	cvt.u64.u32	%rd12, %r19;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %rd1;

BB162_3:
	ret;
}

	// .globl	block_reduce_min_u64_2
.visible .entry block_reduce_min_u64_2(
	.param .u64 block_reduce_min_u64_2_param_0,
	.param .u64 block_reduce_min_u64_2_param_1,
	.param .u32 block_reduce_min_u64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_min_u64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_u64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_min_u64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB163_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.u64 	%rd1, %rd4, %rd5;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB163_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %rd1;

BB163_3:
	ret;
}

	// .globl	block_reduce_min_i32_1024
.visible .entry block_reduce_min_i32_1024(
	.param .u64 block_reduce_min_i32_1024_param_0,
	.param .u64 block_reduce_min_i32_1024_param_1,
	.param .u32 block_reduce_min_i32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_min_i32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB164_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB164_3;

	ld.shared.u32 	%r28, [%r23+2048];
	min.s32 	%r81, %r81, %r28;
	st.shared.u32 	[%r23], %r81;

BB164_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB164_5;

	ld.shared.u32 	%r35, [%r23+1024];
	min.s32 	%r81, %r81, %r35;
	st.shared.u32 	[%r23], %r81;

BB164_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB164_7;

	ld.shared.u32 	%r42, [%r23+512];
	min.s32 	%r81, %r81, %r42;
	st.shared.u32 	[%r23], %r81;

BB164_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB164_9;

	ld.shared.u32 	%r49, [%r23+256];
	min.s32 	%r81, %r81, %r49;
	st.shared.u32 	[%r23], %r81;

BB164_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB164_12;

	ld.shared.u32 	%r56, [%r23+128];
	min.s32 	%r57, %r81, %r56;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	min.s32 	%r62, %r57, %r61;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	min.s32 	%r65, %r62, %r64;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	min.s32 	%r68, %r65, %r67;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	min.s32 	%r71, %r68, %r70;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	min.s32 	%r11, %r71, %r73;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB164_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB164_12:
	ret;
}

	// .globl	block_reduce_min_i32_512
.visible .entry block_reduce_min_i32_512(
	.param .u64 block_reduce_min_i32_512_param_0,
	.param .u64 block_reduce_min_i32_512_param_1,
	.param .u32 block_reduce_min_i32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_min_i32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB165_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB165_3;

	ld.shared.u32 	%r19, [%r4+1024];
	min.s32 	%r54, %r54, %r19;
	st.shared.u32 	[%r4], %r54;

BB165_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB165_5;

	ld.shared.u32 	%r22, [%r4+512];
	min.s32 	%r54, %r54, %r22;
	st.shared.u32 	[%r4], %r54;

BB165_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB165_7;

	ld.shared.u32 	%r25, [%r4+256];
	min.s32 	%r54, %r54, %r25;
	st.shared.u32 	[%r4], %r54;

BB165_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB165_10;

	ld.shared.u32 	%r28, [%r4+128];
	min.s32 	%r29, %r54, %r28;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	min.s32 	%r34, %r29, %r33;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	min.s32 	%r37, %r34, %r36;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	min.s32 	%r40, %r37, %r39;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	min.s32 	%r43, %r40, %r42;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	min.s32 	%r11, %r43, %r45;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB165_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB165_10:
	ret;
}

	// .globl	block_reduce_min_i32_256
.visible .entry block_reduce_min_i32_256(
	.param .u64 block_reduce_min_i32_256_param_0,
	.param .u64 block_reduce_min_i32_256_param_1,
	.param .u32 block_reduce_min_i32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_min_i32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB166_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB166_3;

	ld.shared.u32 	%r16, [%r4+512];
	min.s32 	%r41, %r41, %r16;
	st.shared.u32 	[%r4], %r41;

BB166_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB166_5;

	ld.shared.u32 	%r17, [%r4+256];
	min.s32 	%r41, %r41, %r17;
	st.shared.u32 	[%r4], %r41;

BB166_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB166_8;

	ld.shared.u32 	%r18, [%r4+128];
	min.s32 	%r19, %r41, %r18;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	min.s32 	%r24, %r19, %r23;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	min.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	min.s32 	%r30, %r27, %r29;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	min.s32 	%r33, %r30, %r32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	min.s32 	%r10, %r33, %r35;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB166_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB166_8:
	ret;
}

	// .globl	block_reduce_min_i32_128
.visible .entry block_reduce_min_i32_128(
	.param .u64 block_reduce_min_i32_128_param_0,
	.param .u64 block_reduce_min_i32_128_param_1,
	.param .u32 block_reduce_min_i32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_min_i32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB167_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB167_3;

	ld.shared.u32 	%r14, [%r4+256];
	min.s32 	%r34, %r34, %r14;
	st.shared.u32 	[%r4], %r34;

BB167_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB167_6;

	ld.shared.u32 	%r15, [%r4+128];
	min.s32 	%r16, %r34, %r15;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	min.s32 	%r21, %r16, %r20;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	min.s32 	%r24, %r21, %r23;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	min.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	min.s32 	%r30, %r27, %r29;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	min.s32 	%r8, %r30, %r32;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB167_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB167_6:
	ret;
}

	// .globl	block_reduce_min_i32_64
.visible .entry block_reduce_min_i32_64(
	.param .u64 block_reduce_min_i32_64_param_0,
	.param .u64 block_reduce_min_i32_64_param_1,
	.param .u32 block_reduce_min_i32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_min_i32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB168_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB168_4;

	ld.shared.u32 	%r12, [%r4+128];
	min.s32 	%r13, %r3, %r12;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	min.s32 	%r18, %r13, %r17;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	min.s32 	%r21, %r18, %r20;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	min.s32 	%r24, %r21, %r23;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	min.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	min.s32 	%r6, %r27, %r29;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB168_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB168_4:
	ret;
}

	// .globl	block_reduce_min_i32_32
.visible .entry block_reduce_min_i32_32(
	.param .u64 block_reduce_min_i32_32_param_0,
	.param .u64 block_reduce_min_i32_32_param_1,
	.param .u32 block_reduce_min_i32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_min_i32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB169_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.s32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	min.s32 	%r21, %r18, %r20;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	min.s32 	%r3, %r21, %r23;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB169_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB169_3:
	ret;
}

	// .globl	block_reduce_min_i32_16
.visible .entry block_reduce_min_i32_16(
	.param .u64 block_reduce_min_i32_16_param_0,
	.param .u64 block_reduce_min_i32_16_param_1,
	.param .u32 block_reduce_min_i32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_min_i32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB170_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.s32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	min.s32 	%r3, %r18, %r20;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB170_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB170_3:
	ret;
}

	// .globl	block_reduce_min_i32_8
.visible .entry block_reduce_min_i32_8(
	.param .u64 block_reduce_min_i32_8_param_0,
	.param .u64 block_reduce_min_i32_8_param_1,
	.param .u32 block_reduce_min_i32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_min_i32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB171_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	min.s32 	%r3, %r15, %r17;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB171_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB171_3:
	ret;
}

	// .globl	block_reduce_min_i32_4
.visible .entry block_reduce_min_i32_4(
	.param .u64 block_reduce_min_i32_4_param_0,
	.param .u64 block_reduce_min_i32_4_param_1,
	.param .u32 block_reduce_min_i32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_min_i32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB172_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	min.s32 	%r3, %r12, %r14;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB172_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB172_3:
	ret;
}

	// .globl	block_reduce_min_i32_2
.visible .entry block_reduce_min_i32_2(
	.param .u64 block_reduce_min_i32_2_param_0,
	.param .u64 block_reduce_min_i32_2_param_1,
	.param .u32 block_reduce_min_i32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_min_i32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_min_i32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_min_i32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB173_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	min.s32 	%r3, %r7, %r11;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB173_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB173_3:
	ret;
}

	// .globl	block_reduce_min_i64_1024
.visible .entry block_reduce_min_i64_1024(
	.param .u64 block_reduce_min_i64_1024_param_0,
	.param .u64 block_reduce_min_i64_1024_param_1,
	.param .u32 block_reduce_min_i64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_min_i64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_min_i64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_min_i64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB174_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 8;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u64 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB174_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	min.s64 	%rd35, %rd35, %rd16;
	st.shared.u64 	[%r3], %rd35;

BB174_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB174_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	min.s64 	%rd35, %rd35, %rd17;
	st.shared.u64 	[%r3], %rd35;

BB174_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB174_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	min.s64 	%rd35, %rd35, %rd18;
	st.shared.u64 	[%r3], %rd35;

BB174_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB174_9;

	ld.shared.u64 	%rd19, [%r3+512];
	min.s64 	%rd35, %rd35, %rd19;
	st.shared.u64 	[%r3], %rd35;

BB174_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB174_12;

	ld.shared.u64 	%rd30, [%r3+256];
	min.s64 	%rd20, %rd35, %rd30;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	min.s64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	min.s64 	%rd24, %rd22, %rd23;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	min.s64 	%rd26, %rd24, %rd25;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	min.s64 	%rd28, %rd26, %rd27;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	min.s64 	%rd10, %rd28, %rd29;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB174_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 8;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u64 	[%rd33], %rd10;

BB174_12:
	ret;
}

	// .globl	block_reduce_min_i64_512
.visible .entry block_reduce_min_i64_512(
	.param .u64 block_reduce_min_i64_512_param_0,
	.param .u64 block_reduce_min_i64_512_param_1,
	.param .u32 block_reduce_min_i64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_min_i64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_min_i64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_min_i64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB175_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u64 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB175_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	min.s64 	%rd32, %rd32, %rd14;
	st.shared.u64 	[%r3], %rd32;

BB175_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB175_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	min.s64 	%rd32, %rd32, %rd15;
	st.shared.u64 	[%r3], %rd32;

BB175_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB175_7;

	ld.shared.u64 	%rd16, [%r3+512];
	min.s64 	%rd32, %rd32, %rd16;
	st.shared.u64 	[%r3], %rd32;

BB175_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB175_10;

	ld.shared.u64 	%rd27, [%r3+256];
	min.s64 	%rd17, %rd32, %rd27;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	min.s64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	min.s64 	%rd21, %rd19, %rd20;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	min.s64 	%rd23, %rd21, %rd22;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	min.s64 	%rd25, %rd23, %rd24;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	min.s64 	%rd8, %rd25, %rd26;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB175_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 8;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u64 	[%rd30], %rd8;

BB175_10:
	ret;
}

	// .globl	block_reduce_min_i64_256
.visible .entry block_reduce_min_i64_256(
	.param .u64 block_reduce_min_i64_256_param_0,
	.param .u64 block_reduce_min_i64_256_param_1,
	.param .u32 block_reduce_min_i64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_min_i64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_min_i64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_min_i64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB176_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u64 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB176_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	min.s64 	%rd28, %rd28, %rd12;
	st.shared.u64 	[%r3], %rd28;

BB176_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB176_5;

	ld.shared.u64 	%rd13, [%r3+512];
	min.s64 	%rd28, %rd28, %rd13;
	st.shared.u64 	[%r3], %rd28;

BB176_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB176_8;

	ld.shared.u64 	%rd24, [%r3+256];
	min.s64 	%rd14, %rd28, %rd24;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	min.s64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	min.s64 	%rd18, %rd16, %rd17;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	min.s64 	%rd20, %rd18, %rd19;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	min.s64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	min.s64 	%rd6, %rd22, %rd23;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB176_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 8;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u64 	[%rd27], %rd6;

BB176_8:
	ret;
}

	// .globl	block_reduce_min_i64_128
.visible .entry block_reduce_min_i64_128(
	.param .u64 block_reduce_min_i64_128_param_0,
	.param .u64 block_reduce_min_i64_128_param_1,
	.param .u32 block_reduce_min_i64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_min_i64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_min_i64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_min_i64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB177_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u64 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB177_3;

	ld.shared.u64 	%rd10, [%r3+512];
	min.s64 	%rd25, %rd25, %rd10;
	st.shared.u64 	[%r3], %rd25;

BB177_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB177_6;

	ld.shared.u64 	%rd21, [%r3+256];
	min.s64 	%rd11, %rd25, %rd21;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	min.s64 	%rd13, %rd11, %rd12;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	min.s64 	%rd15, %rd13, %rd14;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	min.s64 	%rd17, %rd15, %rd16;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	min.s64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	min.s64 	%rd4, %rd19, %rd20;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB177_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 8;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u64 	[%rd24], %rd4;

BB177_6:
	ret;
}

	// .globl	block_reduce_min_i64_64
.visible .entry block_reduce_min_i64_64(
	.param .u64 block_reduce_min_i64_64_param_0,
	.param .u64 block_reduce_min_i64_64_param_1,
	.param .u32 block_reduce_min_i64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_min_i64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_min_i64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_min_i64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB178_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u64 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB178_4;

	ld.shared.u64 	%rd18, [%r3+256];
	min.s64 	%rd8, %rd1, %rd18;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	min.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	min.s64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	min.s64 	%rd14, %rd12, %rd13;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	min.s64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	min.s64 	%rd2, %rd16, %rd17;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB178_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 8;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u64 	[%rd21], %rd2;

BB178_4:
	ret;
}

	// .globl	block_reduce_min_i64_32
.visible .entry block_reduce_min_i64_32(
	.param .u64 block_reduce_min_i64_32_param_0,
	.param .u64 block_reduce_min_i64_32_param_1,
	.param .u32 block_reduce_min_i64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_min_i64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_i64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_min_i64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB179_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 8;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u64 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	min.s64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	min.s64 	%rd1, %rd12, %rd13;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB179_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 8;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u64 	[%rd19], %rd1;

BB179_3:
	ret;
}

	// .globl	block_reduce_min_i64_16
.visible .entry block_reduce_min_i64_16(
	.param .u64 block_reduce_min_i64_16_param_0,
	.param .u64 block_reduce_min_i64_16_param_1,
	.param .u32 block_reduce_min_i64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_min_i64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_i64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_min_i64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB180_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u64 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	min.s64 	%rd1, %rd10, %rd11;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB180_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u64 	[%rd17], %rd1;

BB180_3:
	ret;
}

	// .globl	block_reduce_min_i64_8
.visible .entry block_reduce_min_i64_8(
	.param .u64 block_reduce_min_i64_8_param_0,
	.param .u64 block_reduce_min_i64_8_param_1,
	.param .u32 block_reduce_min_i64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_min_i64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_i64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_min_i64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB181_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u64 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	min.s64 	%rd1, %rd8, %rd9;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB181_3;

	cvta.to.global.u64 	%rd13, %rd3;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd14, %r24;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u64 	[%rd15], %rd1;

BB181_3:
	ret;
}

	// .globl	block_reduce_min_i64_4
.visible .entry block_reduce_min_i64_4(
	.param .u64 block_reduce_min_i64_4_param_0,
	.param .u64 block_reduce_min_i64_4_param_1,
	.param .u32 block_reduce_min_i64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_min_i64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_i64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_min_i64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB182_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	min.s64 	%rd1, %rd6, %rd7;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB182_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd11, %rd3;
	mul.wide.u32 	%rd12, %r19, 8;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u64 	[%rd13], %rd1;

BB182_3:
	ret;
}

	// .globl	block_reduce_min_i64_2
.visible .entry block_reduce_min_i64_2(
	.param .u64 block_reduce_min_i64_2_param_0,
	.param .u64 block_reduce_min_i64_2_param_1,
	.param .u32 block_reduce_min_i64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_min_i64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_min_i64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_min_i64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB183_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	min.s64 	%rd1, %rd4, %rd5;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB183_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 8;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u64 	[%rd11], %rd1;

BB183_3:
	ret;
}

	// .globl	block_reduce_max_f16_1024
.visible .entry block_reduce_max_f16_1024(
	.param .u64 block_reduce_max_f16_1024_param_0,
	.param .u64 block_reduce_max_f16_1024_param_1,
	.param .u32 block_reduce_max_f16_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_max_f16_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB184_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f29, %rs1;}

	// inline asm
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f29;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB184_3;

	ld.shared.f32 	%f12, [%r13+2048];
	max.f32 	%f29, %f29, %f12;
	st.shared.f32 	[%r13], %f29;

BB184_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB184_5;

	ld.shared.f32 	%f13, [%r13+1024];
	max.f32 	%f29, %f29, %f13;
	st.shared.f32 	[%r13], %f29;

BB184_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB184_7;

	ld.shared.f32 	%f14, [%r13+512];
	max.f32 	%f29, %f29, %f14;
	st.shared.f32 	[%r13], %f29;

BB184_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB184_9;

	ld.shared.f32 	%f15, [%r13+256];
	max.f32 	%f29, %f29, %f15;
	st.shared.f32 	[%r13], %f29;

BB184_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB184_12;

	ld.shared.f32 	%f16, [%r13+128];
	max.f32 	%f17, %f29, %f16;
	mov.b32 	 %r42, %f17;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f18, %r46;
	max.f32 	%f19, %f17, %f18;
	mov.b32 	 %r47, %f19;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f20, %r49;
	max.f32 	%f21, %f19, %f20;
	mov.b32 	 %r50, %f21;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f22, %r52;
	max.f32 	%f23, %f21, %f22;
	mov.b32 	 %r53, %f23;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f24, %r55;
	max.f32 	%f25, %f23, %f24;
	mov.b32 	 %r56, %f25;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f26, %r58;
	max.f32 	%f10, %f25, %f26;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB184_12;

	shr.u32 	%r64, %r6, 10;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f10;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB184_12:
	ret;
}

	// .globl	block_reduce_max_f16_512
.visible .entry block_reduce_max_f16_512(
	.param .u64 block_reduce_max_f16_512_param_0,
	.param .u64 block_reduce_max_f16_512_param_1,
	.param .u32 block_reduce_max_f16_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_512_param_1];
	ld.param.u32 	%r4, [block_reduce_max_f16_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB185_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f26, %rs1;}

	// inline asm
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f26;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB185_3;

	ld.shared.f32 	%f10, [%r3+1024];
	max.f32 	%f26, %f26, %f10;
	st.shared.f32 	[%r3], %f26;

BB185_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB185_5;

	ld.shared.f32 	%f11, [%r3+512];
	max.f32 	%f26, %f26, %f11;
	st.shared.f32 	[%r3], %f26;

BB185_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB185_7;

	ld.shared.f32 	%f12, [%r3+256];
	max.f32 	%f26, %f26, %f12;
	st.shared.f32 	[%r3], %f26;

BB185_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB185_10;

	ld.shared.f32 	%f13, [%r3+128];
	max.f32 	%f14, %f26, %f13;
	mov.b32 	 %r17, %f14;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f15, %r21;
	max.f32 	%f16, %f14, %f15;
	mov.b32 	 %r22, %f16;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f17, %r24;
	max.f32 	%f18, %f16, %f17;
	mov.b32 	 %r25, %f18;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f19, %r27;
	max.f32 	%f20, %f18, %f19;
	mov.b32 	 %r28, %f20;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f21, %r30;
	max.f32 	%f22, %f20, %f21;
	mov.b32 	 %r31, %f22;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f23, %r33;
	max.f32 	%f8, %f22, %f23;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB185_10;

	shr.u32 	%r40, %r1, 9;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB185_10:
	ret;
}

	// .globl	block_reduce_max_f16_256
.visible .entry block_reduce_max_f16_256(
	.param .u64 block_reduce_max_f16_256_param_0,
	.param .u64 block_reduce_max_f16_256_param_1,
	.param .u32 block_reduce_max_f16_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_256_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f16_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB186_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f22, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f22;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB186_3;

	ld.shared.f32 	%f8, [%r3+512];
	max.f32 	%f22, %f22, %f8;
	st.shared.f32 	[%r3], %f22;

BB186_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB186_5;

	ld.shared.f32 	%f9, [%r3+256];
	max.f32 	%f22, %f22, %f9;
	st.shared.f32 	[%r3], %f22;

BB186_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB186_8;

	ld.shared.f32 	%f10, [%r3+128];
	max.f32 	%f11, %f22, %f10;
	mov.b32 	 %r10, %f11;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f12, %r14;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	 %r15, %f13;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f14, %r17;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	 %r18, %f15;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f16, %r20;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	 %r21, %f17;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f18, %r23;
	max.f32 	%f19, %f17, %f18;
	mov.b32 	 %r24, %f19;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f20, %r26;
	max.f32 	%f6, %f19, %f20;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB186_8;

	shr.u32 	%r31, %r2, 8;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB186_8:
	ret;
}

	// .globl	block_reduce_max_f16_128
.visible .entry block_reduce_max_f16_128(
	.param .u64 block_reduce_max_f16_128_param_0,
	.param .u64 block_reduce_max_f16_128_param_1,
	.param .u32 block_reduce_max_f16_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_128_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f16_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB187_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f19, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f19;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB187_3;

	ld.shared.f32 	%f6, [%r3+256];
	max.f32 	%f19, %f19, %f6;
	st.shared.f32 	[%r3], %f19;

BB187_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB187_6;

	ld.shared.f32 	%f7, [%r3+128];
	max.f32 	%f8, %f19, %f7;
	mov.b32 	 %r10, %f8;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f9, %r14;
	max.f32 	%f10, %f8, %f9;
	mov.b32 	 %r15, %f10;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f11, %r17;
	max.f32 	%f12, %f10, %f11;
	mov.b32 	 %r18, %f12;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f13, %r20;
	max.f32 	%f14, %f12, %f13;
	mov.b32 	 %r21, %f14;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f15, %r23;
	max.f32 	%f16, %f14, %f15;
	mov.b32 	 %r24, %f16;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f17, %r26;
	max.f32 	%f4, %f16, %f17;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB187_6;

	shr.u32 	%r27, %r2, 7;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB187_6:
	ret;
}

	// .globl	block_reduce_max_f16_64
.visible .entry block_reduce_max_f16_64(
	.param .u64 block_reduce_max_f16_64_param_0,
	.param .u64 block_reduce_max_f16_64_param_1,
	.param .u32 block_reduce_max_f16_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_64_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f16_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB188_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f3, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f3;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB188_4;

	ld.shared.f32 	%f4, [%r3+128];
	max.f32 	%f5, %f3, %f4;
	mov.b32 	 %r10, %f5;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f6, %r14;
	max.f32 	%f7, %f5, %f6;
	mov.b32 	 %r15, %f7;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f8, %r17;
	max.f32 	%f9, %f7, %f8;
	mov.b32 	 %r18, %f9;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f10, %r20;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	 %r21, %f11;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f12, %r23;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	 %r24, %f13;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f14, %r26;
	max.f32 	%f2, %f13, %f14;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB188_4;

	shr.u32 	%r27, %r2, 6;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB188_4:
	ret;
}

	// .globl	block_reduce_max_f16_32
.visible .entry block_reduce_max_f16_32(
	.param .u64 block_reduce_max_f16_32_param_0,
	.param .u64 block_reduce_max_f16_32_param_1,
	.param .u32 block_reduce_max_f16_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_32_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f16_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB189_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	max.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	max.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB189_3;

	shr.u32 	%r24, %r2, 5;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB189_3:
	ret;
}

	// .globl	block_reduce_max_f16_16
.visible .entry block_reduce_max_f16_16(
	.param .u64 block_reduce_max_f16_16_param_0,
	.param .u64 block_reduce_max_f16_16_param_1,
	.param .u32 block_reduce_max_f16_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_16_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f16_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB190_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	max.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB190_3;

	shr.u32 	%r21, %r2, 4;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB190_3:
	ret;
}

	// .globl	block_reduce_max_f16_8
.visible .entry block_reduce_max_f16_8(
	.param .u64 block_reduce_max_f16_8_param_0,
	.param .u64 block_reduce_max_f16_8_param_1,
	.param .u32 block_reduce_max_f16_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_8_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f16_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB191_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB191_3;

	shr.u32 	%r18, %r2, 3;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB191_3:
	ret;
}

	// .globl	block_reduce_max_f16_4
.visible .entry block_reduce_max_f16_4(
	.param .u64 block_reduce_max_f16_4_param_0,
	.param .u64 block_reduce_max_f16_4_param_1,
	.param .u32 block_reduce_max_f16_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_4_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f16_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB192_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB192_3;

	shr.u32 	%r15, %r2, 2;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB192_3:
	ret;
}

	// .globl	block_reduce_max_f16_2
.visible .entry block_reduce_max_f16_2(
	.param .u64 block_reduce_max_f16_2_param_0,
	.param .u64 block_reduce_max_f16_2_param_1,
	.param .u32 block_reduce_max_f16_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f16_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f16_2_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f16_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB193_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB193_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r12, %r2, -2;
	cvt.u64.u32	%rd7, %r12;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB193_3:
	ret;
}

	// .globl	block_reduce_max_f32_1024
.visible .entry block_reduce_max_f32_1024(
	.param .u64 block_reduce_max_f32_1024_param_0,
	.param .u64 block_reduce_max_f32_1024_param_1,
	.param .u32 block_reduce_max_f32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_max_f32_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB194_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f27;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB194_3;

	ld.shared.f32 	%f11, [%r13+2048];
	max.f32 	%f27, %f27, %f11;
	st.shared.f32 	[%r13], %f27;

BB194_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB194_5;

	ld.shared.f32 	%f12, [%r13+1024];
	max.f32 	%f27, %f27, %f12;
	st.shared.f32 	[%r13], %f27;

BB194_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB194_7;

	ld.shared.f32 	%f13, [%r13+512];
	max.f32 	%f27, %f27, %f13;
	st.shared.f32 	[%r13], %f27;

BB194_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB194_9;

	ld.shared.f32 	%f14, [%r13+256];
	max.f32 	%f27, %f27, %f14;
	st.shared.f32 	[%r13], %f27;

BB194_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB194_12;

	ld.shared.f32 	%f15, [%r13+128];
	max.f32 	%f16, %f27, %f15;
	mov.b32 	 %r42, %f16;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f17, %r46;
	max.f32 	%f18, %f16, %f17;
	mov.b32 	 %r47, %f18;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f19, %r49;
	max.f32 	%f20, %f18, %f19;
	mov.b32 	 %r50, %f20;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f21, %r52;
	max.f32 	%f22, %f20, %f21;
	mov.b32 	 %r53, %f22;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f23, %r55;
	max.f32 	%f24, %f22, %f23;
	mov.b32 	 %r56, %f24;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f25, %r58;
	max.f32 	%f10, %f24, %f25;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB194_12;

	shr.u32 	%r64, %r6, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

BB194_12:
	ret;
}

	// .globl	block_reduce_max_f32_512
.visible .entry block_reduce_max_f32_512(
	.param .u64 block_reduce_max_f32_512_param_0,
	.param .u64 block_reduce_max_f32_512_param_1,
	.param .u32 block_reduce_max_f32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_512_param_1];
	ld.param.u32 	%r4, [block_reduce_max_f32_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB195_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f24, [%rd5];
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f24;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB195_3;

	ld.shared.f32 	%f9, [%r3+1024];
	max.f32 	%f24, %f24, %f9;
	st.shared.f32 	[%r3], %f24;

BB195_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB195_5;

	ld.shared.f32 	%f10, [%r3+512];
	max.f32 	%f24, %f24, %f10;
	st.shared.f32 	[%r3], %f24;

BB195_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB195_7;

	ld.shared.f32 	%f11, [%r3+256];
	max.f32 	%f24, %f24, %f11;
	st.shared.f32 	[%r3], %f24;

BB195_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB195_10;

	ld.shared.f32 	%f12, [%r3+128];
	max.f32 	%f13, %f24, %f12;
	mov.b32 	 %r17, %f13;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f14, %r21;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	 %r22, %f15;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f16, %r24;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	 %r25, %f17;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f18, %r27;
	max.f32 	%f19, %f17, %f18;
	mov.b32 	 %r28, %f19;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f20, %r30;
	max.f32 	%f21, %f19, %f20;
	mov.b32 	 %r31, %f21;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f22, %r33;
	max.f32 	%f8, %f21, %f22;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB195_10;

	shr.u32 	%r40, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f8;

BB195_10:
	ret;
}

	// .globl	block_reduce_max_f32_256
.visible .entry block_reduce_max_f32_256(
	.param .u64 block_reduce_max_f32_256_param_0,
	.param .u64 block_reduce_max_f32_256_param_1,
	.param .u32 block_reduce_max_f32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_256_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB196_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f20, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB196_3;

	ld.shared.f32 	%f7, [%r3+512];
	max.f32 	%f20, %f20, %f7;
	st.shared.f32 	[%r3], %f20;

BB196_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB196_5;

	ld.shared.f32 	%f8, [%r3+256];
	max.f32 	%f20, %f20, %f8;
	st.shared.f32 	[%r3], %f20;

BB196_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB196_8;

	ld.shared.f32 	%f9, [%r3+128];
	max.f32 	%f10, %f20, %f9;
	mov.b32 	 %r10, %f10;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f11, %r14;
	max.f32 	%f12, %f10, %f11;
	mov.b32 	 %r15, %f12;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f13, %r17;
	max.f32 	%f14, %f12, %f13;
	mov.b32 	 %r18, %f14;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f15, %r20;
	max.f32 	%f16, %f14, %f15;
	mov.b32 	 %r21, %f16;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f17, %r23;
	max.f32 	%f18, %f16, %f17;
	mov.b32 	 %r24, %f18;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f19, %r26;
	max.f32 	%f6, %f18, %f19;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB196_8;

	shr.u32 	%r31, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB196_8:
	ret;
}

	// .globl	block_reduce_max_f32_128
.visible .entry block_reduce_max_f32_128(
	.param .u64 block_reduce_max_f32_128_param_0,
	.param .u64 block_reduce_max_f32_128_param_1,
	.param .u32 block_reduce_max_f32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_128_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB197_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB197_3;

	ld.shared.f32 	%f5, [%r3+256];
	max.f32 	%f17, %f17, %f5;
	st.shared.f32 	[%r3], %f17;

BB197_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB197_6;

	ld.shared.f32 	%f6, [%r3+128];
	max.f32 	%f7, %f17, %f6;
	mov.b32 	 %r10, %f7;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f8, %r14;
	max.f32 	%f9, %f7, %f8;
	mov.b32 	 %r15, %f9;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f10, %r17;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	 %r18, %f11;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f12, %r20;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	 %r21, %f13;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f14, %r23;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	 %r24, %f15;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f16, %r26;
	max.f32 	%f4, %f15, %f16;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB197_6;

	shr.u32 	%r27, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB197_6:
	ret;
}

	// .globl	block_reduce_max_f32_64
.visible .entry block_reduce_max_f32_64(
	.param .u64 block_reduce_max_f32_64_param_0,
	.param .u64 block_reduce_max_f32_64_param_1,
	.param .u32 block_reduce_max_f32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_64_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB198_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB198_4;

	ld.shared.f32 	%f3, [%r3+128];
	max.f32 	%f4, %f1, %f3;
	mov.b32 	 %r10, %f4;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f5, %r14;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r15, %f6;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f7, %r17;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	 %r18, %f8;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f9, %r20;
	max.f32 	%f10, %f8, %f9;
	mov.b32 	 %r21, %f10;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f11, %r23;
	max.f32 	%f12, %f10, %f11;
	mov.b32 	 %r24, %f12;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f13, %r26;
	max.f32 	%f2, %f12, %f13;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB198_4;

	shr.u32 	%r27, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

BB198_4:
	ret;
}

	// .globl	block_reduce_max_f32_32
.visible .entry block_reduce_max_f32_32(
	.param .u64 block_reduce_max_f32_32_param_0,
	.param .u64 block_reduce_max_f32_32_param_1,
	.param .u32 block_reduce_max_f32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_32_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f32_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB199_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	max.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	max.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB199_3;

	shr.u32 	%r24, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB199_3:
	ret;
}

	// .globl	block_reduce_max_f32_16
.visible .entry block_reduce_max_f32_16(
	.param .u64 block_reduce_max_f32_16_param_0,
	.param .u64 block_reduce_max_f32_16_param_1,
	.param .u32 block_reduce_max_f32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_16_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f32_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB200_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	max.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB200_3;

	shr.u32 	%r21, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB200_3:
	ret;
}

	// .globl	block_reduce_max_f32_8
.visible .entry block_reduce_max_f32_8(
	.param .u64 block_reduce_max_f32_8_param_0,
	.param .u64 block_reduce_max_f32_8_param_1,
	.param .u32 block_reduce_max_f32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_8_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f32_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB201_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	max.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB201_3;

	shr.u32 	%r18, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB201_3:
	ret;
}

	// .globl	block_reduce_max_f32_4
.visible .entry block_reduce_max_f32_4(
	.param .u64 block_reduce_max_f32_4_param_0,
	.param .u64 block_reduce_max_f32_4_param_1,
	.param .u32 block_reduce_max_f32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_4_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f32_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB202_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	max.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB202_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r15, %r2, -4;
	cvt.u64.u32	%rd7, %r15;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB202_3:
	ret;
}

	// .globl	block_reduce_max_f32_2
.visible .entry block_reduce_max_f32_2(
	.param .u64 block_reduce_max_f32_2_param_0,
	.param .u64 block_reduce_max_f32_2_param_1,
	.param .u32 block_reduce_max_f32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f32_2_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f32_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB203_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	max.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB203_3;

	shr.u32 	%r12, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB203_3:
	ret;
}

	// .globl	block_reduce_max_f64_1024
.visible .entry block_reduce_max_f64_1024(
	.param .u64 block_reduce_max_f64_1024_param_0,
	.param .u64 block_reduce_max_f64_1024_param_1,
	.param .u32 block_reduce_max_f64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB204_12;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd27, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd27;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB204_3;

	ld.shared.f64 	%fd11, [%r3+4096];
	max.f64 	%fd27, %fd27, %fd11;
	st.shared.f64 	[%r3], %fd27;

BB204_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB204_5;

	ld.shared.f64 	%fd12, [%r3+2048];
	max.f64 	%fd27, %fd27, %fd12;
	st.shared.f64 	[%r3], %fd27;

BB204_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB204_7;

	ld.shared.f64 	%fd13, [%r3+1024];
	max.f64 	%fd27, %fd27, %fd13;
	st.shared.f64 	[%r3], %fd27;

BB204_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB204_9;

	ld.shared.f64 	%fd14, [%r3+512];
	max.f64 	%fd27, %fd27, %fd14;
	st.shared.f64 	[%r3], %fd27;

BB204_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB204_12;

	ld.shared.f64 	%fd25, [%r3+256];
	max.f64 	%fd15, %fd27, %fd25;
	// inline asm
	mov.b64 {%r10,%r11}, %fd15;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r12,%r13};
	// inline asm
	max.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r14,%r15}, %fd17;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r16,%r17};
	// inline asm
	max.f64 	%fd19, %fd17, %fd18;
	// inline asm
	mov.b64 {%r18,%r19}, %fd19;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd20, {%r20,%r21};
	// inline asm
	max.f64 	%fd21, %fd19, %fd20;
	// inline asm
	mov.b64 {%r22,%r23}, %fd21;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd22, {%r24,%r25};
	// inline asm
	max.f64 	%fd23, %fd21, %fd22;
	// inline asm
	mov.b64 {%r26,%r27}, %fd23;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd24, {%r28,%r29};
	// inline asm
	max.f64 	%fd10, %fd23, %fd24;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB204_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd10;

BB204_12:
	ret;
}

	// .globl	block_reduce_max_f64_512
.visible .entry block_reduce_max_f64_512(
	.param .u64 block_reduce_max_f64_512_param_0,
	.param .u64 block_reduce_max_f64_512_param_1,
	.param .u32 block_reduce_max_f64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB205_10;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd24, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd24;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB205_3;

	ld.shared.f64 	%fd9, [%r3+2048];
	max.f64 	%fd24, %fd24, %fd9;
	st.shared.f64 	[%r3], %fd24;

BB205_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB205_5;

	ld.shared.f64 	%fd10, [%r3+1024];
	max.f64 	%fd24, %fd24, %fd10;
	st.shared.f64 	[%r3], %fd24;

BB205_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB205_7;

	ld.shared.f64 	%fd11, [%r3+512];
	max.f64 	%fd24, %fd24, %fd11;
	st.shared.f64 	[%r3], %fd24;

BB205_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB205_10;

	ld.shared.f64 	%fd22, [%r3+256];
	max.f64 	%fd12, %fd24, %fd22;
	// inline asm
	mov.b64 {%r10,%r11}, %fd12;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r12,%r13};
	// inline asm
	max.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r14,%r15}, %fd14;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r16,%r17};
	// inline asm
	max.f64 	%fd16, %fd14, %fd15;
	// inline asm
	mov.b64 {%r18,%r19}, %fd16;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd17, {%r20,%r21};
	// inline asm
	max.f64 	%fd18, %fd16, %fd17;
	// inline asm
	mov.b64 {%r22,%r23}, %fd18;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd19, {%r24,%r25};
	// inline asm
	max.f64 	%fd20, %fd18, %fd19;
	// inline asm
	mov.b64 {%r26,%r27}, %fd20;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd21, {%r28,%r29};
	// inline asm
	max.f64 	%fd8, %fd20, %fd21;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB205_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd8;

BB205_10:
	ret;
}

	// .globl	block_reduce_max_f64_256
.visible .entry block_reduce_max_f64_256(
	.param .u64 block_reduce_max_f64_256_param_0,
	.param .u64 block_reduce_max_f64_256_param_1,
	.param .u32 block_reduce_max_f64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB206_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd20, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB206_3;

	ld.shared.f64 	%fd7, [%r3+1024];
	max.f64 	%fd20, %fd20, %fd7;
	st.shared.f64 	[%r3], %fd20;

BB206_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB206_5;

	ld.shared.f64 	%fd8, [%r3+512];
	max.f64 	%fd20, %fd20, %fd8;
	st.shared.f64 	[%r3], %fd20;

BB206_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB206_8;

	ld.shared.f64 	%fd19, [%r3+256];
	max.f64 	%fd9, %fd20, %fd19;
	// inline asm
	mov.b64 {%r10,%r11}, %fd9;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r12,%r13};
	// inline asm
	max.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r14,%r15}, %fd11;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r16,%r17};
	// inline asm
	max.f64 	%fd13, %fd11, %fd12;
	// inline asm
	mov.b64 {%r18,%r19}, %fd13;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd14, {%r20,%r21};
	// inline asm
	max.f64 	%fd15, %fd13, %fd14;
	// inline asm
	mov.b64 {%r22,%r23}, %fd15;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r24,%r25};
	// inline asm
	max.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r26,%r27}, %fd17;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r28,%r29};
	// inline asm
	max.f64 	%fd6, %fd17, %fd18;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB206_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd6;

BB206_8:
	ret;
}

	// .globl	block_reduce_max_f64_128
.visible .entry block_reduce_max_f64_128(
	.param .u64 block_reduce_max_f64_128_param_0,
	.param .u64 block_reduce_max_f64_128_param_1,
	.param .u32 block_reduce_max_f64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<18>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB207_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB207_3;

	ld.shared.f64 	%fd5, [%r3+512];
	max.f64 	%fd17, %fd17, %fd5;
	st.shared.f64 	[%r3], %fd17;

BB207_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB207_6;

	ld.shared.f64 	%fd16, [%r3+256];
	max.f64 	%fd6, %fd17, %fd16;
	// inline asm
	mov.b64 {%r10,%r11}, %fd6;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd7, {%r12,%r13};
	// inline asm
	max.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r14,%r15}, %fd8;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd9, {%r16,%r17};
	// inline asm
	max.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r18,%r19}, %fd10;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd11, {%r20,%r21};
	// inline asm
	max.f64 	%fd12, %fd10, %fd11;
	// inline asm
	mov.b64 {%r22,%r23}, %fd12;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r24,%r25};
	// inline asm
	max.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r26,%r27}, %fd14;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r28,%r29};
	// inline asm
	max.f64 	%fd4, %fd14, %fd15;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB207_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd4;

BB207_6:
	ret;
}

	// .globl	block_reduce_max_f64_64
.visible .entry block_reduce_max_f64_64(
	.param .u64 block_reduce_max_f64_64_param_0,
	.param .u64 block_reduce_max_f64_64_param_1,
	.param .u32 block_reduce_max_f64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_max_f64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB208_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB208_4;

	ld.shared.f64 	%fd13, [%r3+256];
	max.f64 	%fd3, %fd1, %fd13;
	// inline asm
	mov.b64 {%r10,%r11}, %fd3;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd4, {%r12,%r13};
	// inline asm
	max.f64 	%fd5, %fd3, %fd4;
	// inline asm
	mov.b64 {%r14,%r15}, %fd5;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd6, {%r16,%r17};
	// inline asm
	max.f64 	%fd7, %fd5, %fd6;
	// inline asm
	mov.b64 {%r18,%r19}, %fd7;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd8, {%r20,%r21};
	// inline asm
	max.f64 	%fd9, %fd7, %fd8;
	// inline asm
	mov.b64 {%r22,%r23}, %fd9;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r24,%r25};
	// inline asm
	max.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r26,%r27}, %fd11;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r28,%r29};
	// inline asm
	max.f64 	%fd2, %fd11, %fd12;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB208_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd2;

BB208_4:
	ret;
}

	// .globl	block_reduce_max_f64_32
.visible .entry block_reduce_max_f64_32(
	.param .u64 block_reduce_max_f64_32_param_0,
	.param .u64 block_reduce_max_f64_32_param_1,
	.param .u32 block_reduce_max_f64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB209_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	max.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	max.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	max.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	max.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r22,%r23}, %fd10;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %fd11, {%r24,%r25};
	// inline asm
	max.f64 	%fd1, %fd10, %fd11;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB209_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r34, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB209_3:
	ret;
}

	// .globl	block_reduce_max_f64_16
.visible .entry block_reduce_max_f64_16(
	.param .u64 block_reduce_max_f64_16_param_0,
	.param .u64 block_reduce_max_f64_16_param_1,
	.param .u32 block_reduce_max_f64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB210_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	max.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	max.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	max.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	max.f64 	%fd1, %fd8, %fd9;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB210_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r29, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB210_3:
	ret;
}

	// .globl	block_reduce_max_f64_8
.visible .entry block_reduce_max_f64_8(
	.param .u64 block_reduce_max_f64_8_param_0,
	.param .u64 block_reduce_max_f64_8_param_1,
	.param .u32 block_reduce_max_f64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB211_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	max.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	max.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	max.f64 	%fd1, %fd6, %fd7;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB211_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd7, %r24;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB211_3:
	ret;
}

	// .globl	block_reduce_max_f64_4
.visible .entry block_reduce_max_f64_4(
	.param .u64 block_reduce_max_f64_4_param_0,
	.param .u64 block_reduce_max_f64_4_param_1,
	.param .u32 block_reduce_max_f64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB212_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	max.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	max.f64 	%fd1, %fd4, %fd5;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB212_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB212_3:
	ret;
}

	// .globl	block_reduce_max_f64_2
.visible .entry block_reduce_max_f64_2(
	.param .u64 block_reduce_max_f64_2_param_0,
	.param .u64 block_reduce_max_f64_2_param_1,
	.param .u32 block_reduce_max_f64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_f64_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_f64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_max_f64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB213_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	max.f64 	%fd1, %fd2, %fd3;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB213_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB213_3:
	ret;
}

	// .globl	block_reduce_max_u32_1024
.visible .entry block_reduce_max_u32_1024(
	.param .u64 block_reduce_max_u32_1024_param_0,
	.param .u64 block_reduce_max_u32_1024_param_1,
	.param .u32 block_reduce_max_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_max_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB214_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB214_3;

	ld.shared.u32 	%r28, [%r23+2048];
	max.u32 	%r81, %r81, %r28;
	st.shared.u32 	[%r23], %r81;

BB214_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB214_5;

	ld.shared.u32 	%r35, [%r23+1024];
	max.u32 	%r81, %r81, %r35;
	st.shared.u32 	[%r23], %r81;

BB214_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB214_7;

	ld.shared.u32 	%r42, [%r23+512];
	max.u32 	%r81, %r81, %r42;
	st.shared.u32 	[%r23], %r81;

BB214_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB214_9;

	ld.shared.u32 	%r49, [%r23+256];
	max.u32 	%r81, %r81, %r49;
	st.shared.u32 	[%r23], %r81;

BB214_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB214_12;

	ld.shared.u32 	%r56, [%r23+128];
	max.u32 	%r57, %r81, %r56;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	max.u32 	%r62, %r57, %r61;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	max.u32 	%r65, %r62, %r64;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	max.u32 	%r68, %r65, %r67;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	max.u32 	%r71, %r68, %r70;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	max.u32 	%r11, %r71, %r73;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB214_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB214_12:
	ret;
}

	// .globl	block_reduce_max_u32_512
.visible .entry block_reduce_max_u32_512(
	.param .u64 block_reduce_max_u32_512_param_0,
	.param .u64 block_reduce_max_u32_512_param_1,
	.param .u32 block_reduce_max_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_max_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB215_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB215_3;

	ld.shared.u32 	%r19, [%r4+1024];
	max.u32 	%r54, %r54, %r19;
	st.shared.u32 	[%r4], %r54;

BB215_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB215_5;

	ld.shared.u32 	%r22, [%r4+512];
	max.u32 	%r54, %r54, %r22;
	st.shared.u32 	[%r4], %r54;

BB215_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB215_7;

	ld.shared.u32 	%r25, [%r4+256];
	max.u32 	%r54, %r54, %r25;
	st.shared.u32 	[%r4], %r54;

BB215_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB215_10;

	ld.shared.u32 	%r28, [%r4+128];
	max.u32 	%r29, %r54, %r28;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	max.u32 	%r34, %r29, %r33;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	max.u32 	%r37, %r34, %r36;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	max.u32 	%r40, %r37, %r39;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	max.u32 	%r43, %r40, %r42;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	max.u32 	%r11, %r43, %r45;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB215_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB215_10:
	ret;
}

	// .globl	block_reduce_max_u32_256
.visible .entry block_reduce_max_u32_256(
	.param .u64 block_reduce_max_u32_256_param_0,
	.param .u64 block_reduce_max_u32_256_param_1,
	.param .u32 block_reduce_max_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_max_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB216_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB216_3;

	ld.shared.u32 	%r16, [%r4+512];
	max.u32 	%r41, %r41, %r16;
	st.shared.u32 	[%r4], %r41;

BB216_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB216_5;

	ld.shared.u32 	%r17, [%r4+256];
	max.u32 	%r41, %r41, %r17;
	st.shared.u32 	[%r4], %r41;

BB216_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB216_8;

	ld.shared.u32 	%r18, [%r4+128];
	max.u32 	%r19, %r41, %r18;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	max.u32 	%r24, %r19, %r23;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	max.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	max.u32 	%r30, %r27, %r29;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	max.u32 	%r33, %r30, %r32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	max.u32 	%r10, %r33, %r35;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB216_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB216_8:
	ret;
}

	// .globl	block_reduce_max_u32_128
.visible .entry block_reduce_max_u32_128(
	.param .u64 block_reduce_max_u32_128_param_0,
	.param .u64 block_reduce_max_u32_128_param_1,
	.param .u32 block_reduce_max_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_max_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB217_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB217_3;

	ld.shared.u32 	%r14, [%r4+256];
	max.u32 	%r34, %r34, %r14;
	st.shared.u32 	[%r4], %r34;

BB217_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB217_6;

	ld.shared.u32 	%r15, [%r4+128];
	max.u32 	%r16, %r34, %r15;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	max.u32 	%r21, %r16, %r20;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	max.u32 	%r24, %r21, %r23;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	max.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	max.u32 	%r30, %r27, %r29;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	max.u32 	%r8, %r30, %r32;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB217_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB217_6:
	ret;
}

	// .globl	block_reduce_max_u32_64
.visible .entry block_reduce_max_u32_64(
	.param .u64 block_reduce_max_u32_64_param_0,
	.param .u64 block_reduce_max_u32_64_param_1,
	.param .u32 block_reduce_max_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_max_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB218_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB218_4;

	ld.shared.u32 	%r12, [%r4+128];
	max.u32 	%r13, %r3, %r12;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	max.u32 	%r18, %r13, %r17;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	max.u32 	%r21, %r18, %r20;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	max.u32 	%r24, %r21, %r23;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	max.u32 	%r27, %r24, %r26;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	max.u32 	%r6, %r27, %r29;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB218_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB218_4:
	ret;
}

	// .globl	block_reduce_max_u32_32
.visible .entry block_reduce_max_u32_32(
	.param .u64 block_reduce_max_u32_32_param_0,
	.param .u64 block_reduce_max_u32_32_param_1,
	.param .u32 block_reduce_max_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_max_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB219_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.u32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	max.u32 	%r21, %r18, %r20;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	max.u32 	%r3, %r21, %r23;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB219_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB219_3:
	ret;
}

	// .globl	block_reduce_max_u32_16
.visible .entry block_reduce_max_u32_16(
	.param .u64 block_reduce_max_u32_16_param_0,
	.param .u64 block_reduce_max_u32_16_param_1,
	.param .u32 block_reduce_max_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_max_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB220_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.u32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	max.u32 	%r3, %r18, %r20;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB220_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB220_3:
	ret;
}

	// .globl	block_reduce_max_u32_8
.visible .entry block_reduce_max_u32_8(
	.param .u64 block_reduce_max_u32_8_param_0,
	.param .u64 block_reduce_max_u32_8_param_1,
	.param .u32 block_reduce_max_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_max_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB221_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.u32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.u32 	%r3, %r15, %r17;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB221_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB221_3:
	ret;
}

	// .globl	block_reduce_max_u32_4
.visible .entry block_reduce_max_u32_4(
	.param .u64 block_reduce_max_u32_4_param_0,
	.param .u64 block_reduce_max_u32_4_param_1,
	.param .u32 block_reduce_max_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_max_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB222_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.u32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.u32 	%r3, %r12, %r14;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB222_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB222_3:
	ret;
}

	// .globl	block_reduce_max_u32_2
.visible .entry block_reduce_max_u32_2(
	.param .u64 block_reduce_max_u32_2_param_0,
	.param .u64 block_reduce_max_u32_2_param_1,
	.param .u32 block_reduce_max_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_max_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB223_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.u32 	%r3, %r7, %r11;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB223_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB223_3:
	ret;
}

	// .globl	block_reduce_max_u64_1024
.visible .entry block_reduce_max_u64_1024(
	.param .u64 block_reduce_max_u64_1024_param_0,
	.param .u64 block_reduce_max_u64_1024_param_1,
	.param .u32 block_reduce_max_u64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_max_u64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_max_u64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_max_u64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB224_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u32 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB224_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	max.u64 	%rd35, %rd35, %rd16;
	st.shared.u64 	[%r3], %rd35;

BB224_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB224_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	max.u64 	%rd35, %rd35, %rd17;
	st.shared.u64 	[%r3], %rd35;

BB224_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB224_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	max.u64 	%rd35, %rd35, %rd18;
	st.shared.u64 	[%r3], %rd35;

BB224_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB224_9;

	ld.shared.u64 	%rd19, [%r3+512];
	max.u64 	%rd35, %rd35, %rd19;
	st.shared.u64 	[%r3], %rd35;

BB224_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB224_12;

	ld.shared.u64 	%rd30, [%r3+256];
	max.u64 	%rd20, %rd35, %rd30;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	max.u64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	max.u64 	%rd24, %rd22, %rd23;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	max.u64 	%rd26, %rd24, %rd25;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	max.u64 	%rd28, %rd26, %rd27;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	max.u64 	%rd10, %rd28, %rd29;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB224_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 4;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u32 	[%rd33], %rd10;

BB224_12:
	ret;
}

	// .globl	block_reduce_max_u64_512
.visible .entry block_reduce_max_u64_512(
	.param .u64 block_reduce_max_u64_512_param_0,
	.param .u64 block_reduce_max_u64_512_param_1,
	.param .u32 block_reduce_max_u64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_max_u64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_max_u64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_max_u64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB225_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB225_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	max.u64 	%rd32, %rd32, %rd14;
	st.shared.u64 	[%r3], %rd32;

BB225_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB225_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	max.u64 	%rd32, %rd32, %rd15;
	st.shared.u64 	[%r3], %rd32;

BB225_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB225_7;

	ld.shared.u64 	%rd16, [%r3+512];
	max.u64 	%rd32, %rd32, %rd16;
	st.shared.u64 	[%r3], %rd32;

BB225_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB225_10;

	ld.shared.u64 	%rd27, [%r3+256];
	max.u64 	%rd17, %rd32, %rd27;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	max.u64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	max.u64 	%rd21, %rd19, %rd20;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	max.u64 	%rd23, %rd21, %rd22;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	max.u64 	%rd25, %rd23, %rd24;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	max.u64 	%rd8, %rd25, %rd26;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB225_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u32 	[%rd30], %rd8;

BB225_10:
	ret;
}

	// .globl	block_reduce_max_u64_256
.visible .entry block_reduce_max_u64_256(
	.param .u64 block_reduce_max_u64_256_param_0,
	.param .u64 block_reduce_max_u64_256_param_1,
	.param .u32 block_reduce_max_u64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_max_u64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_max_u64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_max_u64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB226_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB226_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	max.u64 	%rd28, %rd28, %rd12;
	st.shared.u64 	[%r3], %rd28;

BB226_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB226_5;

	ld.shared.u64 	%rd13, [%r3+512];
	max.u64 	%rd28, %rd28, %rd13;
	st.shared.u64 	[%r3], %rd28;

BB226_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB226_8;

	ld.shared.u64 	%rd24, [%r3+256];
	max.u64 	%rd14, %rd28, %rd24;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	max.u64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	max.u64 	%rd18, %rd16, %rd17;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	max.u64 	%rd20, %rd18, %rd19;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	max.u64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	max.u64 	%rd6, %rd22, %rd23;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB226_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %rd6;

BB226_8:
	ret;
}

	// .globl	block_reduce_max_u64_128
.visible .entry block_reduce_max_u64_128(
	.param .u64 block_reduce_max_u64_128_param_0,
	.param .u64 block_reduce_max_u64_128_param_1,
	.param .u32 block_reduce_max_u64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_max_u64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_max_u64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_max_u64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB227_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB227_3;

	ld.shared.u64 	%rd10, [%r3+512];
	max.u64 	%rd25, %rd25, %rd10;
	st.shared.u64 	[%r3], %rd25;

BB227_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB227_6;

	ld.shared.u64 	%rd21, [%r3+256];
	max.u64 	%rd11, %rd25, %rd21;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	max.u64 	%rd13, %rd11, %rd12;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	max.u64 	%rd15, %rd13, %rd14;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	max.u64 	%rd17, %rd15, %rd16;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	max.u64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	max.u64 	%rd4, %rd19, %rd20;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB227_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 4;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u32 	[%rd24], %rd4;

BB227_6:
	ret;
}

	// .globl	block_reduce_max_u64_64
.visible .entry block_reduce_max_u64_64(
	.param .u64 block_reduce_max_u64_64_param_0,
	.param .u64 block_reduce_max_u64_64_param_1,
	.param .u32 block_reduce_max_u64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_max_u64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_max_u64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_max_u64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB228_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB228_4;

	ld.shared.u64 	%rd18, [%r3+256];
	max.u64 	%rd8, %rd1, %rd18;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	max.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	max.u64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	max.u64 	%rd14, %rd12, %rd13;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	max.u64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	max.u64 	%rd2, %rd16, %rd17;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB228_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 4;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %rd2;

BB228_4:
	ret;
}

	// .globl	block_reduce_max_u64_32
.visible .entry block_reduce_max_u64_32(
	.param .u64 block_reduce_max_u64_32_param_0,
	.param .u64 block_reduce_max_u64_32_param_1,
	.param .u32 block_reduce_max_u64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_max_u64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_u64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_max_u64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB229_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u32 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	max.u64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	max.u64 	%rd1, %rd12, %rd13;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB229_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %rd1;

BB229_3:
	ret;
}

	// .globl	block_reduce_max_u64_16
.visible .entry block_reduce_max_u64_16(
	.param .u64 block_reduce_max_u64_16_param_0,
	.param .u64 block_reduce_max_u64_16_param_1,
	.param .u32 block_reduce_max_u64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_max_u64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_u64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_max_u64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB230_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u32 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.u64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	max.u64 	%rd1, %rd10, %rd11;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB230_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %rd1;

BB230_3:
	ret;
}

	// .globl	block_reduce_max_u64_8
.visible .entry block_reduce_max_u64_8(
	.param .u64 block_reduce_max_u64_8_param_0,
	.param .u64 block_reduce_max_u64_8_param_1,
	.param .u32 block_reduce_max_u64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_max_u64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_u64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_max_u64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB231_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.u64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.u64 	%rd1, %rd8, %rd9;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB231_3;

	shr.u32 	%r24, %r2, 3;
	cvta.to.global.u64 	%rd13, %rd3;
	mul.wide.u32 	%rd14, %r24, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %rd1;

BB231_3:
	ret;
}

	// .globl	block_reduce_max_u64_4
.visible .entry block_reduce_max_u64_4(
	.param .u64 block_reduce_max_u64_4_param_0,
	.param .u64 block_reduce_max_u64_4_param_1,
	.param .u32 block_reduce_max_u64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_max_u64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_u64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_max_u64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB232_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.u64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.u64 	%rd1, %rd6, %rd7;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB232_3;

	cvta.to.global.u64 	%rd11, %rd3;
	and.b32  	%r19, %r2, -4;
	cvt.u64.u32	%rd12, %r19;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %rd1;

BB232_3:
	ret;
}

	// .globl	block_reduce_max_u64_2
.visible .entry block_reduce_max_u64_2(
	.param .u64 block_reduce_max_u64_2_param_0,
	.param .u64 block_reduce_max_u64_2_param_1,
	.param .u32 block_reduce_max_u64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_max_u64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_u64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_max_u64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB233_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.u64 	%rd1, %rd4, %rd5;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB233_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %rd1;

BB233_3:
	ret;
}

	// .globl	block_reduce_max_i32_1024
.visible .entry block_reduce_max_i32_1024(
	.param .u64 block_reduce_max_i32_1024_param_0,
	.param .u64 block_reduce_max_i32_1024_param_1,
	.param .u32 block_reduce_max_i32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_max_i32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB234_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB234_3;

	ld.shared.u32 	%r28, [%r23+2048];
	max.s32 	%r81, %r81, %r28;
	st.shared.u32 	[%r23], %r81;

BB234_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB234_5;

	ld.shared.u32 	%r35, [%r23+1024];
	max.s32 	%r81, %r81, %r35;
	st.shared.u32 	[%r23], %r81;

BB234_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB234_7;

	ld.shared.u32 	%r42, [%r23+512];
	max.s32 	%r81, %r81, %r42;
	st.shared.u32 	[%r23], %r81;

BB234_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB234_9;

	ld.shared.u32 	%r49, [%r23+256];
	max.s32 	%r81, %r81, %r49;
	st.shared.u32 	[%r23], %r81;

BB234_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB234_12;

	ld.shared.u32 	%r56, [%r23+128];
	max.s32 	%r57, %r81, %r56;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	max.s32 	%r62, %r57, %r61;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	max.s32 	%r65, %r62, %r64;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	max.s32 	%r68, %r65, %r67;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	max.s32 	%r71, %r68, %r70;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	max.s32 	%r11, %r71, %r73;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB234_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB234_12:
	ret;
}

	// .globl	block_reduce_max_i32_512
.visible .entry block_reduce_max_i32_512(
	.param .u64 block_reduce_max_i32_512_param_0,
	.param .u64 block_reduce_max_i32_512_param_1,
	.param .u32 block_reduce_max_i32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_max_i32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB235_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB235_3;

	ld.shared.u32 	%r19, [%r4+1024];
	max.s32 	%r54, %r54, %r19;
	st.shared.u32 	[%r4], %r54;

BB235_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB235_5;

	ld.shared.u32 	%r22, [%r4+512];
	max.s32 	%r54, %r54, %r22;
	st.shared.u32 	[%r4], %r54;

BB235_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB235_7;

	ld.shared.u32 	%r25, [%r4+256];
	max.s32 	%r54, %r54, %r25;
	st.shared.u32 	[%r4], %r54;

BB235_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB235_10;

	ld.shared.u32 	%r28, [%r4+128];
	max.s32 	%r29, %r54, %r28;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	max.s32 	%r34, %r29, %r33;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	max.s32 	%r37, %r34, %r36;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	max.s32 	%r40, %r37, %r39;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	max.s32 	%r43, %r40, %r42;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	max.s32 	%r11, %r43, %r45;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB235_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB235_10:
	ret;
}

	// .globl	block_reduce_max_i32_256
.visible .entry block_reduce_max_i32_256(
	.param .u64 block_reduce_max_i32_256_param_0,
	.param .u64 block_reduce_max_i32_256_param_1,
	.param .u32 block_reduce_max_i32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_max_i32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB236_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB236_3;

	ld.shared.u32 	%r16, [%r4+512];
	max.s32 	%r41, %r41, %r16;
	st.shared.u32 	[%r4], %r41;

BB236_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB236_5;

	ld.shared.u32 	%r17, [%r4+256];
	max.s32 	%r41, %r41, %r17;
	st.shared.u32 	[%r4], %r41;

BB236_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB236_8;

	ld.shared.u32 	%r18, [%r4+128];
	max.s32 	%r19, %r41, %r18;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	max.s32 	%r24, %r19, %r23;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	max.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	max.s32 	%r30, %r27, %r29;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	max.s32 	%r33, %r30, %r32;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	max.s32 	%r10, %r33, %r35;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB236_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB236_8:
	ret;
}

	// .globl	block_reduce_max_i32_128
.visible .entry block_reduce_max_i32_128(
	.param .u64 block_reduce_max_i32_128_param_0,
	.param .u64 block_reduce_max_i32_128_param_1,
	.param .u32 block_reduce_max_i32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_max_i32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB237_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB237_3;

	ld.shared.u32 	%r14, [%r4+256];
	max.s32 	%r34, %r34, %r14;
	st.shared.u32 	[%r4], %r34;

BB237_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB237_6;

	ld.shared.u32 	%r15, [%r4+128];
	max.s32 	%r16, %r34, %r15;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	max.s32 	%r21, %r16, %r20;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	max.s32 	%r24, %r21, %r23;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	max.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	max.s32 	%r30, %r27, %r29;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	max.s32 	%r8, %r30, %r32;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB237_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB237_6:
	ret;
}

	// .globl	block_reduce_max_i32_64
.visible .entry block_reduce_max_i32_64(
	.param .u64 block_reduce_max_i32_64_param_0,
	.param .u64 block_reduce_max_i32_64_param_1,
	.param .u32 block_reduce_max_i32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_max_i32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB238_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB238_4;

	ld.shared.u32 	%r12, [%r4+128];
	max.s32 	%r13, %r3, %r12;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	max.s32 	%r18, %r13, %r17;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	max.s32 	%r21, %r18, %r20;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	max.s32 	%r24, %r21, %r23;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	max.s32 	%r27, %r24, %r26;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	max.s32 	%r6, %r27, %r29;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB238_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB238_4:
	ret;
}

	// .globl	block_reduce_max_i32_32
.visible .entry block_reduce_max_i32_32(
	.param .u64 block_reduce_max_i32_32_param_0,
	.param .u64 block_reduce_max_i32_32_param_1,
	.param .u32 block_reduce_max_i32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_max_i32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB239_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.s32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	max.s32 	%r21, %r18, %r20;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	max.s32 	%r3, %r21, %r23;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB239_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB239_3:
	ret;
}

	// .globl	block_reduce_max_i32_16
.visible .entry block_reduce_max_i32_16(
	.param .u64 block_reduce_max_i32_16_param_0,
	.param .u64 block_reduce_max_i32_16_param_1,
	.param .u32 block_reduce_max_i32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_max_i32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB240_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.s32 	%r18, %r15, %r17;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	max.s32 	%r3, %r18, %r20;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB240_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB240_3:
	ret;
}

	// .globl	block_reduce_max_i32_8
.visible .entry block_reduce_max_i32_8(
	.param .u64 block_reduce_max_i32_8_param_0,
	.param .u64 block_reduce_max_i32_8_param_1,
	.param .u32 block_reduce_max_i32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_max_i32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB241_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.s32 	%r15, %r12, %r14;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	max.s32 	%r3, %r15, %r17;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB241_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB241_3:
	ret;
}

	// .globl	block_reduce_max_i32_4
.visible .entry block_reduce_max_i32_4(
	.param .u64 block_reduce_max_i32_4_param_0,
	.param .u64 block_reduce_max_i32_4_param_1,
	.param .u32 block_reduce_max_i32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_max_i32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB242_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.s32 	%r12, %r7, %r11;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	max.s32 	%r3, %r12, %r14;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB242_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB242_3:
	ret;
}

	// .globl	block_reduce_max_i32_2
.visible .entry block_reduce_max_i32_2(
	.param .u64 block_reduce_max_i32_2_param_0,
	.param .u64 block_reduce_max_i32_2_param_1,
	.param .u32 block_reduce_max_i32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_max_i32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_max_i32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_max_i32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB243_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	max.s32 	%r3, %r7, %r11;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB243_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB243_3:
	ret;
}

	// .globl	block_reduce_max_i64_1024
.visible .entry block_reduce_max_i64_1024(
	.param .u64 block_reduce_max_i64_1024_param_0,
	.param .u64 block_reduce_max_i64_1024_param_1,
	.param .u32 block_reduce_max_i64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_max_i64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_max_i64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_max_i64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB244_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 8;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u64 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB244_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	max.s64 	%rd35, %rd35, %rd16;
	st.shared.u64 	[%r3], %rd35;

BB244_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB244_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	max.s64 	%rd35, %rd35, %rd17;
	st.shared.u64 	[%r3], %rd35;

BB244_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB244_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	max.s64 	%rd35, %rd35, %rd18;
	st.shared.u64 	[%r3], %rd35;

BB244_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB244_9;

	ld.shared.u64 	%rd19, [%r3+512];
	max.s64 	%rd35, %rd35, %rd19;
	st.shared.u64 	[%r3], %rd35;

BB244_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB244_12;

	ld.shared.u64 	%rd30, [%r3+256];
	max.s64 	%rd20, %rd35, %rd30;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	max.s64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	max.s64 	%rd24, %rd22, %rd23;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	max.s64 	%rd26, %rd24, %rd25;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	max.s64 	%rd28, %rd26, %rd27;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	max.s64 	%rd10, %rd28, %rd29;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB244_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 8;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u64 	[%rd33], %rd10;

BB244_12:
	ret;
}

	// .globl	block_reduce_max_i64_512
.visible .entry block_reduce_max_i64_512(
	.param .u64 block_reduce_max_i64_512_param_0,
	.param .u64 block_reduce_max_i64_512_param_1,
	.param .u32 block_reduce_max_i64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_max_i64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_max_i64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_max_i64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB245_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u64 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB245_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	max.s64 	%rd32, %rd32, %rd14;
	st.shared.u64 	[%r3], %rd32;

BB245_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB245_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	max.s64 	%rd32, %rd32, %rd15;
	st.shared.u64 	[%r3], %rd32;

BB245_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB245_7;

	ld.shared.u64 	%rd16, [%r3+512];
	max.s64 	%rd32, %rd32, %rd16;
	st.shared.u64 	[%r3], %rd32;

BB245_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB245_10;

	ld.shared.u64 	%rd27, [%r3+256];
	max.s64 	%rd17, %rd32, %rd27;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	max.s64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	max.s64 	%rd21, %rd19, %rd20;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	max.s64 	%rd23, %rd21, %rd22;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	max.s64 	%rd25, %rd23, %rd24;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	max.s64 	%rd8, %rd25, %rd26;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB245_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 8;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u64 	[%rd30], %rd8;

BB245_10:
	ret;
}

	// .globl	block_reduce_max_i64_256
.visible .entry block_reduce_max_i64_256(
	.param .u64 block_reduce_max_i64_256_param_0,
	.param .u64 block_reduce_max_i64_256_param_1,
	.param .u32 block_reduce_max_i64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_max_i64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_max_i64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_max_i64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB246_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u64 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB246_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	max.s64 	%rd28, %rd28, %rd12;
	st.shared.u64 	[%r3], %rd28;

BB246_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB246_5;

	ld.shared.u64 	%rd13, [%r3+512];
	max.s64 	%rd28, %rd28, %rd13;
	st.shared.u64 	[%r3], %rd28;

BB246_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB246_8;

	ld.shared.u64 	%rd24, [%r3+256];
	max.s64 	%rd14, %rd28, %rd24;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	max.s64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	max.s64 	%rd18, %rd16, %rd17;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	max.s64 	%rd20, %rd18, %rd19;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	max.s64 	%rd22, %rd20, %rd21;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	max.s64 	%rd6, %rd22, %rd23;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB246_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 8;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u64 	[%rd27], %rd6;

BB246_8:
	ret;
}

	// .globl	block_reduce_max_i64_128
.visible .entry block_reduce_max_i64_128(
	.param .u64 block_reduce_max_i64_128_param_0,
	.param .u64 block_reduce_max_i64_128_param_1,
	.param .u32 block_reduce_max_i64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_max_i64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_max_i64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_max_i64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB247_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u64 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB247_3;

	ld.shared.u64 	%rd10, [%r3+512];
	max.s64 	%rd25, %rd25, %rd10;
	st.shared.u64 	[%r3], %rd25;

BB247_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB247_6;

	ld.shared.u64 	%rd21, [%r3+256];
	max.s64 	%rd11, %rd25, %rd21;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	max.s64 	%rd13, %rd11, %rd12;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	max.s64 	%rd15, %rd13, %rd14;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	max.s64 	%rd17, %rd15, %rd16;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	max.s64 	%rd19, %rd17, %rd18;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	max.s64 	%rd4, %rd19, %rd20;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB247_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 8;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u64 	[%rd24], %rd4;

BB247_6:
	ret;
}

	// .globl	block_reduce_max_i64_64
.visible .entry block_reduce_max_i64_64(
	.param .u64 block_reduce_max_i64_64_param_0,
	.param .u64 block_reduce_max_i64_64_param_1,
	.param .u32 block_reduce_max_i64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_max_i64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_max_i64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_max_i64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB248_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u64 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB248_4;

	ld.shared.u64 	%rd18, [%r3+256];
	max.s64 	%rd8, %rd1, %rd18;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	max.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	max.s64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	max.s64 	%rd14, %rd12, %rd13;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	max.s64 	%rd16, %rd14, %rd15;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	max.s64 	%rd2, %rd16, %rd17;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB248_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 8;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u64 	[%rd21], %rd2;

BB248_4:
	ret;
}

	// .globl	block_reduce_max_i64_32
.visible .entry block_reduce_max_i64_32(
	.param .u64 block_reduce_max_i64_32_param_0,
	.param .u64 block_reduce_max_i64_32_param_1,
	.param .u32 block_reduce_max_i64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_max_i64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_i64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_max_i64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB249_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 8;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u64 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	max.s64 	%rd12, %rd10, %rd11;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	max.s64 	%rd1, %rd12, %rd13;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB249_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 8;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u64 	[%rd19], %rd1;

BB249_3:
	ret;
}

	// .globl	block_reduce_max_i64_16
.visible .entry block_reduce_max_i64_16(
	.param .u64 block_reduce_max_i64_16_param_0,
	.param .u64 block_reduce_max_i64_16_param_1,
	.param .u32 block_reduce_max_i64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_max_i64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_i64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_max_i64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB250_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u64 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.s64 	%rd10, %rd8, %rd9;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	max.s64 	%rd1, %rd10, %rd11;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB250_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u64 	[%rd17], %rd1;

BB250_3:
	ret;
}

	// .globl	block_reduce_max_i64_8
.visible .entry block_reduce_max_i64_8(
	.param .u64 block_reduce_max_i64_8_param_0,
	.param .u64 block_reduce_max_i64_8_param_1,
	.param .u32 block_reduce_max_i64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_max_i64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_i64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_max_i64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB251_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u64 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.s64 	%rd8, %rd6, %rd7;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	max.s64 	%rd1, %rd8, %rd9;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB251_3;

	cvta.to.global.u64 	%rd13, %rd3;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd14, %r24;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u64 	[%rd15], %rd1;

BB251_3:
	ret;
}

	// .globl	block_reduce_max_i64_4
.visible .entry block_reduce_max_i64_4(
	.param .u64 block_reduce_max_i64_4_param_0,
	.param .u64 block_reduce_max_i64_4_param_1,
	.param .u32 block_reduce_max_i64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_max_i64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_i64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_max_i64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB252_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.s64 	%rd6, %rd4, %rd5;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	max.s64 	%rd1, %rd6, %rd7;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB252_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd11, %rd3;
	mul.wide.u32 	%rd12, %r19, 8;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u64 	[%rd13], %rd1;

BB252_3:
	ret;
}

	// .globl	block_reduce_max_i64_2
.visible .entry block_reduce_max_i64_2(
	.param .u64 block_reduce_max_i64_2_param_0,
	.param .u64 block_reduce_max_i64_2_param_1,
	.param .u32 block_reduce_max_i64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_max_i64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_max_i64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_max_i64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB253_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	max.s64 	%rd1, %rd4, %rd5;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB253_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 8;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u64 	[%rd11], %rd1;

BB253_3:
	ret;
}

	// .globl	block_reduce_mul_f16_1024
.visible .entry block_reduce_mul_f16_1024(
	.param .u64 block_reduce_mul_f16_1024_param_0,
	.param .u64 block_reduce_mul_f16_1024_param_1,
	.param .u32 block_reduce_mul_f16_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_mul_f16_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB254_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f29, %rs1;}

	// inline asm
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f29;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB254_3;

	ld.shared.f32 	%f12, [%r13+2048];
	mul.f32 	%f29, %f29, %f12;
	st.shared.f32 	[%r13], %f29;

BB254_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB254_5;

	ld.shared.f32 	%f13, [%r13+1024];
	mul.f32 	%f29, %f29, %f13;
	st.shared.f32 	[%r13], %f29;

BB254_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB254_7;

	ld.shared.f32 	%f14, [%r13+512];
	mul.f32 	%f29, %f29, %f14;
	st.shared.f32 	[%r13], %f29;

BB254_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB254_9;

	ld.shared.f32 	%f15, [%r13+256];
	mul.f32 	%f29, %f29, %f15;
	st.shared.f32 	[%r13], %f29;

BB254_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB254_12;

	ld.shared.f32 	%f16, [%r13+128];
	mul.f32 	%f17, %f29, %f16;
	mov.b32 	 %r42, %f17;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f18, %r46;
	mul.f32 	%f19, %f17, %f18;
	mov.b32 	 %r47, %f19;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f20, %r49;
	mul.f32 	%f21, %f19, %f20;
	mov.b32 	 %r50, %f21;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f22, %r52;
	mul.f32 	%f23, %f21, %f22;
	mov.b32 	 %r53, %f23;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f24, %r55;
	mul.f32 	%f25, %f23, %f24;
	mov.b32 	 %r56, %f25;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f26, %r58;
	mul.f32 	%f10, %f25, %f26;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB254_12;

	shr.u32 	%r64, %r6, 10;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f10;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB254_12:
	ret;
}

	// .globl	block_reduce_mul_f16_512
.visible .entry block_reduce_mul_f16_512(
	.param .u64 block_reduce_mul_f16_512_param_0,
	.param .u64 block_reduce_mul_f16_512_param_1,
	.param .u32 block_reduce_mul_f16_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<28>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_512_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_f16_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB255_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f26, %rs1;}

	// inline asm
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f26;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB255_3;

	ld.shared.f32 	%f10, [%r3+1024];
	mul.f32 	%f26, %f26, %f10;
	st.shared.f32 	[%r3], %f26;

BB255_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB255_5;

	ld.shared.f32 	%f11, [%r3+512];
	mul.f32 	%f26, %f26, %f11;
	st.shared.f32 	[%r3], %f26;

BB255_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB255_7;

	ld.shared.f32 	%f12, [%r3+256];
	mul.f32 	%f26, %f26, %f12;
	st.shared.f32 	[%r3], %f26;

BB255_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB255_10;

	ld.shared.f32 	%f13, [%r3+128];
	mul.f32 	%f14, %f26, %f13;
	mov.b32 	 %r17, %f14;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f15, %r21;
	mul.f32 	%f16, %f14, %f15;
	mov.b32 	 %r22, %f16;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f17, %r24;
	mul.f32 	%f18, %f16, %f17;
	mov.b32 	 %r25, %f18;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f19, %r27;
	mul.f32 	%f20, %f18, %f19;
	mov.b32 	 %r28, %f20;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f21, %r30;
	mul.f32 	%f22, %f20, %f21;
	mov.b32 	 %r31, %f22;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f23, %r33;
	mul.f32 	%f8, %f22, %f23;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB255_10;

	shr.u32 	%r40, %r1, 9;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB255_10:
	ret;
}

	// .globl	block_reduce_mul_f16_256
.visible .entry block_reduce_mul_f16_256(
	.param .u64 block_reduce_mul_f16_256_param_0,
	.param .u64 block_reduce_mul_f16_256_param_1,
	.param .u32 block_reduce_mul_f16_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_256_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f16_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB256_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f22, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f22;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB256_3;

	ld.shared.f32 	%f8, [%r3+512];
	mul.f32 	%f22, %f22, %f8;
	st.shared.f32 	[%r3], %f22;

BB256_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB256_5;

	ld.shared.f32 	%f9, [%r3+256];
	mul.f32 	%f22, %f22, %f9;
	st.shared.f32 	[%r3], %f22;

BB256_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB256_8;

	ld.shared.f32 	%f10, [%r3+128];
	mul.f32 	%f11, %f22, %f10;
	mov.b32 	 %r10, %f11;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f12, %r14;
	mul.f32 	%f13, %f11, %f12;
	mov.b32 	 %r15, %f13;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f14, %r17;
	mul.f32 	%f15, %f13, %f14;
	mov.b32 	 %r18, %f15;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f16, %r20;
	mul.f32 	%f17, %f15, %f16;
	mov.b32 	 %r21, %f17;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f18, %r23;
	mul.f32 	%f19, %f17, %f18;
	mov.b32 	 %r24, %f19;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f20, %r26;
	mul.f32 	%f6, %f19, %f20;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB256_8;

	shr.u32 	%r31, %r2, 8;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB256_8:
	ret;
}

	// .globl	block_reduce_mul_f16_128
.visible .entry block_reduce_mul_f16_128(
	.param .u64 block_reduce_mul_f16_128_param_0,
	.param .u64 block_reduce_mul_f16_128_param_1,
	.param .u32 block_reduce_mul_f16_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_128_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f16_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB257_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f19, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f19;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB257_3;

	ld.shared.f32 	%f6, [%r3+256];
	mul.f32 	%f19, %f19, %f6;
	st.shared.f32 	[%r3], %f19;

BB257_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB257_6;

	ld.shared.f32 	%f7, [%r3+128];
	mul.f32 	%f8, %f19, %f7;
	mov.b32 	 %r10, %f8;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f9, %r14;
	mul.f32 	%f10, %f8, %f9;
	mov.b32 	 %r15, %f10;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f11, %r17;
	mul.f32 	%f12, %f10, %f11;
	mov.b32 	 %r18, %f12;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f13, %r20;
	mul.f32 	%f14, %f12, %f13;
	mov.b32 	 %r21, %f14;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f15, %r23;
	mul.f32 	%f16, %f14, %f15;
	mov.b32 	 %r24, %f16;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f17, %r26;
	mul.f32 	%f4, %f16, %f17;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB257_6;

	shr.u32 	%r27, %r2, 7;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB257_6:
	ret;
}

	// .globl	block_reduce_mul_f16_64
.visible .entry block_reduce_mul_f16_64(
	.param .u64 block_reduce_mul_f16_64_param_0,
	.param .u64 block_reduce_mul_f16_64_param_1,
	.param .u32 block_reduce_mul_f16_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<16>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_64_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f16_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB258_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f3, %rs1;}

	// inline asm
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f3;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB258_4;

	ld.shared.f32 	%f4, [%r3+128];
	mul.f32 	%f5, %f3, %f4;
	mov.b32 	 %r10, %f5;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f6, %r14;
	mul.f32 	%f7, %f5, %f6;
	mov.b32 	 %r15, %f7;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f8, %r17;
	mul.f32 	%f9, %f7, %f8;
	mov.b32 	 %r18, %f9;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f10, %r20;
	mul.f32 	%f11, %f9, %f10;
	mov.b32 	 %r21, %f11;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f12, %r23;
	mul.f32 	%f13, %f11, %f12;
	mov.b32 	 %r24, %f13;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f14, %r26;
	mul.f32 	%f2, %f13, %f14;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB258_4;

	shr.u32 	%r27, %r2, 6;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB258_4:
	ret;
}

	// .globl	block_reduce_mul_f16_32
.visible .entry block_reduce_mul_f16_32(
	.param .u64 block_reduce_mul_f16_32_param_0,
	.param .u64 block_reduce_mul_f16_32_param_1,
	.param .u32 block_reduce_mul_f16_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_32_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f16_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB259_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	mul.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	mul.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB259_3;

	shr.u32 	%r24, %r2, 5;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB259_3:
	ret;
}

	// .globl	block_reduce_mul_f16_16
.visible .entry block_reduce_mul_f16_16(
	.param .u64 block_reduce_mul_f16_16_param_0,
	.param .u64 block_reduce_mul_f16_16_param_1,
	.param .u32 block_reduce_mul_f16_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_16_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f16_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB260_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	mul.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB260_3;

	shr.u32 	%r21, %r2, 4;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB260_3:
	ret;
}

	// .globl	block_reduce_mul_f16_8
.visible .entry block_reduce_mul_f16_8(
	.param .u64 block_reduce_mul_f16_8_param_0,
	.param .u64 block_reduce_mul_f16_8_param_1,
	.param .u32 block_reduce_mul_f16_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_8_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f16_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB261_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB261_3;

	shr.u32 	%r18, %r2, 3;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB261_3:
	ret;
}

	// .globl	block_reduce_mul_f16_4
.visible .entry block_reduce_mul_f16_4(
	.param .u64 block_reduce_mul_f16_4_param_0,
	.param .u64 block_reduce_mul_f16_4_param_1,
	.param .u32 block_reduce_mul_f16_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_4_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f16_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB262_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB262_3;

	shr.u32 	%r15, %r2, 2;
	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r15, 2;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB262_3:
	ret;
}

	// .globl	block_reduce_mul_f16_2
.visible .entry block_reduce_mul_f16_2(
	.param .u64 block_reduce_mul_f16_2_param_0,
	.param .u64 block_reduce_mul_f16_2_param_1,
	.param .u32 block_reduce_mul_f16_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f16_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f16_2_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f16_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB263_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// inline asm
	{  cvt.f32.f16 %f2, %rs1;}

	// inline asm
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB263_3;

	// inline asm
	{  cvt.rn.f16.f32 %rs2, %f1;}

	// inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r12, %r2, -2;
	cvt.u64.u32	%rd7, %r12;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u16 	[%rd8], %rs2;

BB263_3:
	ret;
}

	// .globl	block_reduce_mul_f32_1024
.visible .entry block_reduce_mul_f32_1024(
	.param .u64 block_reduce_mul_f32_1024_param_0,
	.param .u64 block_reduce_mul_f32_1024_param_1,
	.param .u32 block_reduce_mul_f32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<65>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_1024_param_1];
	ld.param.u32 	%r2, [block_reduce_mul_f32_1024_param_2];
	mov.u32 	%r3, %tid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	setp.ge.u32	%p1, %r6, %r2;
	@%p1 bra 	BB264_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r3, 1023;
	mul.wide.u32 	%rd4, %r6, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f27, [%rd5];
	shl.b32 	%r11, %r3, 2;
	mov.u32 	%r12, shared;
	add.s32 	%r13, %r12, %r11;
	st.shared.f32 	[%r13], %f27;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB264_3;

	ld.shared.f32 	%f11, [%r13+2048];
	mul.f32 	%f27, %f27, %f11;
	st.shared.f32 	[%r13], %f27;

BB264_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB264_5;

	ld.shared.f32 	%f12, [%r13+1024];
	mul.f32 	%f27, %f27, %f12;
	st.shared.f32 	[%r13], %f27;

BB264_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB264_7;

	ld.shared.f32 	%f13, [%r13+512];
	mul.f32 	%f27, %f27, %f13;
	st.shared.f32 	[%r13], %f27;

BB264_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB264_9;

	ld.shared.f32 	%f14, [%r13+256];
	mul.f32 	%f27, %f27, %f14;
	st.shared.f32 	[%r13], %f27;

BB264_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB264_12;

	ld.shared.f32 	%f15, [%r13+128];
	mul.f32 	%f16, %f27, %f15;
	mov.b32 	 %r42, %f16;
	mov.u32 	%r43, 31;
	mov.u32 	%r44, 1;
	mov.u32 	%r45, -1;
	shfl.sync.bfly.b32 	%r46|%p7, %r42, %r44, %r43, %r45;
	mov.b32 	 %f17, %r46;
	mul.f32 	%f18, %f16, %f17;
	mov.b32 	 %r47, %f18;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r43, %r45;
	mov.b32 	 %f19, %r49;
	mul.f32 	%f20, %f18, %f19;
	mov.b32 	 %r50, %f20;
	mov.u32 	%r51, 4;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r43, %r45;
	mov.b32 	 %f21, %r52;
	mul.f32 	%f22, %f20, %f21;
	mov.b32 	 %r53, %f22;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r43, %r45;
	mov.b32 	 %f23, %r55;
	mul.f32 	%f24, %f22, %f23;
	mov.b32 	 %r56, %f24;
	mov.u32 	%r57, 16;
	shfl.sync.bfly.b32 	%r58|%p11, %r56, %r57, %r43, %r45;
	mov.b32 	 %f25, %r58;
	mul.f32 	%f10, %f24, %f25;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB264_12;

	shr.u32 	%r64, %r6, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r64, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f10;

BB264_12:
	ret;
}

	// .globl	block_reduce_mul_f32_512
.visible .entry block_reduce_mul_f32_512(
	.param .u64 block_reduce_mul_f32_512_param_0,
	.param .u64 block_reduce_mul_f32_512_param_1,
	.param .u32 block_reduce_mul_f32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<26>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_512_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_f32_512_param_2];
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r1, %r6, %r7, %r5;
	setp.ge.u32	%p1, %r1, %r4;
	@%p1 bra 	BB265_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r5, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f24, [%rd5];
	shl.b32 	%r9, %r5, 2;
	mov.u32 	%r10, shared;
	add.s32 	%r3, %r10, %r9;
	st.shared.f32 	[%r3], %f24;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB265_3;

	ld.shared.f32 	%f9, [%r3+1024];
	mul.f32 	%f24, %f24, %f9;
	st.shared.f32 	[%r3], %f24;

BB265_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB265_5;

	ld.shared.f32 	%f10, [%r3+512];
	mul.f32 	%f24, %f24, %f10;
	st.shared.f32 	[%r3], %f24;

BB265_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB265_7;

	ld.shared.f32 	%f11, [%r3+256];
	mul.f32 	%f24, %f24, %f11;
	st.shared.f32 	[%r3], %f24;

BB265_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB265_10;

	ld.shared.f32 	%f12, [%r3+128];
	mul.f32 	%f13, %f24, %f12;
	mov.b32 	 %r17, %f13;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	 %f14, %r21;
	mul.f32 	%f15, %f13, %f14;
	mov.b32 	 %r22, %f15;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	 %f16, %r24;
	mul.f32 	%f17, %f15, %f16;
	mov.b32 	 %r25, %f17;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	 %f18, %r27;
	mul.f32 	%f19, %f17, %f18;
	mov.b32 	 %r28, %f19;
	mov.u32 	%r29, 8;
	shfl.sync.bfly.b32 	%r30|%p9, %r28, %r29, %r18, %r20;
	mov.b32 	 %f20, %r30;
	mul.f32 	%f21, %f19, %f20;
	mov.b32 	 %r31, %f21;
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r33|%p10, %r31, %r32, %r18, %r20;
	mov.b32 	 %f22, %r33;
	mul.f32 	%f8, %f21, %f22;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB265_10;

	shr.u32 	%r40, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f8;

BB265_10:
	ret;
}

	// .globl	block_reduce_mul_f32_256
.visible .entry block_reduce_mul_f32_256(
	.param .u64 block_reduce_mul_f32_256_param_0,
	.param .u64 block_reduce_mul_f32_256_param_1,
	.param .u32 block_reduce_mul_f32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .f32 	%f<22>;
	.reg .b32 	%r<32>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_256_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB266_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f20, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB266_3;

	ld.shared.f32 	%f7, [%r3+512];
	mul.f32 	%f20, %f20, %f7;
	st.shared.f32 	[%r3], %f20;

BB266_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB266_5;

	ld.shared.f32 	%f8, [%r3+256];
	mul.f32 	%f20, %f20, %f8;
	st.shared.f32 	[%r3], %f20;

BB266_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB266_8;

	ld.shared.f32 	%f9, [%r3+128];
	mul.f32 	%f10, %f20, %f9;
	mov.b32 	 %r10, %f10;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p5, %r10, %r12, %r11, %r13;
	mov.b32 	 %f11, %r14;
	mul.f32 	%f12, %f10, %f11;
	mov.b32 	 %r15, %f12;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r16, %r11, %r13;
	mov.b32 	 %f13, %r17;
	mul.f32 	%f14, %f12, %f13;
	mov.b32 	 %r18, %f14;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r11, %r13;
	mov.b32 	 %f15, %r20;
	mul.f32 	%f16, %f14, %f15;
	mov.b32 	 %r21, %f16;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r11, %r13;
	mov.b32 	 %f17, %r23;
	mul.f32 	%f18, %f16, %f17;
	mov.b32 	 %r24, %f18;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p9, %r24, %r25, %r11, %r13;
	mov.b32 	 %f19, %r26;
	mul.f32 	%f6, %f18, %f19;
	setp.ne.s32	%p10, %r4, 0;
	@%p10 bra 	BB266_8;

	shr.u32 	%r31, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r31, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB266_8:
	ret;
}

	// .globl	block_reduce_mul_f32_128
.visible .entry block_reduce_mul_f32_128(
	.param .u64 block_reduce_mul_f32_128_param_0,
	.param .u64 block_reduce_mul_f32_128_param_1,
	.param .u32 block_reduce_mul_f32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_128_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB267_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f17, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB267_3;

	ld.shared.f32 	%f5, [%r3+256];
	mul.f32 	%f17, %f17, %f5;
	st.shared.f32 	[%r3], %f17;

BB267_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB267_6;

	ld.shared.f32 	%f6, [%r3+128];
	mul.f32 	%f7, %f17, %f6;
	mov.b32 	 %r10, %f7;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p4, %r10, %r12, %r11, %r13;
	mov.b32 	 %f8, %r14;
	mul.f32 	%f9, %f7, %f8;
	mov.b32 	 %r15, %f9;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r16, %r11, %r13;
	mov.b32 	 %f10, %r17;
	mul.f32 	%f11, %f9, %f10;
	mov.b32 	 %r18, %f11;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p6, %r18, %r19, %r11, %r13;
	mov.b32 	 %f12, %r20;
	mul.f32 	%f13, %f11, %f12;
	mov.b32 	 %r21, %f13;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p7, %r21, %r22, %r11, %r13;
	mov.b32 	 %f14, %r23;
	mul.f32 	%f15, %f13, %f14;
	mov.b32 	 %r24, %f15;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p8, %r24, %r25, %r11, %r13;
	mov.b32 	 %f16, %r26;
	mul.f32 	%f4, %f15, %f16;
	setp.ne.s32	%p9, %r4, 0;
	@%p9 bra 	BB267_6;

	shr.u32 	%r27, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB267_6:
	ret;
}

	// .globl	block_reduce_mul_f32_64
.visible .entry block_reduce_mul_f32_64(
	.param .u64 block_reduce_mul_f32_64_param_0,
	.param .u64 block_reduce_mul_f32_64_param_1,
	.param .u32 block_reduce_mul_f32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<14>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_64_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB268_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.f32 	[%r3], %f1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB268_4;

	ld.shared.f32 	%f3, [%r3+128];
	mul.f32 	%f4, %f1, %f3;
	mov.b32 	 %r10, %f4;
	mov.u32 	%r11, 31;
	mov.u32 	%r12, 1;
	mov.u32 	%r13, -1;
	shfl.sync.bfly.b32 	%r14|%p3, %r10, %r12, %r11, %r13;
	mov.b32 	 %f5, %r14;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r15, %f6;
	mov.u32 	%r16, 2;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r11, %r13;
	mov.b32 	 %f7, %r17;
	mul.f32 	%f8, %f6, %f7;
	mov.b32 	 %r18, %f8;
	mov.u32 	%r19, 4;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r11, %r13;
	mov.b32 	 %f9, %r20;
	mul.f32 	%f10, %f8, %f9;
	mov.b32 	 %r21, %f10;
	mov.u32 	%r22, 8;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r11, %r13;
	mov.b32 	 %f11, %r23;
	mul.f32 	%f12, %f10, %f11;
	mov.b32 	 %r24, %f12;
	mov.u32 	%r25, 16;
	shfl.sync.bfly.b32 	%r26|%p7, %r24, %r25, %r11, %r13;
	mov.b32 	 %f13, %r26;
	mul.f32 	%f2, %f12, %f13;
	setp.ne.s32	%p8, %r4, 0;
	@%p8 bra 	BB268_4;

	shr.u32 	%r27, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f2;

BB268_4:
	ret;
}

	// .globl	block_reduce_mul_f32_32
.visible .entry block_reduce_mul_f32_32(
	.param .u64 block_reduce_mul_f32_32_param_0,
	.param .u64 block_reduce_mul_f32_32_param_1,
	.param .u32 block_reduce_mul_f32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<12>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_32_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f32_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB269_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	mul.f32 	%f10, %f8, %f9;
	mov.b32 	 %r20, %f10;
	mov.u32 	%r21, 16;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r7, %r9;
	mov.b32 	 %f11, %r22;
	mul.f32 	%f1, %f10, %f11;
	and.b32  	%r23, %r1, 31;
	setp.ne.s32	%p7, %r23, 0;
	@%p7 bra 	BB269_3;

	shr.u32 	%r24, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB269_3:
	ret;
}

	// .globl	block_reduce_mul_f32_16
.visible .entry block_reduce_mul_f32_16(
	.param .u64 block_reduce_mul_f32_16_param_0,
	.param .u64 block_reduce_mul_f32_16_param_1,
	.param .u32 block_reduce_mul_f32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_16_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f32_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB270_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 4127;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f8, %f6, %f7;
	mov.b32 	 %r17, %f8;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r7, %r9;
	mov.b32 	 %f9, %r19;
	mul.f32 	%f1, %f8, %f9;
	and.b32  	%r20, %r1, 15;
	setp.ne.s32	%p6, %r20, 0;
	@%p6 bra 	BB270_3;

	shr.u32 	%r21, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB270_3:
	ret;
}

	// .globl	block_reduce_mul_f32_8
.visible .entry block_reduce_mul_f32_8(
	.param .u64 block_reduce_mul_f32_8_param_0,
	.param .u64 block_reduce_mul_f32_8_param_1,
	.param .u32 block_reduce_mul_f32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_8_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f32_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB271_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 6175;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f6, %f4, %f5;
	mov.b32 	 %r14, %f6;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r7, %r9;
	mov.b32 	 %f7, %r16;
	mul.f32 	%f1, %f6, %f7;
	and.b32  	%r17, %r1, 7;
	setp.ne.s32	%p5, %r17, 0;
	@%p5 bra 	BB271_3;

	shr.u32 	%r18, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB271_3:
	ret;
}

	// .globl	block_reduce_mul_f32_4
.visible .entry block_reduce_mul_f32_4(
	.param .u64 block_reduce_mul_f32_4_param_0,
	.param .u64 block_reduce_mul_f32_4_param_1,
	.param .u32 block_reduce_mul_f32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<6>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_4_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f32_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB272_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7199;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f4, %f2, %f3;
	mov.b32 	 %r11, %f4;
	mov.u32 	%r12, 2;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r12, %r7, %r9;
	mov.b32 	 %f5, %r13;
	mul.f32 	%f1, %f4, %f5;
	and.b32  	%r14, %r1, 3;
	setp.ne.s32	%p4, %r14, 0;
	@%p4 bra 	BB272_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r15, %r2, -4;
	cvt.u64.u32	%rd7, %r15;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB272_3:
	ret;
}

	// .globl	block_reduce_mul_f32_2
.visible .entry block_reduce_mul_f32_2(
	.param .u64 block_reduce_mul_f32_2_param_0,
	.param .u64 block_reduce_mul_f32_2_param_1,
	.param .u32 block_reduce_mul_f32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f32_2_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f32_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB273_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	mov.b32 	 %r6, %f2;
	mov.u32 	%r7, 7711;
	mov.u32 	%r8, 1;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p2, %r6, %r8, %r7, %r9;
	mov.b32 	 %f3, %r10;
	mul.f32 	%f1, %f2, %f3;
	and.b32  	%r11, %r1, 1;
	setp.eq.b32	%p3, %r11, 1;
	@%p3 bra 	BB273_3;

	shr.u32 	%r12, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

BB273_3:
	ret;
}

	// .globl	block_reduce_mul_f64_1024
.visible .entry block_reduce_mul_f64_1024(
	.param .u64 block_reduce_mul_f64_1024_param_0,
	.param .u64 block_reduce_mul_f64_1024_param_1,
	.param .u32 block_reduce_mul_f64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB274_12;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd27, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd27;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB274_3;

	ld.shared.f64 	%fd11, [%r3+4096];
	mul.f64 	%fd27, %fd27, %fd11;
	st.shared.f64 	[%r3], %fd27;

BB274_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB274_5;

	ld.shared.f64 	%fd12, [%r3+2048];
	mul.f64 	%fd27, %fd27, %fd12;
	st.shared.f64 	[%r3], %fd27;

BB274_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB274_7;

	ld.shared.f64 	%fd13, [%r3+1024];
	mul.f64 	%fd27, %fd27, %fd13;
	st.shared.f64 	[%r3], %fd27;

BB274_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB274_9;

	ld.shared.f64 	%fd14, [%r3+512];
	mul.f64 	%fd27, %fd27, %fd14;
	st.shared.f64 	[%r3], %fd27;

BB274_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB274_12;

	ld.shared.f64 	%fd25, [%r3+256];
	mul.f64 	%fd15, %fd27, %fd25;
	// inline asm
	mov.b64 {%r10,%r11}, %fd15;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r12,%r13};
	// inline asm
	mul.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r14,%r15}, %fd17;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r16,%r17};
	// inline asm
	mul.f64 	%fd19, %fd17, %fd18;
	// inline asm
	mov.b64 {%r18,%r19}, %fd19;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd20, {%r20,%r21};
	// inline asm
	mul.f64 	%fd21, %fd19, %fd20;
	// inline asm
	mov.b64 {%r22,%r23}, %fd21;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd22, {%r24,%r25};
	// inline asm
	mul.f64 	%fd23, %fd21, %fd22;
	// inline asm
	mov.b64 {%r26,%r27}, %fd23;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd24, {%r28,%r29};
	// inline asm
	mul.f64 	%fd10, %fd23, %fd24;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB274_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd10;

BB274_12:
	ret;
}

	// .globl	block_reduce_mul_f64_512
.visible .entry block_reduce_mul_f64_512(
	.param .u64 block_reduce_mul_f64_512_param_0,
	.param .u64 block_reduce_mul_f64_512_param_1,
	.param .u32 block_reduce_mul_f64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB275_10;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd24, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd24;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB275_3;

	ld.shared.f64 	%fd9, [%r3+2048];
	mul.f64 	%fd24, %fd24, %fd9;
	st.shared.f64 	[%r3], %fd24;

BB275_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB275_5;

	ld.shared.f64 	%fd10, [%r3+1024];
	mul.f64 	%fd24, %fd24, %fd10;
	st.shared.f64 	[%r3], %fd24;

BB275_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB275_7;

	ld.shared.f64 	%fd11, [%r3+512];
	mul.f64 	%fd24, %fd24, %fd11;
	st.shared.f64 	[%r3], %fd24;

BB275_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB275_10;

	ld.shared.f64 	%fd22, [%r3+256];
	mul.f64 	%fd12, %fd24, %fd22;
	// inline asm
	mov.b64 {%r10,%r11}, %fd12;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r12,%r13};
	// inline asm
	mul.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r14,%r15}, %fd14;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r16,%r17};
	// inline asm
	mul.f64 	%fd16, %fd14, %fd15;
	// inline asm
	mov.b64 {%r18,%r19}, %fd16;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd17, {%r20,%r21};
	// inline asm
	mul.f64 	%fd18, %fd16, %fd17;
	// inline asm
	mov.b64 {%r22,%r23}, %fd18;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd19, {%r24,%r25};
	// inline asm
	mul.f64 	%fd20, %fd18, %fd19;
	// inline asm
	mov.b64 {%r26,%r27}, %fd20;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd21, {%r28,%r29};
	// inline asm
	mul.f64 	%fd8, %fd20, %fd21;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB275_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd8;

BB275_10:
	ret;
}

	// .globl	block_reduce_mul_f64_256
.visible .entry block_reduce_mul_f64_256(
	.param .u64 block_reduce_mul_f64_256_param_0,
	.param .u64 block_reduce_mul_f64_256_param_1,
	.param .u32 block_reduce_mul_f64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<22>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB276_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd20, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd20;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB276_3;

	ld.shared.f64 	%fd7, [%r3+1024];
	mul.f64 	%fd20, %fd20, %fd7;
	st.shared.f64 	[%r3], %fd20;

BB276_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB276_5;

	ld.shared.f64 	%fd8, [%r3+512];
	mul.f64 	%fd20, %fd20, %fd8;
	st.shared.f64 	[%r3], %fd20;

BB276_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB276_8;

	ld.shared.f64 	%fd19, [%r3+256];
	mul.f64 	%fd9, %fd20, %fd19;
	// inline asm
	mov.b64 {%r10,%r11}, %fd9;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r12,%r13};
	// inline asm
	mul.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r14,%r15}, %fd11;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r16,%r17};
	// inline asm
	mul.f64 	%fd13, %fd11, %fd12;
	// inline asm
	mov.b64 {%r18,%r19}, %fd13;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd14, {%r20,%r21};
	// inline asm
	mul.f64 	%fd15, %fd13, %fd14;
	// inline asm
	mov.b64 {%r22,%r23}, %fd15;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd16, {%r24,%r25};
	// inline asm
	mul.f64 	%fd17, %fd15, %fd16;
	// inline asm
	mov.b64 {%r26,%r27}, %fd17;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd18, {%r28,%r29};
	// inline asm
	mul.f64 	%fd6, %fd17, %fd18;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB276_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd6;

BB276_8:
	ret;
}

	// .globl	block_reduce_mul_f64_128
.visible .entry block_reduce_mul_f64_128(
	.param .u64 block_reduce_mul_f64_128_param_0,
	.param .u64 block_reduce_mul_f64_128_param_1,
	.param .u32 block_reduce_mul_f64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<18>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB277_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd17;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB277_3;

	ld.shared.f64 	%fd5, [%r3+512];
	mul.f64 	%fd17, %fd17, %fd5;
	st.shared.f64 	[%r3], %fd17;

BB277_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB277_6;

	ld.shared.f64 	%fd16, [%r3+256];
	mul.f64 	%fd6, %fd17, %fd16;
	// inline asm
	mov.b64 {%r10,%r11}, %fd6;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd7, {%r12,%r13};
	// inline asm
	mul.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r14,%r15}, %fd8;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd9, {%r16,%r17};
	// inline asm
	mul.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r18,%r19}, %fd10;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd11, {%r20,%r21};
	// inline asm
	mul.f64 	%fd12, %fd10, %fd11;
	// inline asm
	mov.b64 {%r22,%r23}, %fd12;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd13, {%r24,%r25};
	// inline asm
	mul.f64 	%fd14, %fd12, %fd13;
	// inline asm
	mov.b64 {%r26,%r27}, %fd14;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd15, {%r28,%r29};
	// inline asm
	mul.f64 	%fd4, %fd14, %fd15;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB277_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd4;

BB277_6:
	ret;
}

	// .globl	block_reduce_mul_f64_64
.visible .entry block_reduce_mul_f64_64(
	.param .u64 block_reduce_mul_f64_64_param_0,
	.param .u64 block_reduce_mul_f64_64_param_1,
	.param .u32 block_reduce_mul_f64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_f64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB278_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared_d;
	add.s32 	%r3, %r9, %r8;
	st.shared.f64 	[%r3], %fd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB278_4;

	ld.shared.f64 	%fd13, [%r3+256];
	mul.f64 	%fd3, %fd1, %fd13;
	// inline asm
	mov.b64 {%r10,%r11}, %fd3;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %fd4, {%r12,%r13};
	// inline asm
	mul.f64 	%fd5, %fd3, %fd4;
	// inline asm
	mov.b64 {%r14,%r15}, %fd5;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %fd6, {%r16,%r17};
	// inline asm
	mul.f64 	%fd7, %fd5, %fd6;
	// inline asm
	mov.b64 {%r18,%r19}, %fd7;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %fd8, {%r20,%r21};
	// inline asm
	mul.f64 	%fd9, %fd7, %fd8;
	// inline asm
	mov.b64 {%r22,%r23}, %fd9;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %fd10, {%r24,%r25};
	// inline asm
	mul.f64 	%fd11, %fd9, %fd10;
	// inline asm
	mov.b64 {%r26,%r27}, %fd11;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %fd12, {%r28,%r29};
	// inline asm
	mul.f64 	%fd2, %fd11, %fd12;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB278_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r37, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd2;

BB278_4:
	ret;
}

	// .globl	block_reduce_mul_f64_32
.visible .entry block_reduce_mul_f64_32(
	.param .u64 block_reduce_mul_f64_32_param_0,
	.param .u64 block_reduce_mul_f64_32_param_1,
	.param .u32 block_reduce_mul_f64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB279_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	mul.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	mul.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	mul.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	mul.f64 	%fd10, %fd8, %fd9;
	// inline asm
	mov.b64 {%r22,%r23}, %fd10;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %fd11, {%r24,%r25};
	// inline asm
	mul.f64 	%fd1, %fd10, %fd11;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB279_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r34, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB279_3:
	ret;
}

	// .globl	block_reduce_mul_f64_16
.visible .entry block_reduce_mul_f64_16(
	.param .u64 block_reduce_mul_f64_16_param_0,
	.param .u64 block_reduce_mul_f64_16_param_1,
	.param .u32 block_reduce_mul_f64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB280_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	mul.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	mul.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	mul.f64 	%fd8, %fd6, %fd7;
	// inline asm
	mov.b64 {%r18,%r19}, %fd8;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %fd9, {%r20,%r21};
	// inline asm
	mul.f64 	%fd1, %fd8, %fd9;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB280_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r29, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB280_3:
	ret;
}

	// .globl	block_reduce_mul_f64_8
.visible .entry block_reduce_mul_f64_8(
	.param .u64 block_reduce_mul_f64_8_param_0,
	.param .u64 block_reduce_mul_f64_8_param_1,
	.param .u32 block_reduce_mul_f64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB281_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	mul.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	mul.f64 	%fd6, %fd4, %fd5;
	// inline asm
	mov.b64 {%r14,%r15}, %fd6;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %fd7, {%r16,%r17};
	// inline asm
	mul.f64 	%fd1, %fd6, %fd7;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB281_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd7, %r24;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB281_3:
	ret;
}

	// .globl	block_reduce_mul_f64_4
.visible .entry block_reduce_mul_f64_4(
	.param .u64 block_reduce_mul_f64_4_param_0,
	.param .u64 block_reduce_mul_f64_4_param_1,
	.param .u32 block_reduce_mul_f64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB282_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	mul.f64 	%fd4, %fd2, %fd3;
	// inline asm
	mov.b64 {%r10,%r11}, %fd4;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %fd5, {%r12,%r13};
	// inline asm
	mul.f64 	%fd1, %fd4, %fd5;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB282_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB282_3:
	ret;
}

	// .globl	block_reduce_mul_f64_2
.visible .entry block_reduce_mul_f64_2(
	.param .u64 block_reduce_mul_f64_2_param_0,
	.param .u64 block_reduce_mul_f64_2_param_1,
	.param .u32 block_reduce_mul_f64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_f64_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_f64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_f64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB283_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd2, [%rd5];
	// inline asm
	mov.b64 {%r6,%r7}, %fd2;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %fd3, {%r8,%r9};
	// inline asm
	mul.f64 	%fd1, %fd2, %fd3;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB283_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r14, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

BB283_3:
	ret;
}

	// .globl	block_reduce_mul_u32_1024
.visible .entry block_reduce_mul_u32_1024(
	.param .u64 block_reduce_mul_u32_1024_param_0,
	.param .u64 block_reduce_mul_u32_1024_param_1,
	.param .u32 block_reduce_mul_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_mul_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB284_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB284_3;

	ld.shared.u32 	%r28, [%r23+2048];
	mul.lo.s32 	%r81, %r28, %r81;
	st.shared.u32 	[%r23], %r81;

BB284_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB284_5;

	ld.shared.u32 	%r35, [%r23+1024];
	mul.lo.s32 	%r81, %r35, %r81;
	st.shared.u32 	[%r23], %r81;

BB284_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB284_7;

	ld.shared.u32 	%r42, [%r23+512];
	mul.lo.s32 	%r81, %r42, %r81;
	st.shared.u32 	[%r23], %r81;

BB284_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB284_9;

	ld.shared.u32 	%r49, [%r23+256];
	mul.lo.s32 	%r81, %r49, %r81;
	st.shared.u32 	[%r23], %r81;

BB284_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB284_12;

	ld.shared.u32 	%r56, [%r23+128];
	mul.lo.s32 	%r57, %r56, %r81;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	mul.lo.s32 	%r62, %r61, %r57;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	mul.lo.s32 	%r65, %r64, %r62;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	mul.lo.s32 	%r68, %r67, %r65;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	mul.lo.s32 	%r71, %r70, %r68;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	mul.lo.s32 	%r11, %r73, %r71;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB284_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB284_12:
	ret;
}

	// .globl	block_reduce_mul_u32_512
.visible .entry block_reduce_mul_u32_512(
	.param .u64 block_reduce_mul_u32_512_param_0,
	.param .u64 block_reduce_mul_u32_512_param_1,
	.param .u32 block_reduce_mul_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_mul_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB285_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB285_3;

	ld.shared.u32 	%r19, [%r4+1024];
	mul.lo.s32 	%r54, %r19, %r54;
	st.shared.u32 	[%r4], %r54;

BB285_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB285_5;

	ld.shared.u32 	%r22, [%r4+512];
	mul.lo.s32 	%r54, %r22, %r54;
	st.shared.u32 	[%r4], %r54;

BB285_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB285_7;

	ld.shared.u32 	%r25, [%r4+256];
	mul.lo.s32 	%r54, %r25, %r54;
	st.shared.u32 	[%r4], %r54;

BB285_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB285_10;

	ld.shared.u32 	%r28, [%r4+128];
	mul.lo.s32 	%r29, %r28, %r54;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	mul.lo.s32 	%r34, %r33, %r29;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	mul.lo.s32 	%r37, %r36, %r34;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	mul.lo.s32 	%r40, %r39, %r37;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	mul.lo.s32 	%r43, %r42, %r40;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	mul.lo.s32 	%r11, %r45, %r43;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB285_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB285_10:
	ret;
}

	// .globl	block_reduce_mul_u32_256
.visible .entry block_reduce_mul_u32_256(
	.param .u64 block_reduce_mul_u32_256_param_0,
	.param .u64 block_reduce_mul_u32_256_param_1,
	.param .u32 block_reduce_mul_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_mul_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB286_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB286_3;

	ld.shared.u32 	%r16, [%r4+512];
	mul.lo.s32 	%r41, %r16, %r41;
	st.shared.u32 	[%r4], %r41;

BB286_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB286_5;

	ld.shared.u32 	%r17, [%r4+256];
	mul.lo.s32 	%r41, %r17, %r41;
	st.shared.u32 	[%r4], %r41;

BB286_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB286_8;

	ld.shared.u32 	%r18, [%r4+128];
	mul.lo.s32 	%r19, %r18, %r41;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	mul.lo.s32 	%r24, %r23, %r19;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	mul.lo.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	mul.lo.s32 	%r33, %r32, %r30;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	mul.lo.s32 	%r10, %r35, %r33;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB286_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB286_8:
	ret;
}

	// .globl	block_reduce_mul_u32_128
.visible .entry block_reduce_mul_u32_128(
	.param .u64 block_reduce_mul_u32_128_param_0,
	.param .u64 block_reduce_mul_u32_128_param_1,
	.param .u32 block_reduce_mul_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_mul_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB287_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB287_3;

	ld.shared.u32 	%r14, [%r4+256];
	mul.lo.s32 	%r34, %r14, %r34;
	st.shared.u32 	[%r4], %r34;

BB287_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB287_6;

	ld.shared.u32 	%r15, [%r4+128];
	mul.lo.s32 	%r16, %r15, %r34;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	mul.lo.s32 	%r21, %r20, %r16;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	mul.lo.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	mul.lo.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	mul.lo.s32 	%r8, %r32, %r30;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB287_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB287_6:
	ret;
}

	// .globl	block_reduce_mul_u32_64
.visible .entry block_reduce_mul_u32_64(
	.param .u64 block_reduce_mul_u32_64_param_0,
	.param .u64 block_reduce_mul_u32_64_param_1,
	.param .u32 block_reduce_mul_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_mul_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB288_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB288_4;

	ld.shared.u32 	%r12, [%r4+128];
	mul.lo.s32 	%r13, %r12, %r3;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mul.lo.s32 	%r18, %r17, %r13;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mul.lo.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mul.lo.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	mul.lo.s32 	%r6, %r29, %r27;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB288_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB288_4:
	ret;
}

	// .globl	block_reduce_mul_u32_32
.visible .entry block_reduce_mul_u32_32(
	.param .u64 block_reduce_mul_u32_32_param_0,
	.param .u64 block_reduce_mul_u32_32_param_1,
	.param .u32 block_reduce_mul_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB289_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	mul.lo.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	mul.lo.s32 	%r3, %r23, %r21;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB289_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB289_3:
	ret;
}

	// .globl	block_reduce_mul_u32_16
.visible .entry block_reduce_mul_u32_16(
	.param .u64 block_reduce_mul_u32_16_param_0,
	.param .u64 block_reduce_mul_u32_16_param_1,
	.param .u32 block_reduce_mul_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB290_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	mul.lo.s32 	%r3, %r20, %r18;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB290_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB290_3:
	ret;
}

	// .globl	block_reduce_mul_u32_8
.visible .entry block_reduce_mul_u32_8(
	.param .u64 block_reduce_mul_u32_8_param_0,
	.param .u64 block_reduce_mul_u32_8_param_1,
	.param .u32 block_reduce_mul_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB291_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r3, %r17, %r15;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB291_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB291_3:
	ret;
}

	// .globl	block_reduce_mul_u32_4
.visible .entry block_reduce_mul_u32_4(
	.param .u64 block_reduce_mul_u32_4_param_0,
	.param .u64 block_reduce_mul_u32_4_param_1,
	.param .u32 block_reduce_mul_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB292_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r3, %r14, %r12;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB292_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB292_3:
	ret;
}

	// .globl	block_reduce_mul_u32_2
.visible .entry block_reduce_mul_u32_2(
	.param .u64 block_reduce_mul_u32_2_param_0,
	.param .u64 block_reduce_mul_u32_2_param_1,
	.param .u32 block_reduce_mul_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB293_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r3, %r11, %r7;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB293_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB293_3:
	ret;
}

	// .globl	block_reduce_mul_u64_1024
.visible .entry block_reduce_mul_u64_1024(
	.param .u64 block_reduce_mul_u64_1024_param_0,
	.param .u64 block_reduce_mul_u64_1024_param_1,
	.param .u32 block_reduce_mul_u64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_mul_u64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_mul_u64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_u64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB294_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u32 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB294_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	mul.lo.s64 	%rd35, %rd16, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB294_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB294_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	mul.lo.s64 	%rd35, %rd17, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB294_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB294_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	mul.lo.s64 	%rd35, %rd18, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB294_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB294_9;

	ld.shared.u64 	%rd19, [%r3+512];
	mul.lo.s64 	%rd35, %rd19, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB294_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB294_12;

	ld.shared.u64 	%rd30, [%r3+256];
	mul.lo.s64 	%rd20, %rd30, %rd35;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd24, %rd23, %rd22;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd26, %rd25, %rd24;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd28, %rd27, %rd26;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd10, %rd29, %rd28;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB294_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 4;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u32 	[%rd33], %rd10;

BB294_12:
	ret;
}

	// .globl	block_reduce_mul_u64_512
.visible .entry block_reduce_mul_u64_512(
	.param .u64 block_reduce_mul_u64_512_param_0,
	.param .u64 block_reduce_mul_u64_512_param_1,
	.param .u32 block_reduce_mul_u64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_mul_u64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_mul_u64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_u64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB295_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB295_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	mul.lo.s64 	%rd32, %rd14, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB295_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB295_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	mul.lo.s64 	%rd32, %rd15, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB295_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB295_7;

	ld.shared.u64 	%rd16, [%r3+512];
	mul.lo.s64 	%rd32, %rd16, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB295_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB295_10;

	ld.shared.u64 	%rd27, [%r3+256];
	mul.lo.s64 	%rd17, %rd27, %rd32;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd21, %rd20, %rd19;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd23, %rd22, %rd21;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd25, %rd24, %rd23;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd8, %rd26, %rd25;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB295_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 4;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u32 	[%rd30], %rd8;

BB295_10:
	ret;
}

	// .globl	block_reduce_mul_u64_256
.visible .entry block_reduce_mul_u64_256(
	.param .u64 block_reduce_mul_u64_256_param_0,
	.param .u64 block_reduce_mul_u64_256_param_1,
	.param .u32 block_reduce_mul_u64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_mul_u64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_mul_u64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_u64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB296_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB296_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	mul.lo.s64 	%rd28, %rd12, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB296_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB296_5;

	ld.shared.u64 	%rd13, [%r3+512];
	mul.lo.s64 	%rd28, %rd13, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB296_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB296_8;

	ld.shared.u64 	%rd24, [%r3+256];
	mul.lo.s64 	%rd14, %rd24, %rd28;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd18, %rd17, %rd16;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd20, %rd19, %rd18;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd6, %rd23, %rd22;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB296_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 4;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u32 	[%rd27], %rd6;

BB296_8:
	ret;
}

	// .globl	block_reduce_mul_u64_128
.visible .entry block_reduce_mul_u64_128(
	.param .u64 block_reduce_mul_u64_128_param_0,
	.param .u64 block_reduce_mul_u64_128_param_1,
	.param .u32 block_reduce_mul_u64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_mul_u64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_mul_u64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_u64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB297_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB297_3;

	ld.shared.u64 	%rd10, [%r3+512];
	mul.lo.s64 	%rd25, %rd10, %rd25;
	st.shared.u64 	[%r3], %rd25;

BB297_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB297_6;

	ld.shared.u64 	%rd21, [%r3+256];
	mul.lo.s64 	%rd11, %rd21, %rd25;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd13, %rd12, %rd11;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd15, %rd14, %rd13;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd17, %rd16, %rd15;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd4, %rd20, %rd19;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB297_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 4;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u32 	[%rd24], %rd4;

BB297_6:
	ret;
}

	// .globl	block_reduce_mul_u64_64
.visible .entry block_reduce_mul_u64_64(
	.param .u64 block_reduce_mul_u64_64_param_0,
	.param .u64 block_reduce_mul_u64_64_param_1,
	.param .u32 block_reduce_mul_u64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_mul_u64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_mul_u64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_u64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB298_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB298_4;

	ld.shared.u64 	%rd18, [%r3+256];
	mul.lo.s64 	%rd8, %rd18, %rd1;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd14, %rd13, %rd12;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd2, %rd17, %rd16;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB298_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 4;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u32 	[%rd21], %rd2;

BB298_4:
	ret;
}

	// .globl	block_reduce_mul_u64_32
.visible .entry block_reduce_mul_u64_32(
	.param .u64 block_reduce_mul_u64_32_param_0,
	.param .u64 block_reduce_mul_u64_32_param_1,
	.param .u32 block_reduce_mul_u64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_mul_u64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_u64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_u64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB299_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 4;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u32 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd1, %rd13, %rd12;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB299_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 4;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u32 	[%rd19], %rd1;

BB299_3:
	ret;
}

	// .globl	block_reduce_mul_u64_16
.visible .entry block_reduce_mul_u64_16(
	.param .u64 block_reduce_mul_u64_16_param_0,
	.param .u64 block_reduce_mul_u64_16_param_1,
	.param .u32 block_reduce_mul_u64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_mul_u64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_u64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_u64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB300_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u32 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd1, %rd11, %rd10;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB300_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 4;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u32 	[%rd17], %rd1;

BB300_3:
	ret;
}

	// .globl	block_reduce_mul_u64_8
.visible .entry block_reduce_mul_u64_8(
	.param .u64 block_reduce_mul_u64_8_param_0,
	.param .u64 block_reduce_mul_u64_8_param_1,
	.param .u32 block_reduce_mul_u64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_mul_u64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_u64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_u64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB301_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd1, %rd9, %rd8;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB301_3;

	shr.u32 	%r24, %r2, 3;
	cvta.to.global.u64 	%rd13, %rd3;
	mul.wide.u32 	%rd14, %r24, 4;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u32 	[%rd15], %rd1;

BB301_3:
	ret;
}

	// .globl	block_reduce_mul_u64_4
.visible .entry block_reduce_mul_u64_4(
	.param .u64 block_reduce_mul_u64_4_param_0,
	.param .u64 block_reduce_mul_u64_4_param_1,
	.param .u32 block_reduce_mul_u64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_mul_u64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_u64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_u64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB302_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd1, %rd7, %rd6;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB302_3;

	cvta.to.global.u64 	%rd11, %rd3;
	and.b32  	%r19, %r2, -4;
	cvt.u64.u32	%rd12, %r19;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u32 	[%rd13], %rd1;

BB302_3:
	ret;
}

	// .globl	block_reduce_mul_u64_2
.visible .entry block_reduce_mul_u64_2(
	.param .u64 block_reduce_mul_u64_2_param_0,
	.param .u64 block_reduce_mul_u64_2_param_1,
	.param .u32 block_reduce_mul_u64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_mul_u64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_u64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_u64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB303_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd1, %rd5, %rd4;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB303_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 4;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u32 	[%rd11], %rd1;

BB303_3:
	ret;
}

	// .globl	block_reduce_mul_i32_1024
.visible .entry block_reduce_mul_i32_1024(
	.param .u64 block_reduce_mul_i32_1024_param_0,
	.param .u64 block_reduce_mul_i32_1024_param_1,
	.param .u32 block_reduce_mul_i32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_mul_i32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB304_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB304_3;

	ld.shared.u32 	%r28, [%r23+2048];
	mul.lo.s32 	%r81, %r28, %r81;
	st.shared.u32 	[%r23], %r81;

BB304_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB304_5;

	ld.shared.u32 	%r35, [%r23+1024];
	mul.lo.s32 	%r81, %r35, %r81;
	st.shared.u32 	[%r23], %r81;

BB304_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB304_7;

	ld.shared.u32 	%r42, [%r23+512];
	mul.lo.s32 	%r81, %r42, %r81;
	st.shared.u32 	[%r23], %r81;

BB304_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB304_9;

	ld.shared.u32 	%r49, [%r23+256];
	mul.lo.s32 	%r81, %r49, %r81;
	st.shared.u32 	[%r23], %r81;

BB304_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB304_12;

	ld.shared.u32 	%r56, [%r23+128];
	mul.lo.s32 	%r57, %r56, %r81;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	mul.lo.s32 	%r62, %r61, %r57;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	mul.lo.s32 	%r65, %r64, %r62;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	mul.lo.s32 	%r68, %r67, %r65;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	mul.lo.s32 	%r71, %r70, %r68;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	mul.lo.s32 	%r11, %r73, %r71;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB304_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB304_12:
	ret;
}

	// .globl	block_reduce_mul_i32_512
.visible .entry block_reduce_mul_i32_512(
	.param .u64 block_reduce_mul_i32_512_param_0,
	.param .u64 block_reduce_mul_i32_512_param_1,
	.param .u32 block_reduce_mul_i32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_mul_i32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB305_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB305_3;

	ld.shared.u32 	%r19, [%r4+1024];
	mul.lo.s32 	%r54, %r19, %r54;
	st.shared.u32 	[%r4], %r54;

BB305_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB305_5;

	ld.shared.u32 	%r22, [%r4+512];
	mul.lo.s32 	%r54, %r22, %r54;
	st.shared.u32 	[%r4], %r54;

BB305_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB305_7;

	ld.shared.u32 	%r25, [%r4+256];
	mul.lo.s32 	%r54, %r25, %r54;
	st.shared.u32 	[%r4], %r54;

BB305_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB305_10;

	ld.shared.u32 	%r28, [%r4+128];
	mul.lo.s32 	%r29, %r28, %r54;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	mul.lo.s32 	%r34, %r33, %r29;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	mul.lo.s32 	%r37, %r36, %r34;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	mul.lo.s32 	%r40, %r39, %r37;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	mul.lo.s32 	%r43, %r42, %r40;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	mul.lo.s32 	%r11, %r45, %r43;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB305_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB305_10:
	ret;
}

	// .globl	block_reduce_mul_i32_256
.visible .entry block_reduce_mul_i32_256(
	.param .u64 block_reduce_mul_i32_256_param_0,
	.param .u64 block_reduce_mul_i32_256_param_1,
	.param .u32 block_reduce_mul_i32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_mul_i32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB306_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB306_3;

	ld.shared.u32 	%r16, [%r4+512];
	mul.lo.s32 	%r41, %r16, %r41;
	st.shared.u32 	[%r4], %r41;

BB306_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB306_5;

	ld.shared.u32 	%r17, [%r4+256];
	mul.lo.s32 	%r41, %r17, %r41;
	st.shared.u32 	[%r4], %r41;

BB306_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB306_8;

	ld.shared.u32 	%r18, [%r4+128];
	mul.lo.s32 	%r19, %r18, %r41;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	mul.lo.s32 	%r24, %r23, %r19;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	mul.lo.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	mul.lo.s32 	%r33, %r32, %r30;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	mul.lo.s32 	%r10, %r35, %r33;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB306_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB306_8:
	ret;
}

	// .globl	block_reduce_mul_i32_128
.visible .entry block_reduce_mul_i32_128(
	.param .u64 block_reduce_mul_i32_128_param_0,
	.param .u64 block_reduce_mul_i32_128_param_1,
	.param .u32 block_reduce_mul_i32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_mul_i32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB307_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB307_3;

	ld.shared.u32 	%r14, [%r4+256];
	mul.lo.s32 	%r34, %r14, %r34;
	st.shared.u32 	[%r4], %r34;

BB307_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB307_6;

	ld.shared.u32 	%r15, [%r4+128];
	mul.lo.s32 	%r16, %r15, %r34;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	mul.lo.s32 	%r21, %r20, %r16;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	mul.lo.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	mul.lo.s32 	%r30, %r29, %r27;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	mul.lo.s32 	%r8, %r32, %r30;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB307_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB307_6:
	ret;
}

	// .globl	block_reduce_mul_i32_64
.visible .entry block_reduce_mul_i32_64(
	.param .u64 block_reduce_mul_i32_64_param_0,
	.param .u64 block_reduce_mul_i32_64_param_1,
	.param .u32 block_reduce_mul_i32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_mul_i32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB308_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB308_4;

	ld.shared.u32 	%r12, [%r4+128];
	mul.lo.s32 	%r13, %r12, %r3;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mul.lo.s32 	%r18, %r17, %r13;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mul.lo.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mul.lo.s32 	%r24, %r23, %r21;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	mul.lo.s32 	%r27, %r26, %r24;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	mul.lo.s32 	%r6, %r29, %r27;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB308_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB308_4:
	ret;
}

	// .globl	block_reduce_mul_i32_32
.visible .entry block_reduce_mul_i32_32(
	.param .u64 block_reduce_mul_i32_32_param_0,
	.param .u64 block_reduce_mul_i32_32_param_1,
	.param .u32 block_reduce_mul_i32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_i32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB309_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	mul.lo.s32 	%r21, %r20, %r18;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	mul.lo.s32 	%r3, %r23, %r21;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB309_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB309_3:
	ret;
}

	// .globl	block_reduce_mul_i32_16
.visible .entry block_reduce_mul_i32_16(
	.param .u64 block_reduce_mul_i32_16_param_0,
	.param .u64 block_reduce_mul_i32_16_param_1,
	.param .u32 block_reduce_mul_i32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_i32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB310_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	mul.lo.s32 	%r3, %r20, %r18;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB310_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB310_3:
	ret;
}

	// .globl	block_reduce_mul_i32_8
.visible .entry block_reduce_mul_i32_8(
	.param .u64 block_reduce_mul_i32_8_param_0,
	.param .u64 block_reduce_mul_i32_8_param_1,
	.param .u32 block_reduce_mul_i32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_i32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB311_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	mul.lo.s32 	%r3, %r17, %r15;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB311_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB311_3:
	ret;
}

	// .globl	block_reduce_mul_i32_4
.visible .entry block_reduce_mul_i32_4(
	.param .u64 block_reduce_mul_i32_4_param_0,
	.param .u64 block_reduce_mul_i32_4_param_1,
	.param .u32 block_reduce_mul_i32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_i32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB312_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	mul.lo.s32 	%r3, %r14, %r12;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB312_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB312_3:
	ret;
}

	// .globl	block_reduce_mul_i32_2
.visible .entry block_reduce_mul_i32_2(
	.param .u64 block_reduce_mul_i32_2_param_0,
	.param .u64 block_reduce_mul_i32_2_param_1,
	.param .u32 block_reduce_mul_i32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_mul_i32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_mul_i32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_mul_i32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB313_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	mul.lo.s32 	%r3, %r11, %r7;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB313_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB313_3:
	ret;
}

	// .globl	block_reduce_mul_i64_1024
.visible .entry block_reduce_mul_i64_1024(
	.param .u64 block_reduce_mul_i64_1024_param_0,
	.param .u64 block_reduce_mul_i64_1024_param_1,
	.param .u32 block_reduce_mul_i64_1024_param_2
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd11, [block_reduce_mul_i64_1024_param_0];
	ld.param.u64 	%rd12, [block_reduce_mul_i64_1024_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_i64_1024_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB314_12;

	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.u32 	%rd14, %r2, 8;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.u64 	%rd35, [%rd15];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd35;
	bar.sync 	0;
	and.b32  	%r4, %r1, 1023;
	setp.gt.u32	%p2, %r4, 511;
	@%p2 bra 	BB314_3;

	ld.shared.u64 	%rd16, [%r3+4096];
	mul.lo.s64 	%rd35, %rd16, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB314_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 255;
	@%p3 bra 	BB314_5;

	ld.shared.u64 	%rd17, [%r3+2048];
	mul.lo.s64 	%rd35, %rd17, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB314_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB314_7;

	ld.shared.u64 	%rd18, [%r3+1024];
	mul.lo.s64 	%rd35, %rd18, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB314_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 63;
	@%p5 bra 	BB314_9;

	ld.shared.u64 	%rd19, [%r3+512];
	mul.lo.s64 	%rd35, %rd19, %rd35;
	st.shared.u64 	[%r3], %rd35;

BB314_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 31;
	@%p6 bra 	BB314_12;

	ld.shared.u64 	%rd30, [%r3+256];
	mul.lo.s64 	%rd20, %rd30, %rd35;
	// inline asm
	mov.b64 {%r10,%r11}, %rd20;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p7, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p8, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r14,%r15}, %rd22;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p10, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd24, %rd23, %rd22;
	// inline asm
	mov.b64 {%r18,%r19}, %rd24;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p11, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p12, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd25, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd26, %rd25, %rd24;
	// inline asm
	mov.b64 {%r22,%r23}, %rd26;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p13, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p14, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd27, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd28, %rd27, %rd26;
	// inline asm
	mov.b64 {%r26,%r27}, %rd28;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p15, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p16, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd29, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd10, %rd29, %rd28;
	setp.ne.s32	%p17, %r4, 0;
	@%p17 bra 	BB314_12;

	shr.u32 	%r37, %r2, 10;
	cvta.to.global.u64 	%rd31, %rd12;
	mul.wide.u32 	%rd32, %r37, 8;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.u64 	[%rd33], %rd10;

BB314_12:
	ret;
}

	// .globl	block_reduce_mul_i64_512
.visible .entry block_reduce_mul_i64_512(
	.param .u64 block_reduce_mul_i64_512_param_0,
	.param .u64 block_reduce_mul_i64_512_param_1,
	.param .u32 block_reduce_mul_i64_512_param_2
)
{
	.reg .pred 	%p<17>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd9, [block_reduce_mul_i64_512_param_0];
	ld.param.u64 	%rd10, [block_reduce_mul_i64_512_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_i64_512_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB315_10;

	cvta.to.global.u64 	%rd11, %rd9;
	mul.wide.u32 	%rd12, %r2, 8;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u64 	%rd32, [%rd13];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd32;
	bar.sync 	0;
	and.b32  	%r4, %r1, 511;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB315_3;

	ld.shared.u64 	%rd14, [%r3+2048];
	mul.lo.s64 	%rd32, %rd14, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB315_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 127;
	@%p3 bra 	BB315_5;

	ld.shared.u64 	%rd15, [%r3+1024];
	mul.lo.s64 	%rd32, %rd15, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB315_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 63;
	@%p4 bra 	BB315_7;

	ld.shared.u64 	%rd16, [%r3+512];
	mul.lo.s64 	%rd32, %rd16, %rd32;
	st.shared.u64 	[%r3], %rd32;

BB315_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r4, 31;
	@%p5 bra 	BB315_10;

	ld.shared.u64 	%rd27, [%r3+256];
	mul.lo.s64 	%rd17, %rd27, %rd32;
	// inline asm
	mov.b64 {%r10,%r11}, %rd17;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p7, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r14,%r15}, %rd19;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p8, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p9, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd21, %rd20, %rd19;
	// inline asm
	mov.b64 {%r18,%r19}, %rd21;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p10, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p11, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd22, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd23, %rd22, %rd21;
	// inline asm
	mov.b64 {%r22,%r23}, %rd23;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p12, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p13, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd24, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd25, %rd24, %rd23;
	// inline asm
	mov.b64 {%r26,%r27}, %rd25;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p14, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p15, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd26, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd8, %rd26, %rd25;
	setp.ne.s32	%p16, %r4, 0;
	@%p16 bra 	BB315_10;

	shr.u32 	%r37, %r2, 9;
	cvta.to.global.u64 	%rd28, %rd10;
	mul.wide.u32 	%rd29, %r37, 8;
	add.s64 	%rd30, %rd28, %rd29;
	st.global.u64 	[%rd30], %rd8;

BB315_10:
	ret;
}

	// .globl	block_reduce_mul_i64_256
.visible .entry block_reduce_mul_i64_256(
	.param .u64 block_reduce_mul_i64_256_param_0,
	.param .u64 block_reduce_mul_i64_256_param_1,
	.param .u32 block_reduce_mul_i64_256_param_2
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<30>;


	ld.param.u64 	%rd7, [block_reduce_mul_i64_256_param_0];
	ld.param.u64 	%rd8, [block_reduce_mul_i64_256_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_i64_256_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB316_8;

	cvta.to.global.u64 	%rd9, %rd7;
	mul.wide.u32 	%rd10, %r2, 8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u64 	%rd28, [%rd11];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd28;
	bar.sync 	0;
	and.b32  	%r4, %r1, 255;
	setp.gt.u32	%p2, %r4, 127;
	@%p2 bra 	BB316_3;

	ld.shared.u64 	%rd12, [%r3+1024];
	mul.lo.s64 	%rd28, %rd12, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB316_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 63;
	@%p3 bra 	BB316_5;

	ld.shared.u64 	%rd13, [%r3+512];
	mul.lo.s64 	%rd28, %rd13, %rd28;
	st.shared.u64 	[%r3], %rd28;

BB316_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 31;
	@%p4 bra 	BB316_8;

	ld.shared.u64 	%rd24, [%r3+256];
	mul.lo.s64 	%rd14, %rd24, %rd28;
	// inline asm
	mov.b64 {%r10,%r11}, %rd14;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p5, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p6, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r14,%r15}, %rd16;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p7, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p8, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd18, %rd17, %rd16;
	// inline asm
	mov.b64 {%r18,%r19}, %rd18;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p9, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd19, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd20, %rd19, %rd18;
	// inline asm
	mov.b64 {%r22,%r23}, %rd20;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p12, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd21, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd22, %rd21, %rd20;
	// inline asm
	mov.b64 {%r26,%r27}, %rd22;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p13, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p14, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd23, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd6, %rd23, %rd22;
	setp.ne.s32	%p15, %r4, 0;
	@%p15 bra 	BB316_8;

	shr.u32 	%r37, %r2, 8;
	cvta.to.global.u64 	%rd25, %rd8;
	mul.wide.u32 	%rd26, %r37, 8;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.u64 	[%rd27], %rd6;

BB316_8:
	ret;
}

	// .globl	block_reduce_mul_i64_128
.visible .entry block_reduce_mul_i64_128(
	.param .u64 block_reduce_mul_i64_128_param_0,
	.param .u64 block_reduce_mul_i64_128_param_1,
	.param .u32 block_reduce_mul_i64_128_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd5, [block_reduce_mul_i64_128_param_0];
	ld.param.u64 	%rd6, [block_reduce_mul_i64_128_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_i64_128_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB317_6;

	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r2, 8;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u64 	%rd25, [%rd9];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd25;
	bar.sync 	0;
	and.b32  	%r4, %r1, 127;
	setp.gt.u32	%p2, %r4, 63;
	@%p2 bra 	BB317_3;

	ld.shared.u64 	%rd10, [%r3+512];
	mul.lo.s64 	%rd25, %rd10, %rd25;
	st.shared.u64 	[%r3], %rd25;

BB317_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r4, 31;
	@%p3 bra 	BB317_6;

	ld.shared.u64 	%rd21, [%r3+256];
	mul.lo.s64 	%rd11, %rd21, %rd25;
	// inline asm
	mov.b64 {%r10,%r11}, %rd11;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd12, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd13, %rd12, %rd11;
	// inline asm
	mov.b64 {%r14,%r15}, %rd13;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd14, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd15, %rd14, %rd13;
	// inline asm
	mov.b64 {%r18,%r19}, %rd15;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd16, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd17, %rd16, %rd15;
	// inline asm
	mov.b64 {%r22,%r23}, %rd17;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd18, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd19, %rd18, %rd17;
	// inline asm
	mov.b64 {%r26,%r27}, %rd19;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p12, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p13, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd20, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd4, %rd20, %rd19;
	setp.ne.s32	%p14, %r4, 0;
	@%p14 bra 	BB317_6;

	shr.u32 	%r37, %r2, 7;
	cvta.to.global.u64 	%rd22, %rd6;
	mul.wide.u32 	%rd23, %r37, 8;
	add.s64 	%rd24, %rd22, %rd23;
	st.global.u64 	[%rd24], %rd4;

BB317_6:
	ret;
}

	// .globl	block_reduce_mul_i64_64
.visible .entry block_reduce_mul_i64_64(
	.param .u64 block_reduce_mul_i64_64_param_0,
	.param .u64 block_reduce_mul_i64_64_param_1,
	.param .u32 block_reduce_mul_i64_64_param_2
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd3, [block_reduce_mul_i64_64_param_0];
	ld.param.u64 	%rd4, [block_reduce_mul_i64_64_param_1];
	ld.param.u32 	%r5, [block_reduce_mul_i64_64_param_2];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r6, %r7, %r1;
	setp.ge.u32	%p1, %r2, %r5;
	@%p1 bra 	BB318_4;

	cvta.to.global.u64 	%rd5, %rd3;
	mul.wide.u32 	%rd6, %r2, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u64 	%rd1, [%rd7];
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r3, %r9, %r8;
	st.shared.u64 	[%r3], %rd1;
	bar.sync 	0;
	and.b32  	%r4, %r1, 63;
	setp.gt.u32	%p2, %r4, 31;
	@%p2 bra 	BB318_4;

	ld.shared.u64 	%rd18, [%r3+256];
	mul.lo.s64 	%rd8, %rd18, %rd1;
	// inline asm
	mov.b64 {%r10,%r11}, %rd8;
	// inline asm
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r11, %r31, %r30, %r32;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r31, %r30, %r32;
	// inline asm
	mov.b64 %rd9, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r14,%r15}, %rd10;
	// inline asm
	mov.u32 	%r33, 2;
	shfl.sync.bfly.b32 	%r17|%p5, %r15, %r33, %r30, %r32;
	shfl.sync.bfly.b32 	%r16|%p6, %r14, %r33, %r30, %r32;
	// inline asm
	mov.b64 %rd11, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r18,%r19}, %rd12;
	// inline asm
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r34, %r30, %r32;
	shfl.sync.bfly.b32 	%r20|%p8, %r18, %r34, %r30, %r32;
	// inline asm
	mov.b64 %rd13, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd14, %rd13, %rd12;
	// inline asm
	mov.b64 {%r22,%r23}, %rd14;
	// inline asm
	mov.u32 	%r35, 8;
	shfl.sync.bfly.b32 	%r25|%p9, %r23, %r35, %r30, %r32;
	shfl.sync.bfly.b32 	%r24|%p10, %r22, %r35, %r30, %r32;
	// inline asm
	mov.b64 %rd15, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd16, %rd15, %rd14;
	// inline asm
	mov.b64 {%r26,%r27}, %rd16;
	// inline asm
	mov.u32 	%r36, 16;
	shfl.sync.bfly.b32 	%r29|%p11, %r27, %r36, %r30, %r32;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r36, %r30, %r32;
	// inline asm
	mov.b64 %rd17, {%r28,%r29};
	// inline asm
	mul.lo.s64 	%rd2, %rd17, %rd16;
	setp.ne.s32	%p13, %r4, 0;
	@%p13 bra 	BB318_4;

	shr.u32 	%r37, %r2, 6;
	cvta.to.global.u64 	%rd19, %rd4;
	mul.wide.u32 	%rd20, %r37, 8;
	add.s64 	%rd21, %rd19, %rd20;
	st.global.u64 	[%rd21], %rd2;

BB318_4:
	ret;
}

	// .globl	block_reduce_mul_i64_32
.visible .entry block_reduce_mul_i64_32(
	.param .u64 block_reduce_mul_i64_32_param_0,
	.param .u64 block_reduce_mul_i64_32_param_1,
	.param .u32 block_reduce_mul_i64_32_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd2, [block_reduce_mul_i64_32_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_i64_32_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_i64_32_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB319_3;

	cvta.to.global.u64 	%rd14, %rd2;
	mul.wide.u32 	%rd15, %r2, 8;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.u64 	%rd4, [%rd16];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 1;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r27, %r26, %r28;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r27, %r26, %r28;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r29, %r26, %r28;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r29, %r26, %r28;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r30, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r30, %r26, %r28;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r30, %r26, %r28;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r31, %r26, %r28;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r31, %r26, %r28;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd12, %rd11, %rd10;
	// inline asm
	mov.b64 {%r22,%r23}, %rd12;
	// inline asm
	mov.u32 	%r32, 16;
	shfl.sync.bfly.b32 	%r25|%p10, %r23, %r32, %r26, %r28;
	shfl.sync.bfly.b32 	%r24|%p11, %r22, %r32, %r26, %r28;
	// inline asm
	mov.b64 %rd13, {%r24,%r25};
	// inline asm
	mul.lo.s64 	%rd1, %rd13, %rd12;
	and.b32  	%r33, %r1, 31;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB319_3;

	shr.u32 	%r34, %r2, 5;
	cvta.to.global.u64 	%rd17, %rd3;
	mul.wide.u32 	%rd18, %r34, 8;
	add.s64 	%rd19, %rd17, %rd18;
	st.global.u64 	[%rd19], %rd1;

BB319_3:
	ret;
}

	// .globl	block_reduce_mul_i64_16
.visible .entry block_reduce_mul_i64_16(
	.param .u64 block_reduce_mul_i64_16_param_0,
	.param .u64 block_reduce_mul_i64_16_param_1,
	.param .u32 block_reduce_mul_i64_16_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd2, [block_reduce_mul_i64_16_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_i64_16_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_i64_16_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB320_3;

	cvta.to.global.u64 	%rd12, %rd2;
	mul.wide.u32 	%rd13, %r2, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.u64 	%rd4, [%rd14];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r22, 4127;
	mov.u32 	%r23, 1;
	mov.u32 	%r24, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r23, %r22, %r24;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r23, %r22, %r24;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r25, %r22, %r24;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r25, %r22, %r24;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r26, %r22, %r24;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r26, %r22, %r24;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd10, %rd9, %rd8;
	// inline asm
	mov.b64 {%r18,%r19}, %rd10;
	// inline asm
	mov.u32 	%r27, 8;
	shfl.sync.bfly.b32 	%r21|%p8, %r19, %r27, %r22, %r24;
	shfl.sync.bfly.b32 	%r20|%p9, %r18, %r27, %r22, %r24;
	// inline asm
	mov.b64 %rd11, {%r20,%r21};
	// inline asm
	mul.lo.s64 	%rd1, %rd11, %rd10;
	and.b32  	%r28, %r1, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB320_3;

	shr.u32 	%r29, %r2, 4;
	cvta.to.global.u64 	%rd15, %rd3;
	mul.wide.u32 	%rd16, %r29, 8;
	add.s64 	%rd17, %rd15, %rd16;
	st.global.u64 	[%rd17], %rd1;

BB320_3:
	ret;
}

	// .globl	block_reduce_mul_i64_8
.visible .entry block_reduce_mul_i64_8(
	.param .u64 block_reduce_mul_i64_8_param_0,
	.param .u64 block_reduce_mul_i64_8_param_1,
	.param .u32 block_reduce_mul_i64_8_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [block_reduce_mul_i64_8_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_i64_8_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_i64_8_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB321_3;

	cvta.to.global.u64 	%rd10, %rd2;
	mul.wide.u32 	%rd11, %r2, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u64 	%rd4, [%rd12];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r18, 6175;
	mov.u32 	%r19, 1;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r19, %r18, %r20;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r19, %r18, %r20;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r21, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r21, %r18, %r20;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r21, %r18, %r20;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd8, %rd7, %rd6;
	// inline asm
	mov.b64 {%r14,%r15}, %rd8;
	// inline asm
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r17|%p6, %r15, %r22, %r18, %r20;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r22, %r18, %r20;
	// inline asm
	mov.b64 %rd9, {%r16,%r17};
	// inline asm
	mul.lo.s64 	%rd1, %rd9, %rd8;
	and.b32  	%r23, %r1, 7;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB321_3;

	cvta.to.global.u64 	%rd13, %rd3;
	and.b32  	%r24, %r2, -8;
	cvt.u64.u32	%rd14, %r24;
	add.s64 	%rd15, %rd13, %rd14;
	st.global.u64 	[%rd15], %rd1;

BB321_3:
	ret;
}

	// .globl	block_reduce_mul_i64_4
.visible .entry block_reduce_mul_i64_4(
	.param .u64 block_reduce_mul_i64_4_param_0,
	.param .u64 block_reduce_mul_i64_4_param_1,
	.param .u32 block_reduce_mul_i64_4_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd2, [block_reduce_mul_i64_4_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_i64_4_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_i64_4_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB322_3;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r2, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u64 	%rd4, [%rd10];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r14, 7199;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r15, %r14, %r16;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r15, %r14, %r16;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd6, %rd5, %rd4;
	// inline asm
	mov.b64 {%r10,%r11}, %rd6;
	// inline asm
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r17, %r14, %r16;
	shfl.sync.bfly.b32 	%r12|%p5, %r10, %r17, %r14, %r16;
	// inline asm
	mov.b64 %rd7, {%r12,%r13};
	// inline asm
	mul.lo.s64 	%rd1, %rd7, %rd6;
	and.b32  	%r18, %r1, 3;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB322_3;

	shr.u32 	%r19, %r2, 2;
	cvta.to.global.u64 	%rd11, %rd3;
	mul.wide.u32 	%rd12, %r19, 8;
	add.s64 	%rd13, %rd11, %rd12;
	st.global.u64 	[%rd13], %rd1;

BB322_3:
	ret;
}

	// .globl	block_reduce_mul_i64_2
.visible .entry block_reduce_mul_i64_2(
	.param .u64 block_reduce_mul_i64_2_param_0,
	.param .u64 block_reduce_mul_i64_2_param_1,
	.param .u32 block_reduce_mul_i64_2_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd2, [block_reduce_mul_i64_2_param_0];
	ld.param.u64 	%rd3, [block_reduce_mul_i64_2_param_1];
	ld.param.u32 	%r3, [block_reduce_mul_i64_2_param_2];
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r4, %r5, %r1;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB323_3;

	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u64 	%rd4, [%rd8];
	// inline asm
	mov.b64 {%r6,%r7}, %rd4;
	// inline asm
	mov.u32 	%r10, 7711;
	mov.u32 	%r11, 1;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r9|%p2, %r7, %r11, %r10, %r12;
	shfl.sync.bfly.b32 	%r8|%p3, %r6, %r11, %r10, %r12;
	// inline asm
	mov.b64 %rd5, {%r8,%r9};
	// inline asm
	mul.lo.s64 	%rd1, %rd5, %rd4;
	and.b32  	%r13, %r1, 1;
	setp.eq.b32	%p4, %r13, 1;
	@%p4 bra 	BB323_3;

	shr.u32 	%r14, %r2, 1;
	cvta.to.global.u64 	%rd9, %rd3;
	mul.wide.u32 	%rd10, %r14, 8;
	add.s64 	%rd11, %rd9, %rd10;
	st.global.u64 	[%rd11], %rd1;

BB323_3:
	ret;
}

	// .globl	block_reduce_or_u32_1024
.visible .entry block_reduce_or_u32_1024(
	.param .u64 block_reduce_or_u32_1024_param_0,
	.param .u64 block_reduce_or_u32_1024_param_1,
	.param .u32 block_reduce_or_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_or_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB324_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB324_3;

	ld.shared.u32 	%r28, [%r23+2048];
	or.b32  	%r81, %r28, %r81;
	st.shared.u32 	[%r23], %r81;

BB324_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB324_5;

	ld.shared.u32 	%r35, [%r23+1024];
	or.b32  	%r81, %r35, %r81;
	st.shared.u32 	[%r23], %r81;

BB324_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB324_7;

	ld.shared.u32 	%r42, [%r23+512];
	or.b32  	%r81, %r42, %r81;
	st.shared.u32 	[%r23], %r81;

BB324_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB324_9;

	ld.shared.u32 	%r49, [%r23+256];
	or.b32  	%r81, %r49, %r81;
	st.shared.u32 	[%r23], %r81;

BB324_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB324_12;

	ld.shared.u32 	%r56, [%r23+128];
	or.b32  	%r57, %r56, %r81;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	or.b32  	%r62, %r61, %r57;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	or.b32  	%r65, %r64, %r62;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	or.b32  	%r68, %r67, %r65;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	or.b32  	%r71, %r70, %r68;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	or.b32  	%r11, %r73, %r71;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB324_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB324_12:
	ret;
}

	// .globl	block_reduce_or_u32_512
.visible .entry block_reduce_or_u32_512(
	.param .u64 block_reduce_or_u32_512_param_0,
	.param .u64 block_reduce_or_u32_512_param_1,
	.param .u32 block_reduce_or_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_or_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB325_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB325_3;

	ld.shared.u32 	%r19, [%r4+1024];
	or.b32  	%r54, %r19, %r54;
	st.shared.u32 	[%r4], %r54;

BB325_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB325_5;

	ld.shared.u32 	%r22, [%r4+512];
	or.b32  	%r54, %r22, %r54;
	st.shared.u32 	[%r4], %r54;

BB325_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB325_7;

	ld.shared.u32 	%r25, [%r4+256];
	or.b32  	%r54, %r25, %r54;
	st.shared.u32 	[%r4], %r54;

BB325_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB325_10;

	ld.shared.u32 	%r28, [%r4+128];
	or.b32  	%r29, %r28, %r54;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	or.b32  	%r34, %r33, %r29;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	or.b32  	%r37, %r36, %r34;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	or.b32  	%r40, %r39, %r37;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	or.b32  	%r43, %r42, %r40;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	or.b32  	%r11, %r45, %r43;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB325_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB325_10:
	ret;
}

	// .globl	block_reduce_or_u32_256
.visible .entry block_reduce_or_u32_256(
	.param .u64 block_reduce_or_u32_256_param_0,
	.param .u64 block_reduce_or_u32_256_param_1,
	.param .u32 block_reduce_or_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_or_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB326_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB326_3;

	ld.shared.u32 	%r16, [%r4+512];
	or.b32  	%r41, %r16, %r41;
	st.shared.u32 	[%r4], %r41;

BB326_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB326_5;

	ld.shared.u32 	%r17, [%r4+256];
	or.b32  	%r41, %r17, %r41;
	st.shared.u32 	[%r4], %r41;

BB326_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB326_8;

	ld.shared.u32 	%r18, [%r4+128];
	or.b32  	%r19, %r18, %r41;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	or.b32  	%r24, %r23, %r19;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	or.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	or.b32  	%r30, %r29, %r27;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	or.b32  	%r33, %r32, %r30;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	or.b32  	%r10, %r35, %r33;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB326_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB326_8:
	ret;
}

	// .globl	block_reduce_or_u32_128
.visible .entry block_reduce_or_u32_128(
	.param .u64 block_reduce_or_u32_128_param_0,
	.param .u64 block_reduce_or_u32_128_param_1,
	.param .u32 block_reduce_or_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_or_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB327_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB327_3;

	ld.shared.u32 	%r14, [%r4+256];
	or.b32  	%r34, %r14, %r34;
	st.shared.u32 	[%r4], %r34;

BB327_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB327_6;

	ld.shared.u32 	%r15, [%r4+128];
	or.b32  	%r16, %r15, %r34;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	or.b32  	%r21, %r20, %r16;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	or.b32  	%r24, %r23, %r21;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	or.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	or.b32  	%r30, %r29, %r27;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	or.b32  	%r8, %r32, %r30;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB327_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB327_6:
	ret;
}

	// .globl	block_reduce_or_u32_64
.visible .entry block_reduce_or_u32_64(
	.param .u64 block_reduce_or_u32_64_param_0,
	.param .u64 block_reduce_or_u32_64_param_1,
	.param .u32 block_reduce_or_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_or_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB328_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB328_4;

	ld.shared.u32 	%r12, [%r4+128];
	or.b32  	%r13, %r12, %r3;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	or.b32  	%r18, %r17, %r13;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	or.b32  	%r21, %r20, %r18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	or.b32  	%r24, %r23, %r21;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	or.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	or.b32  	%r6, %r29, %r27;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB328_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB328_4:
	ret;
}

	// .globl	block_reduce_or_u32_32
.visible .entry block_reduce_or_u32_32(
	.param .u64 block_reduce_or_u32_32_param_0,
	.param .u64 block_reduce_or_u32_32_param_1,
	.param .u32 block_reduce_or_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_or_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB329_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	or.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	or.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	or.b32  	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	or.b32  	%r21, %r20, %r18;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	or.b32  	%r3, %r23, %r21;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB329_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB329_3:
	ret;
}

	// .globl	block_reduce_or_u32_16
.visible .entry block_reduce_or_u32_16(
	.param .u64 block_reduce_or_u32_16_param_0,
	.param .u64 block_reduce_or_u32_16_param_1,
	.param .u32 block_reduce_or_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_or_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB330_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	or.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	or.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	or.b32  	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	or.b32  	%r3, %r20, %r18;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB330_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB330_3:
	ret;
}

	// .globl	block_reduce_or_u32_8
.visible .entry block_reduce_or_u32_8(
	.param .u64 block_reduce_or_u32_8_param_0,
	.param .u64 block_reduce_or_u32_8_param_1,
	.param .u32 block_reduce_or_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_or_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB331_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	or.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	or.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	or.b32  	%r3, %r17, %r15;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB331_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB331_3:
	ret;
}

	// .globl	block_reduce_or_u32_4
.visible .entry block_reduce_or_u32_4(
	.param .u64 block_reduce_or_u32_4_param_0,
	.param .u64 block_reduce_or_u32_4_param_1,
	.param .u32 block_reduce_or_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_or_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB332_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	or.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	or.b32  	%r3, %r14, %r12;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB332_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB332_3:
	ret;
}

	// .globl	block_reduce_or_u32_2
.visible .entry block_reduce_or_u32_2(
	.param .u64 block_reduce_or_u32_2_param_0,
	.param .u64 block_reduce_or_u32_2_param_1,
	.param .u32 block_reduce_or_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_or_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_or_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_or_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB333_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	or.b32  	%r3, %r11, %r7;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB333_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB333_3:
	ret;
}

	// .globl	block_reduce_and_u32_1024
.visible .entry block_reduce_and_u32_1024(
	.param .u64 block_reduce_and_u32_1024_param_0,
	.param .u64 block_reduce_and_u32_1024_param_1,
	.param .u32 block_reduce_and_u32_1024_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<84>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_1024_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_1024_param_1];
	ld.param.u32 	%r12, [block_reduce_and_u32_1024_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r16, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r16, %r12;
	@%p1 bra 	BB334_12;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r1, %r13, 1023;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r81, [%rd5];
	shl.b32 	%r21, %r13, 2;
	mov.u32 	%r22, shared;
	add.s32 	%r23, %r22, %r21;
	st.shared.u32 	[%r23], %r81;
	bar.sync 	0;
	setp.gt.u32	%p2, %r1, 511;
	@%p2 bra 	BB334_3;

	ld.shared.u32 	%r28, [%r23+2048];
	and.b32  	%r81, %r28, %r81;
	st.shared.u32 	[%r23], %r81;

BB334_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r1, 255;
	@%p3 bra 	BB334_5;

	ld.shared.u32 	%r35, [%r23+1024];
	and.b32  	%r81, %r35, %r81;
	st.shared.u32 	[%r23], %r81;

BB334_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r1, 127;
	@%p4 bra 	BB334_7;

	ld.shared.u32 	%r42, [%r23+512];
	and.b32  	%r81, %r42, %r81;
	st.shared.u32 	[%r23], %r81;

BB334_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r1, 63;
	@%p5 bra 	BB334_9;

	ld.shared.u32 	%r49, [%r23+256];
	and.b32  	%r81, %r49, %r81;
	st.shared.u32 	[%r23], %r81;

BB334_9:
	bar.sync 	0;
	setp.gt.u32	%p6, %r1, 31;
	@%p6 bra 	BB334_12;

	ld.shared.u32 	%r56, [%r23+128];
	and.b32  	%r57, %r56, %r81;
	mov.u32 	%r58, 31;
	mov.u32 	%r59, 1;
	mov.u32 	%r60, -1;
	shfl.sync.bfly.b32 	%r61|%p7, %r57, %r59, %r58, %r60;
	and.b32  	%r62, %r61, %r57;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r58, %r60;
	and.b32  	%r65, %r64, %r62;
	mov.u32 	%r66, 4;
	shfl.sync.bfly.b32 	%r67|%p9, %r65, %r66, %r58, %r60;
	and.b32  	%r68, %r67, %r65;
	mov.u32 	%r69, 8;
	shfl.sync.bfly.b32 	%r70|%p10, %r68, %r69, %r58, %r60;
	and.b32  	%r71, %r70, %r68;
	mov.u32 	%r72, 16;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r72, %r58, %r60;
	and.b32  	%r11, %r73, %r71;
	setp.ne.s32	%p12, %r1, 0;
	@%p12 bra 	BB334_12;

	shr.u32 	%r79, %r16, 10;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r79, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB334_12:
	ret;
}

	// .globl	block_reduce_and_u32_512
.visible .entry block_reduce_and_u32_512(
	.param .u64 block_reduce_and_u32_512_param_0,
	.param .u64 block_reduce_and_u32_512_param_1,
	.param .u32 block_reduce_and_u32_512_param_2
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_512_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_512_param_1];
	ld.param.u32 	%r12, [block_reduce_and_u32_512_param_2];
	mov.u32 	%r13, %tid.x;
	mov.u32 	%r14, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mad.lo.s32 	%r1, %r14, %r15, %r13;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	BB335_10;

	cvta.to.global.u64 	%rd3, %rd1;
	and.b32  	%r2, %r13, 511;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r54, [%rd5];
	shl.b32 	%r17, %r13, 2;
	mov.u32 	%r18, shared;
	add.s32 	%r4, %r18, %r17;
	st.shared.u32 	[%r4], %r54;
	bar.sync 	0;
	setp.gt.u32	%p2, %r2, 255;
	@%p2 bra 	BB335_3;

	ld.shared.u32 	%r19, [%r4+1024];
	and.b32  	%r54, %r19, %r54;
	st.shared.u32 	[%r4], %r54;

BB335_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r2, 127;
	@%p3 bra 	BB335_5;

	ld.shared.u32 	%r22, [%r4+512];
	and.b32  	%r54, %r22, %r54;
	st.shared.u32 	[%r4], %r54;

BB335_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r2, 63;
	@%p4 bra 	BB335_7;

	ld.shared.u32 	%r25, [%r4+256];
	and.b32  	%r54, %r25, %r54;
	st.shared.u32 	[%r4], %r54;

BB335_7:
	bar.sync 	0;
	setp.gt.u32	%p5, %r2, 31;
	@%p5 bra 	BB335_10;

	ld.shared.u32 	%r28, [%r4+128];
	and.b32  	%r29, %r28, %r54;
	mov.u32 	%r30, 31;
	mov.u32 	%r31, 1;
	mov.u32 	%r32, -1;
	shfl.sync.bfly.b32 	%r33|%p6, %r29, %r31, %r30, %r32;
	and.b32  	%r34, %r33, %r29;
	mov.u32 	%r35, 2;
	shfl.sync.bfly.b32 	%r36|%p7, %r34, %r35, %r30, %r32;
	and.b32  	%r37, %r36, %r34;
	mov.u32 	%r38, 4;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r30, %r32;
	and.b32  	%r40, %r39, %r37;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r30, %r32;
	and.b32  	%r43, %r42, %r40;
	mov.u32 	%r44, 16;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r30, %r32;
	and.b32  	%r11, %r45, %r43;
	setp.ne.s32	%p11, %r2, 0;
	@%p11 bra 	BB335_10;

	shr.u32 	%r52, %r1, 9;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r52, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r11;

BB335_10:
	ret;
}

	// .globl	block_reduce_and_u32_256
.visible .entry block_reduce_and_u32_256(
	.param .u64 block_reduce_and_u32_256_param_0,
	.param .u64 block_reduce_and_u32_256_param_1,
	.param .u32 block_reduce_and_u32_256_param_2
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_256_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_256_param_1];
	ld.param.u32 	%r11, [block_reduce_and_u32_256_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mad.lo.s32 	%r2, %r12, %r13, %r1;
	setp.ge.u32	%p1, %r2, %r11;
	@%p1 bra 	BB336_8;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r41, [%rd5];
	shl.b32 	%r14, %r1, 2;
	mov.u32 	%r15, shared;
	add.s32 	%r4, %r15, %r14;
	st.shared.u32 	[%r4], %r41;
	bar.sync 	0;
	and.b32  	%r5, %r1, 255;
	setp.gt.u32	%p2, %r5, 127;
	@%p2 bra 	BB336_3;

	ld.shared.u32 	%r16, [%r4+512];
	and.b32  	%r41, %r16, %r41;
	st.shared.u32 	[%r4], %r41;

BB336_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 63;
	@%p3 bra 	BB336_5;

	ld.shared.u32 	%r17, [%r4+256];
	and.b32  	%r41, %r17, %r41;
	st.shared.u32 	[%r4], %r41;

BB336_5:
	bar.sync 	0;
	setp.gt.u32	%p4, %r5, 31;
	@%p4 bra 	BB336_8;

	ld.shared.u32 	%r18, [%r4+128];
	and.b32  	%r19, %r18, %r41;
	mov.u32 	%r20, 31;
	mov.u32 	%r21, 1;
	mov.u32 	%r22, -1;
	shfl.sync.bfly.b32 	%r23|%p5, %r19, %r21, %r20, %r22;
	and.b32  	%r24, %r23, %r19;
	mov.u32 	%r25, 2;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r20, %r22;
	and.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 4;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r20, %r22;
	and.b32  	%r30, %r29, %r27;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r20, %r22;
	and.b32  	%r33, %r32, %r30;
	mov.u32 	%r34, 16;
	shfl.sync.bfly.b32 	%r35|%p9, %r33, %r34, %r20, %r22;
	and.b32  	%r10, %r35, %r33;
	setp.ne.s32	%p10, %r5, 0;
	@%p10 bra 	BB336_8;

	shr.u32 	%r40, %r2, 8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r40, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;

BB336_8:
	ret;
}

	// .globl	block_reduce_and_u32_128
.visible .entry block_reduce_and_u32_128(
	.param .u64 block_reduce_and_u32_128_param_0,
	.param .u64 block_reduce_and_u32_128_param_1,
	.param .u32 block_reduce_and_u32_128_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_128_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_128_param_1];
	ld.param.u32 	%r9, [block_reduce_and_u32_128_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mad.lo.s32 	%r2, %r10, %r11, %r1;
	setp.ge.u32	%p1, %r2, %r9;
	@%p1 bra 	BB337_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r34, [%rd5];
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r4, %r13, %r12;
	st.shared.u32 	[%r4], %r34;
	bar.sync 	0;
	and.b32  	%r5, %r1, 127;
	setp.gt.u32	%p2, %r5, 63;
	@%p2 bra 	BB337_3;

	ld.shared.u32 	%r14, [%r4+256];
	and.b32  	%r34, %r14, %r34;
	st.shared.u32 	[%r4], %r34;

BB337_3:
	bar.sync 	0;
	setp.gt.u32	%p3, %r5, 31;
	@%p3 bra 	BB337_6;

	ld.shared.u32 	%r15, [%r4+128];
	and.b32  	%r16, %r15, %r34;
	mov.u32 	%r17, 31;
	mov.u32 	%r18, 1;
	mov.u32 	%r19, -1;
	shfl.sync.bfly.b32 	%r20|%p4, %r16, %r18, %r17, %r19;
	and.b32  	%r21, %r20, %r16;
	mov.u32 	%r22, 2;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r17, %r19;
	and.b32  	%r24, %r23, %r21;
	mov.u32 	%r25, 4;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r17, %r19;
	and.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r17, %r19;
	and.b32  	%r30, %r29, %r27;
	mov.u32 	%r31, 16;
	shfl.sync.bfly.b32 	%r32|%p8, %r30, %r31, %r17, %r19;
	and.b32  	%r8, %r32, %r30;
	setp.ne.s32	%p9, %r5, 0;
	@%p9 bra 	BB337_6;

	shr.u32 	%r33, %r2, 7;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r33, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

BB337_6:
	ret;
}

	// .globl	block_reduce_and_u32_64
.visible .entry block_reduce_and_u32_64(
	.param .u64 block_reduce_and_u32_64_param_0,
	.param .u64 block_reduce_and_u32_64_param_1,
	.param .u32 block_reduce_and_u32_64_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_64_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_64_param_1];
	ld.param.u32 	%r7, [block_reduce_and_u32_64_param_2];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r8, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mad.lo.s32 	%r2, %r8, %r9, %r1;
	setp.ge.u32	%p1, %r2, %r7;
	@%p1 bra 	BB338_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r3, [%rd5];
	shl.b32 	%r10, %r1, 2;
	mov.u32 	%r11, shared;
	add.s32 	%r4, %r11, %r10;
	st.shared.u32 	[%r4], %r3;
	bar.sync 	0;
	and.b32  	%r5, %r1, 63;
	setp.gt.u32	%p2, %r5, 31;
	@%p2 bra 	BB338_4;

	ld.shared.u32 	%r12, [%r4+128];
	and.b32  	%r13, %r12, %r3;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 1;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	and.b32  	%r18, %r17, %r13;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	and.b32  	%r21, %r20, %r18;
	mov.u32 	%r22, 4;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	and.b32  	%r24, %r23, %r21;
	mov.u32 	%r25, 8;
	shfl.sync.bfly.b32 	%r26|%p6, %r24, %r25, %r14, %r16;
	and.b32  	%r27, %r26, %r24;
	mov.u32 	%r28, 16;
	shfl.sync.bfly.b32 	%r29|%p7, %r27, %r28, %r14, %r16;
	and.b32  	%r6, %r29, %r27;
	setp.ne.s32	%p8, %r5, 0;
	@%p8 bra 	BB338_4;

	shr.u32 	%r30, %r2, 6;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r30, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r6;

BB338_4:
	ret;
}

	// .globl	block_reduce_and_u32_32
.visible .entry block_reduce_and_u32_32(
	.param .u64 block_reduce_and_u32_32_param_0,
	.param .u64 block_reduce_and_u32_32_param_1,
	.param .u32 block_reduce_and_u32_32_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_32_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_32_param_1];
	ld.param.u32 	%r4, [block_reduce_and_u32_32_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB339_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	and.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	and.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	and.b32  	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	and.b32  	%r21, %r20, %r18;
	mov.u32 	%r22, 16;
	shfl.sync.bfly.b32 	%r23|%p6, %r21, %r22, %r8, %r10;
	and.b32  	%r3, %r23, %r21;
	and.b32  	%r24, %r1, 31;
	setp.ne.s32	%p7, %r24, 0;
	@%p7 bra 	BB339_3;

	shr.u32 	%r25, %r2, 5;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB339_3:
	ret;
}

	// .globl	block_reduce_and_u32_16
.visible .entry block_reduce_and_u32_16(
	.param .u64 block_reduce_and_u32_16_param_0,
	.param .u64 block_reduce_and_u32_16_param_1,
	.param .u32 block_reduce_and_u32_16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_16_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_16_param_1];
	ld.param.u32 	%r4, [block_reduce_and_u32_16_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB340_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 4127;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	and.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	and.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	and.b32  	%r18, %r17, %r15;
	mov.u32 	%r19, 8;
	shfl.sync.bfly.b32 	%r20|%p5, %r18, %r19, %r8, %r10;
	and.b32  	%r3, %r20, %r18;
	and.b32  	%r21, %r1, 15;
	setp.ne.s32	%p6, %r21, 0;
	@%p6 bra 	BB340_3;

	shr.u32 	%r22, %r2, 4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB340_3:
	ret;
}

	// .globl	block_reduce_and_u32_8
.visible .entry block_reduce_and_u32_8(
	.param .u64 block_reduce_and_u32_8_param_0,
	.param .u64 block_reduce_and_u32_8_param_1,
	.param .u32 block_reduce_and_u32_8_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_8_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_8_param_1];
	ld.param.u32 	%r4, [block_reduce_and_u32_8_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB341_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 6175;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	and.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	and.b32  	%r15, %r14, %r12;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p4, %r15, %r16, %r8, %r10;
	and.b32  	%r3, %r17, %r15;
	and.b32  	%r18, %r1, 7;
	setp.ne.s32	%p5, %r18, 0;
	@%p5 bra 	BB341_3;

	shr.u32 	%r19, %r2, 3;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB341_3:
	ret;
}

	// .globl	block_reduce_and_u32_4
.visible .entry block_reduce_and_u32_4(
	.param .u64 block_reduce_and_u32_4_param_0,
	.param .u64 block_reduce_and_u32_4_param_1,
	.param .u32 block_reduce_and_u32_4_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_4_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_4_param_1];
	ld.param.u32 	%r4, [block_reduce_and_u32_4_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB342_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7199;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	and.b32  	%r12, %r11, %r7;
	mov.u32 	%r13, 2;
	shfl.sync.bfly.b32 	%r14|%p3, %r12, %r13, %r8, %r10;
	and.b32  	%r3, %r14, %r12;
	and.b32  	%r15, %r1, 3;
	setp.ne.s32	%p4, %r15, 0;
	@%p4 bra 	BB342_3;

	cvta.to.global.u64 	%rd6, %rd2;
	and.b32  	%r16, %r2, -4;
	cvt.u64.u32	%rd7, %r16;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB342_3:
	ret;
}

	// .globl	block_reduce_and_u32_2
.visible .entry block_reduce_and_u32_2(
	.param .u64 block_reduce_and_u32_2_param_0,
	.param .u64 block_reduce_and_u32_2_param_1,
	.param .u32 block_reduce_and_u32_2_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_reduce_and_u32_2_param_0];
	ld.param.u64 	%rd2, [block_reduce_and_u32_2_param_1];
	ld.param.u32 	%r4, [block_reduce_and_u32_2_param_2];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r5, %r6, %r1;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB343_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r7, [%rd5];
	mov.u32 	%r8, 7711;
	mov.u32 	%r9, 1;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p2, %r7, %r9, %r8, %r10;
	and.b32  	%r3, %r11, %r7;
	and.b32  	%r12, %r1, 1;
	setp.eq.b32	%p3, %r12, 1;
	@%p3 bra 	BB343_3;

	shr.u32 	%r13, %r2, 1;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r3;

BB343_3:
	ret;
}


